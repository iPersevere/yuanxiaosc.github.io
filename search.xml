<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[TensorFlow]]></title>
    <url>%2F2018%2F11%2F06%2FTensorFlow%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 TensorFlow官方文档 W3Cschool TensorFlow-Examples aymericdamien @tflearn Author 比官方更简洁的Tensorflow入门教程 10 分钟阅读 20180507 Tensorflow快餐教程 TensorFlow 基本概念 20180604 问题集锦tf.nn.conv2d12345input = tf.Variable(tf.random_normal([1,5,5,5]))filter = tf.Variable(tf.random_normal([3,3,5,7]))op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='VALID')print(op) tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None)除去name参数用以指定该操作的name，与方法有关的一共五个参数： 第一个参数input：指需要做卷积的输入图像，它要求是一个Tensor，具有[batch, in_height, in_width, in_channels]这样的shape，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一 第二个参数filter：相当于CNN中的卷积核，它要求是一个Tensor，具有[filter_height, filter_width, in_channels, out_channels]这样的shape，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同，有一个地方需要注意，第三维in_channels，就是参数input的第四维 第三个参数strides：卷积时在图像每一维的步长，这是一个一维的向量，长度4 第四个参数padding：string类型的量，只能是”SAME”,”VALID”其中之一，这个值决定了不同的卷积方式（后面会介绍） 第五个参数：use_cudnn_on_gpu:bool类型，是否使用cudnn加速，默认为true 结果返回一个Tensor，这个输出，就是我们常说的feature map tensorflow tf.layers.dense 实例12345import tensorflow as tfbatch_size = 5ones = tf.ones([batch_size,6,8,20])logits = tf.layers.dense(ones,10)print(logits.get_shape()) 输出：1(5,6,8,10) tf.one_hot() tf.one_hot()函数是将input转化为one-hot类型数据输出，相当于将多个数值联合放在一起作为多个相同类型的向量，可用于表示各自的概率分布，通常用于分类任务中作为最后的FC层的输出，有时翻译成“独热”编码。123456789import tensorflow as tfclasses = 10labels= tf.constant([0, 2, 2, 3, 4])onehot_output = tf.one_hot(labels, classes)with tf.Session() as sess: print(sess.run(onehot_output)) print(onehot_output.get_shape()) 输出：123456[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]](5, 10) tf.reshape() 将矩阵t变换为一维矩阵，然后再对矩阵的形式进行更改就好了，具体的流程如下：reshape(t,shape) =&gt;reshape(t,[-1]) =&gt;reshape(t,shape)1234567891011121314151617181920212223242526272829303132333435363738394041# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]# tensor 't' has shape [9]reshape(t, [3, 3]) ==&gt; [[1, 2, 3], [4, 5, 6], [7, 8, 9]]# tensor 't' is [[[1, 1], [2, 2]],# [[3, 3], [4, 4]]]# tensor 't' has shape [2, 2, 2]reshape(t, [2, 4]) ==&gt; [[1, 1, 2, 2], [3, 3, 4, 4]]# tensor 't' is [[[1, 1, 1],# [2, 2, 2]],# [[3, 3, 3],# [4, 4, 4]],# [[5, 5, 5],# [6, 6, 6]]]# tensor 't' has shape [3, 2, 3]# pass '[-1]' to flatten 't'reshape(t, [-1]) ==&gt; [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]# -1 can also be used to infer the shape# -1 is inferred to be 9:reshape(t, [2, -1]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3], [4, 4, 4, 5, 5, 5, 6, 6, 6]]# -1 is inferred to be 2:reshape(t, [-1, 9]) ==&gt; [[1, 1, 1, 2, 2, 2, 3, 3, 3], [4, 4, 4, 5, 5, 5, 6, 6, 6]]# -1 is inferred to be 3:reshape(t, [ 2, -1, 3]) ==&gt; [[[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[4, 4, 4], [5, 5, 5], [6, 6, 6]]]# tensor 't' is [7]# shape `[]` reshapes to a scalarreshape(t, []) ==&gt; 7 tf.matmul matmul 会把张量最后两个维度看做二维矩阵，然后进行矩阵乘法。123456789101112import tensorflow as tfimport numpy as npweight = tf.constant(np.array([[np.eye(4)] *3]*2), dtype=tf.float32)a = tf.constant(np.arange(120), shape=[2,3,4,5], dtype=tf.float32)output = tf.matmul(weight, a)with tf.Session() as sess: print('weight:\t', weight.get_shape(),'\n', sess.run(weight)) print('a:\t', a.get_shape(),'\n', sess.run(a)) print('output:\t', output.get_shape(), '\n', sess.run(output)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293weight: (2, 3, 4, 4) [[[[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]]] [[[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]] [[1. 0. 0. 0.] [0. 1. 0. 0.] [0. 0. 1. 0.] [0. 0. 0. 1.]]]]a: (2, 3, 4, 5) [[[[ 0. 1. 2. 3. 4.] [ 5. 6. 7. 8. 9.] [ 10. 11. 12. 13. 14.] [ 15. 16. 17. 18. 19.]] [[ 20. 21. 22. 23. 24.] [ 25. 26. 27. 28. 29.] [ 30. 31. 32. 33. 34.] [ 35. 36. 37. 38. 39.]] [[ 40. 41. 42. 43. 44.] [ 45. 46. 47. 48. 49.] [ 50. 51. 52. 53. 54.] [ 55. 56. 57. 58. 59.]]] [[[ 60. 61. 62. 63. 64.] [ 65. 66. 67. 68. 69.] [ 70. 71. 72. 73. 74.] [ 75. 76. 77. 78. 79.]] [[ 80. 81. 82. 83. 84.] [ 85. 86. 87. 88. 89.] [ 90. 91. 92. 93. 94.] [ 95. 96. 97. 98. 99.]] [[100. 101. 102. 103. 104.] [105. 106. 107. 108. 109.] [110. 111. 112. 113. 114.] [115. 116. 117. 118. 119.]]]]output: (2, 3, 4, 5) [[[[ 0. 1. 2. 3. 4.] [ 5. 6. 7. 8. 9.] [ 10. 11. 12. 13. 14.] [ 15. 16. 17. 18. 19.]] [[ 20. 21. 22. 23. 24.] [ 25. 26. 27. 28. 29.] [ 30. 31. 32. 33. 34.] [ 35. 36. 37. 38. 39.]] [[ 40. 41. 42. 43. 44.] [ 45. 46. 47. 48. 49.] [ 50. 51. 52. 53. 54.] [ 55. 56. 57. 58. 59.]]] [[[ 60. 61. 62. 63. 64.] [ 65. 66. 67. 68. 69.] [ 70. 71. 72. 73. 74.] [ 75. 76. 77. 78. 79.]] [[ 80. 81. 82. 83. 84.] [ 85. 86. 87. 88. 89.] [ 90. 91. 92. 93. 94.] [ 95. 96. 97. 98. 99.]] [[100. 101. 102. 103. 104.] [105. 106. 107. 108. 109.] [110. 111. 112. 113. 114.] [115. 116. 117. 118. 119.]]]] tf.app.flags的作用及使用方法1234567891011121314151617import tensorflow as tf# 第一个是参数名称，第二个参数是默认值，第三个是参数描述tf.app.flags.DEFINE_string('str_name', 'default_value', "description1")tf.app.flags.DEFINE_integer('int_name', 10, "description2")tf.app.flags.DEFINE_boolean('bool_name', False, "description3")FLAGS = tf.app.flags.FLAGS# 必须带参数，否则：'TypeError: main() takes no arguments (1 given)';main的参数名随意定义，无要求def main(_): print(FLAGS.str_name) print(FLAGS.int_name) print(FLAGS.bool_name)if __name__ == '__main__': tf.app.run()# 执行main函数 输出：123default_value10False 使用tf.ConfigProto()配置Session运行参数&amp;&amp;GPU设备指定tf提供了两种控制GPU资源使用的方法，一是让TensorFlow在运行过程中动态申请显存，需要多少就申请多少;第二种方式就是限制GPU的使用率。123config = tf.ConfigProto()config.gpu_options.allow_growth = Truesession = tf.Session(config=config) tf.reducemean()到底是什么意思？1tf.reduce_mean(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None) 根据给出的axis在input_tensor上求平均值。除非keep_dims为真，axis中的每个的张量秩会减少1。如果keep_dims为真，求平均值的维度的长度都会保持为1.如果不设置axis，所有维度上的元素都会被求平均值，并且只会返回一个只有一个元素的张量。 123456789101112import numpy as npimport tensorflow as tfx = np.array([[1.,2.,3.],[4.,5.,6.]])sess = tf.Session()mean_none = sess.run(tf.reduce_mean(x))mean_0 = sess.run(tf.reduce_mean(x, 0))mean_1 = sess.run(tf.reduce_mean(x, 1))print (x)print (mean_none)print (mean_0)print (mean_1)sess.close() 12345[[1. 2. 3.] [4. 5. 6.]]3.5[2.5 3.5 4.5][2. 5.] tf.unstack与tf.stackunstack(value,num=None,axis=0,name=’unstack’ ) 官方解释：https://tensorflow.google.cn/api_docs/python/tf/unstack 解释：这是一个对矩阵进行分解的函数，以下为关键参数解释： value：代表需要分解的矩阵变量（其实就是一个多维数组，一般为二维）； axis：指明对矩阵的哪个维度进行分解。 tf.squeeze() 该函数返回一个张量，这个张量是将原始input中所有维度为1的那些维都删掉的结果axis可以用来指定要删掉的为1的维度，此处要注意指定的维度必须确保其是1，否则会报错。12345# 't' 是一个维度是[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t)) # [2, 3]， 默认删除所有为1的维度# 't' 是一个维度[1, 2, 1, 3, 1, 1]的张量tf.shape(tf.squeeze(t, [2, 4])) # [1, 2, 3, 1]，标号从零开始，只删掉了2和4维的1 关于tensorflow中的softmax_cross_entropy_with_logits_v2函数的区别tf.nn.softmax_cross_entropy_with_logits(记为f1) 和tf.nn.sparse_softmax_cross_entropy_with_logits(记为f3),以及tf.nn.softmax_cross_entropy_with_logits_v2(记为f2)之间的区别。 f1和f3对于参数logits的要求都是一样的，即未经处理的，直接由神经网络输出的数值， 比如 [3.5,2.1,7.89,4.4]。两个函数不一样的地方在于labels格式的要求，f1的要求labels的格式和logits类似，比如[0,0,1,0]。而f3的要求labels是一个数值，这个数值记录着ground truth所在的索引。以[0,0,1,0]为例，这里真值1的索引为2。所以f3要求labels的输入为数字2(tensor)。一般可以用tf.argmax()来从[0,0,1,0]中取得真值的索引。 f1和f2之间很像，实际上官方文档已经标记出f1已经是deprecated 状态，推荐使用f2。两者唯一的区别在于f1在进行反向传播的时候，只对logits进行反向传播，labels保持不变。而f2在进行反向传播的时候，同时对logits和labels都进行反向传播，如果将labels传入的tensor设置为stop_gradients，就和f1一样了。那么问题来了，一般我们在进行监督学习的时候，labels都是标记好的真值，什么时候会需要改变label？f2存在的意义是什么？实际上在应用中labels并不一定都是人工手动标注的，有的时候还可能是神经网络生成的，一个实际的例子就是对抗生成网络（GAN）。 1234567891011121314import tensorflow as tfimport numpy as npTruth = np.array([0,0,1,0])Pred_logits = np.array([3.5,2.1,7.89,4.4])loss = tf.nn.softmax_cross_entropy_with_logits(labels=Truth,logits=Pred_logits)loss2 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=Truth,logits=Pred_logits)loss3 = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.argmax(Truth),logits=Pred_logits)with tf.Session() as sess: print(sess.run(loss)) print(sess.run(loss2)) print(sess.run(loss3)) [tensorflow]sparse_softmax_cross_entropy_with_logits 与 softmax_cross_entropy_with_logits的区别原函数：12tf.nn.sparse_softmax_cross'_entropy_with_logits(logits=net, labels=y)tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=y2) sparse_softmax_cross_entropy_with_logits 中 lables接受直接的数字标签如[1], [2], [3], [4] （类型只能为int32，int64）而softmax_cross_entropy_with_logits中 labels接受one-hot标签如[1,0,0,0], [0,1,0,0],[0,0,1,0], [0,0,0,1] （类型为int32， int64） 『TensorFlow』网络操作API_中_损失函数及分类器tf.gather1234567temp = tf.range(0,10)*10 + tf.constant(1,shape=[10])#收集下标1、5、9处的值temp2 = tf.gather(temp,[1,5,9])with tf.Session() as sess: print(sess.run(temp)) print(sess.run(temp2)) 12[ 1 11 21 31 41 51 61 71 81 91][11 51 91] 矩阵数学函数：tf.matrix_band_part 复制一个张量，将每个最内层矩阵中的所有中心区域外的所有内容设置为零。num_lower, num_upper 分别控制矩阵对角线的列数。123456matrix_band_part( input, num_lower, num_upper, name=None) 例如： 1234567891011121314# if 'input' is [[ 0, 1, 2, 3] [-1, 0, 1, 2] [-2, -1, 0, 1] [-3, -2, -1, 0]],tf.matrix_band_part(input, 1, -1) ==&gt; [[ 0, 1, 2, 3] [-1, 0, 1, 2] [ 0, -1, 0, 1] [ 0, 0, -1, 0]],tf.matrix_band_part(input, 2, 1) ==&gt; [[ 0, 1, 0, 0] [-1, 0, 1, 0] [-2, -1, 0, 1] [ 0, -2, -1, 0]]]]></content>
      <categories>
        <category>TensorFlow</category>
      </categories>
      <tags>
        <tag>深度学习框架</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LearningRate模型的学习率]]></title>
    <url>%2F2018%2F11%2F06%2FLearningRate%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%2F</url>
    <content type="text"><![CDATA[LearningRate 在使用不同优化器（例如随机梯度下降，Adam）神经网络相关训练中，学习速率作为一个超参数控制了权重更新的幅度，以及训练的速度和精度。学习速率太大容易导致目标（代价）函数波动较大从而难以找到最优，而弱学习速率设置太小，则会导致收敛过慢耗时太长 LearningRate 详细解读]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LearningRate</tag>
        <tag>学习率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention is All You Need]]></title>
    <url>%2F2018%2F11%2F06%2FAttention_is_All_You_Need%2F</url>
    <content type="text"><![CDATA[序列到序列任务与Transformer模型序列到序列任务与Encoder-Decoder框架序列到序列（Sequence-to-Sequence）是自然语言处理中的一个常见任务，主要用来做泛文本生成的任务，像机器翻译、文本摘要、歌词/故事生成、对话机器人等。最具有代表性的一个任务就是机器翻译（Machine Translation），将一种语言的序列映射到另一个语言的序列。例如，在汉-英机器翻译任务中，模型要将一个汉语句子（词序列）转化成一个英语句子（词序列）。 目前Encoder-Decoder框架是解决序列到序列问题的一个主流模型。模型使用Encoder对source sequence进行压缩表示，使用Decoder基于源端的压缩表示生成target sequence。该结构的好处是可以实现两个sequence之间end-to-end方式的建模，模型中所有的参数变量统一到一个目标函数下进行训练，模型表现较好。图1展示了Encoder-Decoder模型的结构，从底向上是一个机器翻译的过程。Encoder和Decoder可以选用不同结构的Neural Network，比如RNN、CNN。RNN的工作方式是对序列根据时间步，依次进行压缩表示。使用RNN的时候，一般会使用双向的RNN结构。具体方式是使用一个RNN对序列中的元素进行从左往右的压缩表示，另一个RNN对序列进行从右向左的压缩表示。两种表示被联合起来使用，作为最终序列的分布式表示。使用CNN结构的时候，一般使用多层的结构，来实现序列局部表示到全局表示的过程。使用RNN建模句子可以看做是一种时间序列的观点，使用CNN建模句子可以看做一种结构化的观点。使用RNN结构的序列到序列模型主要包括RNNSearch、GNMT等，使用CNN结构的序列到序列模型主要有ConvS2S等。 神经网络模型与语言距离依赖现象Transformer是一种建模序列的新方法，序列到序列的模型依然是沿用了上述经典的Encoder-Decoder结构，不同的是不再使用RNN或是CNN作为序列建模机制了，而是使用了self-attention机制。这种机制理论上的优势就是更容易捕获“长距离依赖信息（long distance dependency）”。所谓的“长距离依赖信息”可以这么来理解：1）一个词其实是一个可以表达多样性语义信息的符号（歧义问题）。2）一个词的语义确定，要依赖其所在的上下文环境。（根据上下文消岐）3）有的词可能需要一个范围较小的上下文环境就能确定其语义（短距离依赖现象），有的词可能需要一个范围较大的上下文环境才能确定其语义（长距离依赖现象）。 举个例子，看下面两句话：“山上有很多杜鹃，春天到了的时候，会漫山遍野的开放，非常美丽。” “山上有很多杜鹃，春天到了的时候，会漫山遍野的啼鸣，非常婉转。”在这两句话中，“杜鹃”分别指花（azalea）和鸟（cuckoo）。在机器翻译问题中，如果不看距其比较远的距离的词，很难将“杜鹃”这个词翻译正确。该例子是比较明显的一个例子，可以明显的看到词之间的远距离依赖关系。当然，绝大多数的词义在一个较小范围的上下文语义环境中就可以确定，像上述的例子在语言中占的比例会相对较小。我们期望的是模型既能够很好的学习到短距离的依赖知识，也能够学习到长距离依赖的知识。 那么，为什么Transformer中的self-attention理论上能够更好的捕获这种长短距离的依赖知识呢？我们直观的来看一下，基于RNN、CNN、self-attention的三种序列建模方法，任意两个词之间的交互距离上的区别。图2是一个使用双向RNN来对序列进行建模的方法。由于是对序列中的元素按顺序处理的，两个词之间的交互距离可以认为是他们之间的相对距离。W1和Wn之间的交互距离是n-1。带有门控（Gate）机制的RNN模型理论上可以对历史信息进行有选择的存储和遗忘，具有比纯RNN结构更好的表现，但是门控参数量一定的情况下，这种能力是一定的。随着句子的增长，相对距离的增大，存在明显的理论上限。图3展示了使用多层CNN对序列进行建模的方法。第一层的CNN单元覆盖的语义环境范围较小，第二层覆盖的语义环境范围会变大，依次类推，越深层的CNN单元，覆盖的语义环境会越大。一个词首先会在底层CNN单元上与其近距离的词产生交互，然后在稍高层次的CNN单元上与其更远一些词产生交互。所以，多层的CNN结构体现的是一种从局部到全局的特征抽取过程。词之间的交互距离，与他们的相对距离成正比。距离较远的词只能在较高的CNN节点上相遇，才产生交互。这个过程可能会存在较多的信息丢失。图4展示的是基于self-attention机制的序列建模方法。注意，为了使图展示的更清晰，少画了一些连接线，图中“sentence”层中的每个词和第一层self-attention layer中的节点都是全连接的关系，第一层self-attention layer和第二层self-attention layer之间的节点也都是全连接的关系。我们可以看到在这种建模方法中，任意两个词之间的交互距离都是1，与词之间的相对距离不存在关系。这种方式下，每个词的语义的确定，都考虑了与整个句子中所有的词的关系。多层的self-attention机制，使得这种全局交互变的更加复杂，能够捕获到更多的信息。综上，self-attention机制在建模序列问题时，能够捕获长距离依赖知识，具有更好的理论基础。 Attention Is All You NeedAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin(Submitted on 12 Jun 2017 (v1), last revised 6 Dec 2017 (this version, v5)) The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments: 15 pages, 5 figuresSubjects: Computation and Language (cs.CL); Machine Learning (cs.LG)Cite as: arXiv:1706.03762 [cs.CL] (or arXiv:1706.03762v5 [cs.CL] for this version) 论文模型图片 论文结果可视化 论文原文 seq2seq_example编码器 - 解码器架构 - ：编码器将源句子转换为“含义”向量，该向量通过解码器以产生翻译。 标题 说明 附加 Attention Is All You Need 原始论文 20170612 The Annotated Transformer harvard NLP 解读原文 20180403 Transformer Translation Model TensorFlow 官方模型复现 长期更新 Transformer Translation Model TensorFlow NVIDIA模型复现 长期更新 attention-is-all-you-need-keras Keras 论文复现 201807 [Attention Is All You Need code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.] 原始论文模型评估 基于注意力机制，机器之心带你理解与训练神经机器翻译系统 复现论文+ 解析 20180512 大规模集成Transformer模型，阿里达摩院如何打造WMT 2018机器翻译获胜系统 论文实际应用 201806 attention_keras.py 论文注意力机制（部分）复现 20180528 “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统 Google Tensor2Tensor系统是一套十分强大的深度学习系统，在多个任务上的表现非常抢眼。尤其在机器翻译问题上，单模型的表现就可以超过之前方法的集成模型。这一套系统的模型结构、训练和优化技巧等，可以被利用到公司的产品线上，直接转化成生产力。本文对Tensor2Tensor系统从模型到代码进行了全面的解析，期望能够给大家提供有用的信息。 20181106 NMT-KerasAttentional recurrent neural network NMT model Transformer NMT model 解析attention_keras.pyattention_keras 源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#! -*- coding: utf-8 -*-from keras import backend as Kfrom keras.engine.topology import Layerclass Position_Embedding(Layer): def __init__(self, size=None, mode='sum', **kwargs): self.size = size #必须为偶数 self.mode = mode super(Position_Embedding, self).__init__(**kwargs) def call(self, x): if (self.size == None) or (self.mode == 'sum'): self.size = int(x.shape[-1]) batch_size,seq_len = K.shape(x)[0],K.shape(x)[1] position_j = 1. / K.pow(10000., \ 2 * K.arange(self.size / 2, dtype='float32' \ ) / self.size) position_j = K.expand_dims(position_j, 0) position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成 position_i = K.expand_dims(position_i, 2) position_ij = K.dot(position_i, position_j) position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2) if self.mode == 'sum': return position_ij + x elif self.mode == 'concat': return K.concatenate([position_ij, x], 2) def compute_output_shape(self, input_shape): if self.mode == 'sum': return input_shape elif self.mode == 'concat': return (input_shape[0], input_shape[1], input_shape[2]+self.size)class Attention(Layer): def __init__(self, nb_head, size_per_head, **kwargs): self.nb_head = nb_head self.size_per_head = size_per_head self.output_dim = nb_head*size_per_head super(Attention, self).__init__(**kwargs) def build(self, input_shape): self.WQ = self.add_weight(name='WQ', shape=(input_shape[0][-1], self.output_dim), initializer='glorot_uniform', trainable=True) self.WK = self.add_weight(name='WK', shape=(input_shape[1][-1], self.output_dim), initializer='glorot_uniform', trainable=True) self.WV = self.add_weight(name='WV', shape=(input_shape[2][-1], self.output_dim), initializer='glorot_uniform', trainable=True) super(Attention, self).build(input_shape) def Mask(self, inputs, seq_len, mode='mul'): if seq_len == None: return inputs else: mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1]) mask = 1 - K.cumsum(mask, 1) for _ in range(len(inputs.shape)-2): mask = K.expand_dims(mask, 2) if mode == 'mul': return inputs * mask if mode == 'add': return inputs - (1 - mask) * 1e12 def call(self, x): #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask if len(x) == 3: Q_seq,K_seq,V_seq = x Q_len,V_len = None,None elif len(x) == 5: Q_seq,K_seq,V_seq,Q_len,V_len = x #对Q、K、V做线性变换 Q_seq = K.dot(Q_seq, self.WQ) Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head)) Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3)) K_seq = K.dot(K_seq, self.WK) K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head)) K_seq = K.permute_dimensions(K_seq, (0,2,1,3)) V_seq = K.dot(V_seq, self.WV) V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head)) V_seq = K.permute_dimensions(V_seq, (0,2,1,3)) #计算内积，然后mask，然后softmax A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5 A = K.permute_dimensions(A, (0,3,2,1)) A = self.Mask(A, V_len, 'add') A = K.permute_dimensions(A, (0,3,2,1)) A = K.softmax(A) #输出并mask O_seq = K.batch_dot(A, V_seq, axes=[3,2]) O_seq = K.permute_dimensions(O_seq, (0,2,1,3)) O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim)) O_seq = self.Mask(O_seq, Q_len, 'mul') return O_seq def compute_output_shape(self, input_shape): return (input_shape[0][0], input_shape[0][1], self.output_dim) 多头attention维度变换查看1from keras import backend as K Using TensorFlow backend. 12nb_head = 8size_per_head = 16 12output_dim = nb_head * size_per_headoutput_dim 128 1batch_size = 10 对Q、K、V做线性变换1Q_seq = K.placeholder(shape=(batch_size, 80, 30)) 123WQ = K.placeholder(shape=(30 ,128))WK = K.placeholder(shape=(30 ,128))WV = K.placeholder(shape=(30 ,128)) 12Q_seq = K.dot(Q_seq, WQ)K.int_shape(Q_seq) (10, 80, 128) 12Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], nb_head, size_per_head))K.int_shape(Q_seq) (10, 80, 8, 16) 12Q_seq = K.permute_dimensions(Q_seq, (0, 2, 1, 3))K.int_shape(Q_seq) (10, 8, 80, 16) 同理123Q_seq = K.placeholder(shape=(batch_size, 8, 80, 16))K_seq = K.placeholder(shape=(batch_size, 8, 80, 16))V_seq = K.placeholder(shape=(batch_size, 8, 80, 16)) 计算内积，然后mask，然后softmax1A = K.batch_dot(Q_seq, K_seq, axes=[3, 3]) 1K.int_shape(A) (10, 8, 80, 80) 123A = K.softmax(A)K.int_shape(A) (10, 8, 80, 80) 12O_seq = K.batch_dot(A, V_seq, axes=[3, 2])K.int_shape(O_seq) (10, 8, 80, 16) 12O_seq = K.permute_dimensions(O_seq, (0, 2, 1, 3))K.int_shape(O_seq) (10, 80, 8, 16) 12O_seq = K.reshape(O_seq, (batch_size, 80, output_dim))K.int_shape(O_seq) (10, 80, 128) 使用123456789101112131415161718from keras.models import Modelfrom keras.layers import *S_inputs = Input(shape=(None,), dtype='int32')embeddings = Embedding(max_features, 30)(S_inputs)embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率O_seq = Attention(8,16)([embeddings,embeddings,embeddings])O_seq = GlobalAveragePooling1D()(O_seq)O_seq = Dropout(0.15)(O_seq)outputs = Dense(1, activation='sigmoid')(O_seq)model = Model(inputs=S_inputs, outputs=outputs)# try using different optimizers and different optimizer configsmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])plot_model(model, to_file='imdb_attention.png', show_shapes=True) 模型图]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>Sequence-to-Sequence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformer 代码实现]]></title>
    <url>%2F2018%2F11%2F04%2FTransformer%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[通过使用 TensorFlow 对 Attention Is All You Need 中的 Transformer 进行复现，从而真正理解论文，为进一步使用 Transformer 打下基础。 标题 说明 附加 Attention Is All You Need 原始论文 20170612 The Illustrated Transformer Transformer基础解读 Attention Is All You NeedAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin(Submitted on 12 Jun 2017 (v1), last revised 6 Dec 2017 (this version, v5)) The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Comments: 15 pages, 5 figuresSubjects: Computation and Language (cs.CL); Machine Learning (cs.LG)Cite as: arXiv:1706.03762 [cs.CL] (or arXiv:1706.03762v5 [cs.CL] for this version) 代码来自复现 bert_language_understanding multi_head_attention.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128# -*- coding: utf-8 -*-#test self-attentionimport tensorflow as tfimport time"""multi head attention.1.linearly project the queries,keys and values h times(with different,learned linear projections to d_k,d_k,d_v dimensions)2.scaled dot product attention for each projected version of Q,K,V3.concatenated result4.linear projection to get final resultthree kinds of usage:1. attention for encoder2. attention for decoder(need a mask to pay attention for only known position)3. attention as bridge of encoder and decoder"""class MultiHeadAttention(object): """ multi head attention""" def __init__(self,Q,K_s,V_s,d_model,d_k,d_v,sequence_length,h,typee=None,is_training=None,mask=None,dropout_rate=0.1): self.d_model=d_model self.d_k=d_k self.d_v=d_v self.sequence_length=sequence_length self.h=h self.Q=Q self.K_s=K_s self.V_s=V_s self.typee=typee self.is_training=is_training self.mask=mask self.dropout_rate=dropout_rate #print("MultiHeadAttention.self.dropout_rate:",self.dropout_rate) def multi_head_attention_fn(self): """ multi head attention :param Q: query. shape:[batch,sequence_length,d_model] :param K_s: keys. shape:[batch,sequence_length,d_model]. :param V_s:values.shape:[batch,sequence_length,d_model]. :param h: h times :return: result of scaled dot product attention. shape:[sequence_length,d_model] """ # 1. linearly project the queries,keys and values h times(with different,learned linear projections to d_k,d_k,d_v dimensions) Q_projected = tf.layers.dense(self.Q,units=self.d_model) # [batch,sequence_length,d_model] K_s_projected = tf.layers.dense(self.K_s, units=self.d_model) # [batch,sequence_length,d_model] V_s_projected = tf.layers.dense(self.V_s, units=self.d_model) # [batch,sequence_length,d_model] # 2. scaled dot product attention for each projected version of Q,K,V dot_product=self.scaled_dot_product_attention_batch(Q_projected,K_s_projected,V_s_projected) # [batch,h,sequence_length,d_v] # 3. concatenated batch_size,h,length,d_v=dot_product.get_shape().as_list() #print("dot_product:",dot_product,";self.sequence_length:",self.sequence_length) ##dot_product:(128, 8, 6, 64);5 dot_product=tf.reshape(dot_product,shape=(-1,length,self.d_model)) # [batch,sequence_length,d_model] # 4. linear projection output=tf.layers.dense(dot_product,units=self.d_model) # [batch,sequence_length,d_model] return output #[batch,sequence_length,d_model] def scaled_dot_product_attention_batch(self, Q, K_s, V_s):# scaled dot product attention: implementation style like tensor2tensor from google """ scaled dot product attention :param Q: query. shape:[batch,sequence_length,d_model] :param K_s: keys. shape:[batch,sequence_length,d_model] :param V_s:values. shape:[batch,sequence_length,d_model] :param mask: shape:[sequence_length,sequence_length] :return: result of scaled dot product attention. shape:[batch,h,sequence_length,d_k] """ # 1. split Q,K,V #K_s=tf.layers.dense(K_s,self.d_model) # transform K_s, while keep as shape. TODO add 2018.10.21. so that Q and K shoud be not the same. Q_heads = tf.stack(tf.split(Q,self.h,axis=2),axis=1) # [batch,h,sequence_length,d_k] K_heads = tf.stack(tf.split(K_s, self.h, axis=2), axis=1) # [batch,h,sequence_length,d_k] V_heads = tf.stack(tf.split(V_s, self.h, axis=2), axis=1) # [batch,h,sequence_length,d_v]. during implementation, d_v=d_k. # 2. dot product of Q,K dot_product=tf.matmul(Q_heads,K_heads,transpose_b=True) # [batch,h,sequence_length,sequence_length] dot_product=dot_product*(1.0/tf.sqrt(tf.cast(self.d_model,tf.float32))) # [batch,h,sequence_length,sequence_length] # 3. add mask if it is none #print("scaled_dot_product_attention_batch.mask is not none?",self.mask is not None) if self.mask is not None: mask_expand=tf.expand_dims(tf.expand_dims(self.mask,axis=0),axis=0) # [1,1,sequence_length,sequence_length] #dot_product:(128, 8, 6, 6);mask_expand:(1, 1, 6, 6) #print("scaled_dot_product_attention_batch.dot_product:",dot_product,";mask_expand:",mask_expand) dot_product=dot_product+mask_expand # [batch,h,sequence_length,sequence_length] # 4.get possibility weights=tf.nn.softmax(dot_product) # [batch,h,sequence_length,sequence_length] # drop out weights weights=tf.nn.dropout(weights,1.0-self.dropout_rate) # [batch,h,sequence_length,sequence_length] # 5. final output output=tf.matmul(weights,V_heads) # [batch,h,sequence_length,d_v] return output#vectorized implementation of multi head attention for sentences with batchdef multi_head_attention_for_sentence_vectorized(layer_number): print("started...") start = time.time() # 1.set parameter d_model = 512 d_k = 64 d_v = 64 sequence_length = 1000 h = 8 batch_size=128 initializer = tf.random_normal_initializer(stddev=0.1) # 2.set Q,K,V vocab_size=1000 embed_size=d_model typee='decoder' Embedding = tf.get_variable("Embedding_", shape=[vocab_size, embed_size],initializer=initializer) input_x = tf.placeholder(tf.int32, [batch_size,sequence_length], name="input_x") embedded_words = tf.nn.embedding_lookup(Embedding, input_x) #[batch_size,sequence_length,embed_size] mask=get_mask(batch_size,sequence_length) #tf.ones((batch_size,sequence_length))*-1e8 #[batch,sequence_length] with tf.variable_scope("query_at_each_sentence"+str(layer_number)): Q = embedded_words # [batch_size*sequence_length,embed_size] K_s=embedded_words #[batch_size*sequence_length,embed_size] V_s=embedded_words #tf.get_variable("V_s_original_", shape=embedded_words.get_shape().as_list(),initializer=initializer) #[batch_size,sequence_length,embed_size] # 3.call method to get result multi_head_attention_class = MultiHeadAttention(Q, K_s, V_s, d_model, d_k, d_v, sequence_length, h,typee='decoder',mask=mask) encoder_output=multi_head_attention_class.multi_head_attention_fn() #shape:[sequence_length,d_model] encoder_output=tf.reshape(encoder_output,shape=(batch_size,sequence_length,d_model)) end = time.time() print("input_x:",input_x) print("encoder_output:",encoder_output,";time_spent:",(end-start))def get_mask(batch_size,sequence_length): lower_triangle=tf.matrix_band_part(tf.ones([sequence_length,sequence_length]),-1,0) result=-1e9*(1.0-lower_triangle) print("get_mask==&gt;result:",result) return resultlayer_number=0#multi_head_attention_for_sentence_vectorized(0) poistion_wise_feed_forward.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104# -*- coding: utf-8 -*-import tensorflow as tfimport time"""Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fullyconnected feed-forward network, which is applied to each position separately and identically. Thisconsists of two linear transformations with a ReLU activation in between.FFN(x) = max(0,xW1+b1)W2+b2While the linear transformations are the same across different positions, they use different parametersfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.The dimensionality of input and output is d_model= 512, and the inner-layer has dimensionalityd_ff= 2048."""class PositionWiseFeedFoward(object): """ position-wise feed forward networks. formula as below: FFN(x)=max(0,xW1+b1)W2+b2 """ def __init__(self,x,layer_index,d_model=512,d_ff=2048): """ :param x: shape should be:[batch,sequence_length,d_model] :param layer_index: index of layer :return: shape:[sequence_length,d_model] """ shape_list=x.get_shape().as_list() assert(len(shape_list)==3) self.x=x self.layer_index=layer_index self.d_model=d_model self.d_ff=d_ff self.initializer = tf.random_normal_initializer(stddev=0.1) def position_wise_feed_forward_fn(self): """ positional wise fully connected feed forward implement as two layers of cnn x: [batch,sequence_length,d_model] :return: [batch,sequence_length,d_model] """ # 1.conv layer 1 input=tf.expand_dims(self.x,axis=3) # [batch,sequence_length,d_model,1] # conv2d.input: [batch,sentence_length,embed_size,1]. filter=[filter_size,self.embed_size,1,self.num_filters] output_conv1=tf.layers.conv2d( # output_conv1: [batch_size,sequence_length,1,d_ff] input,filters=self.d_ff,kernel_size=[1,self.d_model],padding="VALID", name='conv1',kernel_initializer=self.initializer,activation=tf.nn.relu ) output_conv1 = tf.transpose(output_conv1, [0,1,3,2]) #output_conv1:[batch_size,sequence_length,d_ff,1] # print("output_conv1:",output_conv1) # 2.conv layer 2 output_conv2 = tf.layers.conv2d( # output_conv2:[batch_size, sequence_length,1,d_model] output_conv1,filters=self.d_model,kernel_size=[1,self.d_ff],padding="VALID", name='conv2',kernel_initializer=self.initializer,activation=None ) output=tf.squeeze(output_conv2) #[batch,sequence_length,d_model] return output #[batch,sequence_length,d_model] def position_wise_feed_forward_fc_fn(self): """ positional wise fully connected feed forward implement as original version. FFN(x) = max(0,xW1+b1)W2+b2 this function provide you as an alternative if you want to use original version, or you don't want to use two layers of cnn, but may be less efficient as sequence become longer. x: [batch,sequence_length,d_model] :return: [batch,sequence_length,d_model] """ # 0. pre-process input x _,sequence_length,d_model=self.x.get_shape().as_list() element_list = tf.split(self.x, sequence_length,axis=1) # it is a list,length is sequence_length, each element is [batch_size,1,d_model] element_list = [tf.squeeze(element, axis=1) for element in element_list] # it is a list,length is sequence_length, each element is [batch_size,d_model] output_list=[] for i, element in enumerate(element_list): with tf.variable_scope("foo", reuse=True if i&gt;0 else False): # 1. layer 1 W1 = tf.get_variable("ff_layer1", shape=[self.d_model, self.d_ff], initializer=self.initializer) z1=tf.nn.relu(tf.matmul(element,W1)) # z1:[batch_size,d_ff]&lt;--------tf.matmul([batch_size,d_model],[d_model, d_ff]) # 2. layer 2 W2 = tf.get_variable("ff_layer2", shape=[self.d_ff, self.d_model], initializer=self.initializer) output_element=tf.matmul(z1,W2) # output:[batch_size,d_model]&lt;----------tf.matmul([batch_size,d_ff],[d_ff, d_model]) output_list.append(output_element) # a list, each element is [batch_size,d_model] output=tf.stack(output_list,axis=1) # [batch,sequence_length,d_model] return output # [batch,sequence_length,d_model]#test function of position_wise_feed_forward_fn#time spent:OLD VERSION(FC): length=1000,time spent:2.04 s; NEW VERSION(CNN):0.03s, speed up as 68x.def test_position_wise_feed_forward_fn(): start=time.time() x=tf.ones((8,1000,512)) #batch_size=8,sequence_length=10 ; layer_index=0 postion_wise_feed_forward=PositionWiseFeedFoward(x,layer_index) output=postion_wise_feed_forward.position_wise_feed_forward_fn() end=time.time() print("x:",x.shape,";output:",output.shape) print("time spent:",(end-start)) return outputdef test(): with tf.Session() as sess: result=test_position_wise_feed_forward_fn() sess.run(tf.global_variables_initializer()) result_=sess.run(result) print("result_.shape:",result_.shape)#test() layer_norm_residual_conn.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import tensorflow as tfimport time"""We employ a residual connection around each of the two sub-layers, followed by layer normalization.That is, the output of each sub-layer is LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. """class LayerNormResidualConnection(object): def __init__(self,x,y,layer_index,residual_dropout=0.1,use_residual_conn=True): self.x=x self.y=y self.layer_index=layer_index self.residual_dropout=residual_dropout #print("LayerNormResidualConnection.residual_dropout:",self.residual_dropout) self.use_residual_conn=use_residual_conn #call residual connection and layer normalization def layer_norm_residual_connection(self): #print("LayerNormResidualConnection.use_residual_conn:",self.use_residual_conn) if self.use_residual_conn: # todo previously it is removed in a classification task, may be because result become not stable x_residual=self.residual_connection() x_layer_norm=self.layer_normalization(x_residual) else: x_layer_norm = self.layer_normalization(self.x) return x_layer_norm def residual_connection(self): output=self.x + tf.nn.dropout(self.y, 1.0 - self.residual_dropout) return output # layer normalize the tensor x, averaging over the last dimension. def layer_normalization(self,x): """ x should be:[batch_size,sequence_length,d_model] :return: """ filter=x.get_shape()[-1] #last dimension of x. e.g. 512 #print("layer_normalization:==================&gt;variable_scope:","layer_normalization"+str(self.layer_index)) with tf.variable_scope("layer_normalization"+str(self.layer_index)): # 1. normalize input by using mean and variance according to last dimension mean=tf.reduce_mean(x,axis=-1,keepdims=True) #[batch_size,sequence_length,1] variance=tf.reduce_mean(tf.square(x-mean),axis=-1,keepdims=True) #[batch_size,sequence_length,1] norm_x=(x-mean)*tf.rsqrt(variance+1e-6) #[batch_size,sequence_length,d_model] # 2. re-scale normalized input back scale=tf.get_variable("layer_norm_scale",[filter],initializer=tf.ones_initializer) #[filter] bias=tf.get_variable("layer_norm_bias",[filter],initializer=tf.ones_initializer) #[filter] output=norm_x*scale+bias #[batch_size,sequence_length,d_model] return output #[batch_size,sequence_length,d_model]def test(): start = time.time() batch_size=128 sequence_length=1000 d_model=512 x=tf.ones((batch_size,sequence_length,d_model)) y=x*3-0.5 layer_norm_residual_conn=LayerNormResidualConnection(x,y,0) output=layer_norm_residual_conn.layer_norm_residual_connection() end = time.time() print("x:",x,";y:",y) print("output:",output,";time spent:",(end-start))#test() base_model.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374# -*- coding: utf-8 -*-import tensorflow as tffrom model.multi_head_attention import MultiHeadAttentionfrom model.poistion_wise_feed_forward import PositionWiseFeedFowardfrom model.layer_norm_residual_conn import LayerNormResidualConnectionclass BaseClass(object): """ base class has some common fields and functions. """ def __init__(self,d_model,d_k,d_v,sequence_length,h,batch_size,num_layer=6,decoder_sent_length=None): """ :param d_model: :param d_k: :param d_v: :param sequence_length: :param h: :param batch_size: :param embedded_words: shape:[batch_size,sequence_length,embed_size] """ self.d_model=d_model self.d_k=d_k self.d_v=d_v self.sequence_length=sequence_length self.h=h self.num_layer=num_layer self.batch_size=batch_size self.decoder_sent_length=decoder_sent_length def sub_layer_postion_wise_feed_forward(self, x, layer_index) :# COMMON FUNCTION """ position-wise feed forward. you can implement it as feed forward network, or two layers of CNN. :param x: shape should be:[batch_size,sequence_length,d_model] :param layer_index: index of layer number :return: [batch_size,sequence_length,d_model] """ # use variable scope here with input of layer index, to make sure each layer has different parameters. with tf.variable_scope("sub_layer_postion_wise_feed_forward" + str(layer_index)): postion_wise_feed_forward = PositionWiseFeedFoward(x, layer_index,d_model=self.d_model,d_ff=self.d_model*4) postion_wise_feed_forward_output = postion_wise_feed_forward.position_wise_feed_forward_fn() return postion_wise_feed_forward_output def sub_layer_multi_head_attention(self ,layer_index ,Q ,K_s,V_s,mask=None,is_training=None,dropout_keep_prob=0.9) :# COMMON FUNCTION """ multi head attention as sub layer :param layer_index: index of layer number :param Q: shape should be: [batch_size,sequence_length,embed_size] :param k_s: shape should be: [batch_size,sequence_length,embed_size] :param mask: when use mask,illegal connection will be mask as huge big negative value.so it's possiblitity will become zero. :return: output of multi head attention.shape:[batch_size,sequence_length,d_model] """ #print("sub_layer_multi_head_attention.",";layer_index:",layer_index) with tf.variable_scope("base_mode_sub_layer_multi_head_attention_" +str(layer_index)): #2. call function of multi head attention to get result multi_head_attention_class = MultiHeadAttention(Q, K_s, V_s, self.d_model, self.d_k, self.d_v, self.sequence_length,self.h, is_training=is_training,mask=mask,dropout_rate=(1.0-dropout_keep_prob)) sub_layer_multi_head_attention_output = multi_head_attention_class.multi_head_attention_fn() # [batch_size*sequence_length,d_model] return sub_layer_multi_head_attention_output # [batch_size,sequence_length,d_model] def sub_layer_layer_norm_residual_connection(self,layer_input ,layer_output,layer_index,dropout_keep_prob=0.9,use_residual_conn=True,sub_layer_name='layer1'): # COMMON FUNCTION """ layer norm &amp; residual connection :param input: [batch_size,equence_length,d_model] :param output:[batch_size,sequence_length,d_model] :return: """ #print("sub_layer_layer_norm_residual_connection.layer_input:",layer_input,";layer_output:",layer_output,";dropout_keep_prob:",dropout_keep_prob) #assert layer_input.get_shape().as_list()==layer_output.get_shape().as_list() #layer_output_new= layer_input+ layer_output variable_scope="sub_layer_layer_norm_residual_connection_" +str(layer_index)+'_'+sub_layer_name #print("######sub_layer_layer_norm_residual_connection.variable_scope:",variable_scope) with tf.variable_scope(variable_scope): layer_norm_residual_conn=LayerNormResidualConnection(layer_input,layer_output,layer_index,residual_dropout=(1-dropout_keep_prob),use_residual_conn=use_residual_conn) output = layer_norm_residual_conn.layer_norm_residual_connection() return output # [batch_size,sequence_length,d_model] encoder.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# -*- coding: utf-8 -*-"""encoder for the transformer:6 layers.each layers has two sub-layers.the first is multi-head self-attention mechanism;the second is position-wise fully connected feed-forward network.for each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512."""import tensorflow as tffrom model.base_model import BaseClassimport timeclass Encoder(BaseClass): def __init__(self,d_model,d_k,d_v,sequence_length,h,batch_size,num_layer,Q,K_s,mask=None,dropout_keep_prob=0.9,use_residual_conn=True): """ :param d_model: :param d_k: :param d_v: :param sequence_length: :param h: :param batch_size: :param embedded_words: shape:[batch_size*sequence_length,embed_size] """ super(Encoder, self).__init__(d_model,d_k,d_v,sequence_length,h,batch_size,num_layer=num_layer) self.Q=Q self.K_s=K_s self.mask=mask self.initializer = tf.random_normal_initializer(stddev=0.1) self.dropout_keep_prob=dropout_keep_prob self.use_residual_conn=use_residual_conn def encoder_fn(self): """ use transformer encoder to encode the input, output a sequence. input: [batch_size,sequence_length,d_embedding] :return: output:[batch_size*sequence_length,d_model] """ start = time.time() #print("encoder_fn.started.") x=self.Q for layer_index in range(self.num_layer): x=self.encoder_single_layer(x,x,x,layer_index) # Q,K_s,V_s #print("encoder_fn.",layer_index,".x:",x) end = time.time() #print("encoder_fn.ended.x:",x) #print("time spent:",(end-start)) return x def encoder_single_layer(self,Q,K_s,V_s,layer_index): """ singel layer for encoder.each layers has two sub-layers: the first is multi-head self-attention mechanism; the second is position-wise fully connected feed-forward network. for each sublayer. use LayerNorm(x+Sublayer(x)). input and output of last dimension: d_model :param Q: shape should be: [batch_size,sequence_length,d_model] :param K_s: shape should be: [batch_size,sequence_length,d_model] :return:output: shape should be: [batch_size,sequence_length,d_model] """ #1.1 the first is multi-head self-attention mechanism multi_head_attention_output=self.sub_layer_multi_head_attention(layer_index,Q,K_s,V_s,mask=self.mask,dropout_keep_prob=self.dropout_keep_prob) #[batch_size,sequence_length,d_model] #1.2 use LayerNorm(x+Sublayer(x)). all dimension=512. multi_head_attention_output=self.sub_layer_layer_norm_residual_connection(K_s,multi_head_attention_output,layer_index, dropout_keep_prob=self.dropout_keep_prob,use_residual_conn=self.use_residual_conn,sub_layer_name='layer1') #2.1 the second is position-wise fully connected feed-forward network. postion_wise_feed_forward_output=self.sub_layer_postion_wise_feed_forward(multi_head_attention_output,layer_index) #2.2 use LayerNorm(x+Sublayer(x)). all dimension=512. postion_wise_feed_forward_output= self.sub_layer_layer_norm_residual_connection(multi_head_attention_output,postion_wise_feed_forward_output,layer_index, dropout_keep_prob=self.dropout_keep_prob,sub_layer_name='layer2') return postion_wise_feed_forward_output #,postion_wise_feed_forward_outputdef init(): #1. assign value to fields vocab_size=1000 d_model = 512 d_k = 64 d_v = 64 sequence_length = 5*10 h = 8 batch_size=4*32 initializer = tf.random_normal_initializer(stddev=0.1) # 2.set values for Q,K,V vocab_size=1000 embed_size=d_model Embedding = tf.get_variable("Embedding_E", shape=[vocab_size, embed_size],initializer=initializer) input_x = tf.placeholder(tf.int32, [batch_size,sequence_length], name="input_x") #[4,10] print("input_x:",input_x) embedded_words = tf.nn.embedding_lookup(Embedding, input_x) #[batch_size*sequence_length,embed_size] Q = embedded_words # [batch_size*sequence_length,embed_size] K_s = embedded_words # [batch_size*sequence_length,embed_size] V_s = embedded_words # [batch_size*sequence_length,embed_size] num_layer=6 mask = get_mask(batch_size, sequence_length) #3. get class object encoder_class=Encoder(d_model,d_k,d_v,sequence_length,h,batch_size,num_layer,Q,K_s,mask=mask) #Q,K_s,embedded_words return encoder_class,Q,K_s,V_sdef get_mask(batch_size,sequence_length): lower_triangle=tf.matrix_band_part(tf.ones([sequence_length,sequence_length]),-1,0) result=-1e9*(1.0-lower_triangle) print("get_mask==&gt;result:",result) return resultdef test_postion_wise_feed_forward(encoder_class,x,layer_index): sub_layer_postion_wise_feed_forward_output=encoder_class.sub_layer_postion_wise_feed_forward(x, layer_index) return sub_layer_postion_wise_feed_forward_outputdef test_sub_layer_multi_head_attention(encoder_class,index_layer,Q,K_s,V_s): sub_layer_multi_head_attention_output=encoder_class.sub_layer_multi_head_attention(index_layer,Q,K_s,V_s) return sub_layer_multi_head_attention_outputencoder_class,Q,K_s,V_s=init()#below is 4 callable codes for testing functions: from sub(small) function to whole function of encoder.def test(): #1.test 1: for sub layer of multi head attention index_layer=0 #sub_layer_multi_head_attention_output=test_sub_layer_multi_head_attention(encoder_class,index_layer,Q,K_s,V_s) #print("sub_layer_multi_head_attention_output1:",sub_layer_multi_head_attention_output) #2. test 2: for sub layer of multi head attention with poistion-wise feed forward #d1,d2,d3=sub_layer_multi_head_attention_output.get_shape().as_list() #print("d1:",d1,";d2:",d2,";d3:",d3) #postion_wise_ff_input=sub_layer_multi_head_attention_output #tf.reshape(sub_layer_multi_head_attention_output,shape=[-1,d3]) #print("sub_layer_postion_wise_feed_forward_input:",postion_wise_ff_input) #sub_layer_postion_wise_feed_forward_output=test_postion_wise_feed_forward(encoder_class,postion_wise_ff_input,index_layer) #sub_layer_postion_wise_feed_forward_output=tf.reshape(sub_layer_postion_wise_feed_forward_output,shape=(d1,d2,d3)) #print("sub_layer_postion_wise_feed_forward_output:",sub_layer_postion_wise_feed_forward_output) #3.test 3: test for single layer of encoder #encoder_class.encoder_single_layer(Q,K_s,V_s,index_layer) #4.test 4: test for encoder. with N layers representation = encoder_class.encoder_fn() print("representation:",representation)#test()]]></content>
      <categories>
        <category>论文</category>
        <category>论文实现</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semi-supervised sequence tagging with bidirectional language models]]></title>
    <url>%2F2018%2F10%2F30%2FSemi_Supervised_sequence_tagging_with_bidirectional_language_models%2F</url>
    <content type="text"><![CDATA[本文 是 NAACL 2018 最佳论文 Deep contextualized word representations 的前作，详细介绍了一种用预训练的双向语言模型提高其它模型（序列标注）效果的半监督方法。 Semi-supervised sequence tagging with bidirectional language modelsMatthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power(Submitted on 29 Apr 2017) Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers. Comments: To appear in ACL 2017Subjects: Computation and Language (cs.CL)Cite as: arXiv:1705.00108 [cs.CL] (or arXiv:1705.00108v1 [cs.CL] for this version) 标题 说明 附加 Semi-supervised sequence tagging with bidirectional language models 原文 20170429 知乎 Semi-supervised sequence tagging with bidirectional language models 吴明昊 解读 《Semi-supervised sequence tagging with bidirectional language models》阅读笔记 Shen 20170605]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>word representations</tag>
        <tag>Language Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Embedding 和语言模型 LanguageModel]]></title>
    <url>%2F2018%2F10%2F30%2F%E8%AF%8D%E5%B5%8C%E5%85%A5Embedding%E5%92%8C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLanguageModel%2F</url>
    <content type="text"><![CDATA[ELMo Deep contextualized word representations 的目标也仅仅是学习到上下文相关的、更强大的词向量，其目的依然是为下游任务提供一个扎实的根基，还没有想要弑君称王的意思。 而我们知道，仅仅是对文本进行充分而强大的encoding（即得到每个词位非常精准丰富的特征）是远不够覆盖所有NLP任务的。在QA、机器阅读理解（MRC）、自然语言推理（NLI）、对话等任务中，还有很多更复杂的模式需要捕捉，比如句间关系。为此，下游任务中的网络会加入各种花式attention（参考NLI、MRC、Chatbot中的SOTA们）。 而随着捕捉更多神奇模式的需要，研究者们为每个下游任务定制出各种各样的网络结构，导致同一个模型，稍微一换任务就挂掉了，甚至在同一个任务的情况下换另一种分布的数据集都会出现显著的性能损失，这显然不符合人类的语言行为呀～要知道人类的generalization能力是非常强的，这就说明，或许现在整个NLP的发展轨迹就是错的，尤其是在SQuAD的带领下，穷尽各种trick和花式结构去刷榜，真正之于NLP的意义多大呢？ 不过所幸，这条越走越偏的道路终于被一个模型 shutdown了，那就是 Google 发布的 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 传统词嵌入的过程embedding_lookup()词嵌入的过程，注意其中的维度变化只是一种可能的方式。 123456789101112import tensorflow as tfimport numpy as npinput_ids = tf.placeholder(dtype=tf.int32, shape=[None])embedding = tf.Variable(np.identity(5, dtype=np.float32))input_embedding = tf.nn.embedding_lookup(embedding, input_ids)sess = tf.InteractiveSession()sess.run(tf.global_variables_initializer())print(embedding.eval())print(sess.run(input_embedding, feed_dict=&#123;input_ids:[1, 2, 3, 0, 3, 2, 1]&#125;)) 代码中先使用palceholder定义了一个未知变量input_ids用于存储索引，和一个已知变量embedding，是一个5*5的对角矩阵。运行结果为,123456789101112embedding = [[1. 0. 0. 0. 0.] [0. 1. 0. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.]]input_embedding = [[0. 1. 0. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 0. 1. 0.] [1. 0. 0. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 1. 0. 0.] [0. 1. 0. 0. 0.]] 简单的讲就是根据input_ids中的id，寻找embedding中的对应元素。比如，input_ids=[1,3,5]，则找出embedding中下标为1,3,5的向量组成一个矩阵返回。 如果将input_ids改写成下面的格式： 123456789101112import tensorflow as tfimport numpy as npinput_ids = tf.placeholder(dtype=tf.int32, shape=[None, None])embedding = tf.Variable(np.identity(5, dtype=np.float32))input_embedding = tf.nn.embedding_lookup(embedding, input_ids)sess = tf.InteractiveSession()sess.run(tf.global_variables_initializer())print(embedding.eval())print(sess.run(input_embedding, feed_dict=&#123;input_ids:[[1, 2], [2, 1], [3, 3]]&#125;)) 输出结果就会变成如下的格式：123456input_embedding = [[[0 1 0 0 0] [0 0 1 0 0]] [[0 0 1 0 0] [0 1 0 0 0]] [[0 0 0 1 0] [0 0 0 1 0]]] 对比上下两个结果不难发现，相当于在np.array中直接采用下标数组获取数据。需要注意的细节是返回的tensor的dtype和传入的被查询的tensor的dtype保持一致；和ids的dtype无关。 字符嵌入在NLP中，我们通常使用的过滤器会滑过整个矩阵(单词)。因此，过滤器的“宽度（width）”通常与输入矩阵的宽度相同。高度，或区域大小（region size），可能会有所不同，但是滑动窗口一次在2-5个字是典型的。 一个NLP上的卷积实例是下面这样：上图展示了CNN在文本分类的使用，使用了2种过滤器（卷积核），每个过滤器有3种高度（区域大小），即有6种卷积结构（左起第2列），所以会产生6中卷积后的结果（左起第3列），经过最大池化层（后面还会提到池化层），每个卷积的结果将变为1个值（左起第4列），最终生成一个向量（左起第5列），最终经过分类器得到一个二分类结果（最后一列）。来自：A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification。 语言模型词嵌入的过程参看 allennlp.org - elmo 标题 说明 附加 词向量技术-从word2vec到ELMo 20180724 NLP领域的ImageNet时代到来：词嵌入「已死」，语言模型当立 翻译自 NLP’s ImageNet moment has arrived 20180709 2018最好的词句嵌入技术概览：从无监督学习到监督、多任务学习 翻译自 The Current Best of Universal Word Embeddings and Sentence Embeddings 20180606 NLP的游戏规则从此改写？从word2vec, ELMo到BERT 夕小瑶 详细解读 词嵌入资源 标题 说明 附加 Tencent AI Lab Embedding Corpus for Chinese Words and Phrases|This corpus provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases, which are pre-trained on large-scale high-quality data. These vectors, capturing semantic meanings for Chinese words and phrases, can be widely applied in many downstream Chinese processing tasks (e.g., named entity recognition and text classification) and in further research.腾讯AI Lab开源大规模高质量中文词向量数据，800万中文词随你用||]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>词嵌入</tag>
        <tag>Embedding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Character-Aware Neural Language Models]]></title>
    <url>%2F2018%2F10%2F29%2FCharacter_Aware_Neural_Language_Models%2F</url>
    <content type="text"><![CDATA[本文 是 NAACL 2018 最佳论文 Deep contextualized word representations 的 ELMo 模型字符卷积的基础。 注意点： 卷积核的高度一般与单词矩阵的高度一致（字符向量维度）； 不同的卷积核宽度代表着不同的 N-gram 语法； 上图一共有 12 个卷积核，宽度一样的卷积核放在一起了（黄色、蓝色和红色）； Character-Aware Neural Language ModelsYoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush(Submitted on 26 Aug 2015 (v1), last revised 1 Dec 2015 (this version, v4)) We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information. Comments: AAAI 2016Subjects: Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)Cite as: arXiv:1508.06615 [cs.CL] (or arXiv:1508.06615v4 [cs.CL] for this version) 标题 说明 附加 自然语言处理中CNN模型几种常见的Max Pooling操作 张俊林 20160407]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>Character Embedding</tag>
        <tag>Language Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELMo Deep contextualized word representations]]></title>
    <url>%2F2018%2F10%2F29%2FELMo_Deep_contextualized_word_representations%2F</url>
    <content type="text"><![CDATA[NAACL 2018最佳论文 Deep contextualized word representations：艾伦人工智能研究所提出新型深度语境化词表征（研究者使用从双向 LSTM 中得到的向量，该 LSTM 是使用成对语言模型（LM）目标在大型文本语料库上训练得到的。因此，该表征叫作 ELMo（Embeddings from Language Models）表征。）。 ELMo 使用方法如何使用ELMo的词向量呢？( 论文 3.3 有详细描述)在supervised learning的情况下，可以各种自如的使用: 直接将ELMo词向量 ELMo_k 与普通的词向量 x_k拼接（concat）[ x_k;ELMo_k ]。 直接将ELMo词向量ELMo_k 与隐层输出向量 h_k 拼接[ h_k;ELMo_k ]，在SNLI,SQuAD上都有提升。 代码使用实例： 官方版 知乎简版 ELMo TensorFlow Hub 的使用方法OverviewComputes contextualized word representations using character-based word representations and bidirectional LSTMs, as described in the paper “Deep contextualized word representations” [1]. This modules supports inputs both in the form of raw text strings or tokenized text strings. The module outputs fixed embeddings at each LSTM layer, a learnable aggregation of the 3 layers, and a fixed mean-pooled vector representation of the input. The complex architecture achieves state of the art results on several benchmarks. Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups. The use of an accelerator is recommended. Trainable parametersThe module exposes 4 trainable scalar weights for layer aggregation. Example use12345elmo = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)embeddings = elmo(["the cat is on the mat", "dogs are in the fog"],signature="default",as_dict=True)["elmo"] 1234567891011elmo = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)tokens_input = [["the", "cat", "is", "on", "the", "mat"],["dogs", "are", "in", "the", "fog", ""]]tokens_length = [6, 5]embeddings = elmo(inputs=&#123;"tokens": tokens_input,"sequence_len": tokens_length&#125;,signature="tokens",as_dict=True)["elmo"] InputThe module defines two signatures: default, and tokens. With the default signature, the module takes untokenized sentences as input. The input tensor is a string tensor with shape [batch_size]. The module tokenizes each string by splitting on spaces. With the tokens signature, the module takes tokenized sentences as input. The input tensor is a string tensor with shape [batch_size, max_length] and an int32 tensor with shape [batch_size] corresponding to the sentence length. The length input is necessary to exclude padding in the case of sentences with varying length. OutputThe output dictionary contains: word_emb: the character-based word representations with shape [batch_size, max_length, 512]. lstm_outputs1: the first LSTM hidden state with shape [batch_size, max_length, 1024]. lstm_outputs2: the second LSTM hidden state with shape [batch_size, max_length, 1024]. elmo: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape [batch_size, max_length, 1024] default: a fixed mean-pooling of all contextualized word representations with shape [batch_size, 1024]. 标题 说明 附加 Deep contextualized word representations 该研究提出了一种新型深度语境化词表征，可对词使用的复杂特征（如句法和语义）和词使用在语言语境中的变化进行建模（即对多义词进行建模）。这些表征可以轻松添加至已有模型，并在 6 个 NLP 问题中显著提高当前最优性能。 20180215 TensorFlow Hub 实现 Embeddings from a language model trained on the 1 Billion Word Benchmark. allennlp.org - elmo 论文官网 elmo bilm-tf 论文官方实现 Tensorflow implementation of contextualized word representations from bi-directional language models NAACL 2018最佳论文：艾伦人工智能研究所提出新型深度语境化词表征 机器之心解读 20180607 把 ELMo 作为 keras 的一个嵌入层使用 GitHub 201804 NAACL18 Best Paper: ELMo Liyuan Liu 解读 论文笔记ELMo 赵来福 详细解读 ELMo 最好用的词向量《Deep Contextualized Word Representations》 mountain blue 详细解读 Deep contextualized word representationsMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals. 在本论文中，我们介绍了一种新型深度语境化词表征，可对词使用的复杂特征（如句法和语义）和词使用在语言语境中的变化进行建模（即对多义词进行建模）。我们的词向量是深度双向语言模型（biLM）内部状态的函数，在一个大型文本语料库中预训练而成。本研究表明，这些表征能够被轻易地添加到现有的模型中，并在六个颇具挑战性的 NLP 问题（包括问答、文本蕴涵和情感分析）中显著提高当前最优性能。此外，我们的分析还表明，揭示预训练网络的深层内部状态至关重要，可以允许下游模型综合不同类型的半监督信号。 Comments: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera readySubjects: Computation and Language (cs.CL)Cite as: arXiv:1802.05365 [cs.CL] (or arXiv:1802.05365v2 [cs.CL] for this version)]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>ELMO</tag>
        <tag>word representations</tag>
        <tag>Language Modeling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经机器翻译 | Neural Machine Translation]]></title>
    <url>%2F2018%2F10%2F24%2F%E7%A5%9E%E7%BB%8F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 袁宵的机器翻译模型示例 分享一些神经翻译模型代码 持续更新 Tensor2Tensor TensorFlow 库 持续更新 Neural Machine Translation (seq2seq) Tutorial TensorFlow 库 持续更新 OpenNMT-tf 哈佛机器翻译库 持续更新 习翔宇 深度学习、传统机器学习、自然语言处理算法及实现 Neural Machine Translation (seq2seq) TutorialWMT English-German &mdash; Full ComparisonThe first 2 rows are our models with GNMTattention:model 1 (4 layers),model 2 (8 layers). Systems newstest2014 newstest2015 Ours &mdash; NMT + GNMT attention (4 layers) 23.7 26.5 Ours &mdash; NMT + GNMT attention (8 layers) 24.4 27.6 WMT SOTA 20.6 24.9 OpenNMT (Klein et al., 2017) 19.3 - tf-seq2seq (Britz et al., 2017) 22.2 25.2 GNMT (Wu et al., 2016) 24.6 - The above results show our models are very competitive among models of similar architectures.\[Note that OpenNMT uses smaller models and the current best result (as of this writing) is 28.4 obtained by the Transformer network (Vaswani et al., 2017) which has a significantly different architecture.] Other details for better NMT modelsBidirectional RNNsBidirectionality on the encoder side generally gives better performance (withsome degradation in speed as more layers are used). Here, we give a simplifiedexample of how to build an encoder with a single bidirectional layer: 12345678# Construct forward and backward cellsforward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn( forward_cell, backward_cell, encoder_emb_inp, sequence_length=source_sequence_length, time_major=True)encoder_outputs = tf.concat(bi_outputs, -1) The variables encoder_outputs and encoder_state can be used in the same wayas in Section Encoder. Note that, for multiple bidirectional layers, we need tomanipulate the encoder_state a bit, see model.py, method_build_bidirectional_rnn() for more details. Beam searchWhile greedy decoding can give us quite reasonable translation quality, a beamsearch decoder can further boost performance. The idea of beam search is tobetter explore the search space of all possible translations by keeping around asmall set of top candidates as we translate. The size of the beam is calledbeam width; a minimal beam width of, say size 10, is generally sufficient. Formore information, we refer readers to Section 7.2.3of Neubig, (2017). Here’s an example of howbeam search can be done: OpenNMT-tfOpenNMT-tf is a general purpose sequence learning toolkit using TensorFlow. While neural machine translation is the main target task, it has been designed to more generally support: sequence to sequence mapping sequence tagging sequence classification The project is production-oriented and comes with stability guarantees. Key featuresOpenNMT-tf focuses on modularity to support advanced modeling and training capabilities: arbitrarily complex encoder architecturese.g. mixing RNNs, CNNs, self-attention, etc. in parallel or in sequence. hybrid encoder-decoder modelse.g. self-attention encoder and RNN decoder or vice versa. neural source-target alignmenttrain with guided alignment to constrain attention vectors and output alignments as part of the translation API. multi-source traininge.g. source text and Moses translation as inputs for machine translation. multiple input formattext with support of mixed word/character embeddings or real vectors serialized in TFRecord files. on-the-fly tokenizationapply advanced tokenization dynamically during the training and detokenize the predictions during inference or evaluation. domain adaptationspecialize a model to a new domain in a few training steps by updating the word vocabularies in checkpoints. automatic evaluationsupport for saving evaluation predictions and running external evaluators (e.g. BLEU). mixed precision trainingtake advantage of the latest NVIDIA optimizations to train models with half-precision floating points. and all of the above can be used simultaneously to train novel and complex architectures. See the predefined models to discover how they are defined and the API documentation to customize them.]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>神经机器翻译</tag>
        <tag>NMT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras]]></title>
    <url>%2F2018%2F10%2F24%2FKeras%2F</url>
    <content type="text"><![CDATA[名称 网址 Keras 英文官网 https://keras.io/ Keras 中文官网 https://keras.io/zh/ Keras 中文 https://keras-cn.readthedocs.io/en/latest/ Keras Example 实验Vision models examplesmnist_mlp.py Trains a simple deep multi-layer perceptron on the MNIST dataset. 网络结构 实验结果Gets to 98.43% test accuracy after 20 epochs mnist_cnn.py Trains a simple convnet on the MNIST dataset. 网络结构 实验结果Gets to 99.19% test accuracy after 12 epochs cifar10_cnn.py Trains a simple deep CNN on the CIFAR10 small images dataset. It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.(it’s still underfitting at that point, though). 网络结构 实验结果 cifar10_cnn_capsule.py Trains a simple CNN-Capsule Network on the CIFAR10 small images dataset. Capsule Implement is from https://github.com/bojone/Capsule/Capsule Paper: https://arxiv.org/abs/1710.09829 “””Train a simple CNN-Capsule Network on the CIFAR10 small images dataset. Without Data Augmentation:It gets to 75% validation accuracy in 10 epochs,and 79% after 15 epochs, and overfitting after 20 epochs With Data Augmentation:It gets to 75% validation accuracy in 10 epochs,and 79% after 15 epochs, and 83% after 30 epochs.In my test, highest validation accuracy is 83.79% after 50 epochs. This is a fast Implement, just 20s/epoch with a gtx 1070 gpu.“”” 网络结构 实验结果 cifar10_resnet.py Trains a ResNet on the CIFAR10 small images dataset. “””Trains a ResNet on the CIFAR10 dataset. ResNet v1[a] Deep Residual Learning for Image Recognitionhttps://arxiv.org/pdf/1512.03385.pdf ResNet v2[b] Identity Mappings in Deep Residual Networkshttps://arxiv.org/pdf/1603.05027.pdf“”” 1234567891011121314# Model parameter# ----------------------------------------------------------------------------# | | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch# Model | n | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti# |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)# ----------------------------------------------------------------------------# ResNet20 | 3 (2)| 92.16 | 91.25 | ----- | ----- | 35 (---)# ResNet32 | 5(NA)| 92.46 | 92.49 | NA | NA | 50 ( NA)# ResNet44 | 7(NA)| 92.50 | 92.83 | NA | NA | 70 ( NA)# ResNet56 | 9 (6)| 92.71 | 93.03 | 93.01 | NA | 90 (100)# ResNet110 |18(12)| 92.65 | 93.39+-.16| 93.15 | 93.63 | 165(180)# ResNet164 |27(18)| ----- | 94.07 | ----- | 94.54 | ---(---)# ResNet1001| (111)| ----- | 92.39 | ----- | 95.08+-.14| ---(---)# --------------------------------------------------------------------------- 网络结构 实验结果 conv_lstm.py Demonstrates the use of a convolutional LSTM network. 网络结构 实验结果epochs=100 - loss: 3.8720e-05 - val_loss: 1.0020e-04 image_ocr.pyTrains a convolutional stack followed by a recurrent stack and a CTC logloss function to perform optical character recognition (OCR). mnist_acgan.py Implementation of AC-GAN (Auxiliary Classifier GAN) on the MNIST dataset 123456789101112131415161718192021222324"""Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on theMNIST dataset. See https://arxiv.org/abs/1610.09585 for more details.You should start to see reasonable images after ~5 epochs, and good imagesby ~15 epochs. You should use a GPU, as the convolution-heavy operations arevery slow on the CPU. Prefer the TensorFlow backend if you plan on iterating,as the compilation time can be a blocker using Theano.Timings:Hardware | Backend | Time / Epoch------------------------------------------- CPU | TF | 3 hrs Titan X (maxwell) | TF | 4 min Titan X (maxwell) | TH | 7 minConsult https://github.com/lukedeo/keras-acgan for more information andexample output"""# Adam parameters suggested inhttps://arxiv.org/abs/1511.06434adam_lr = 0.0002adam_beta_1 = 0.5 网络结构mnist_acgan_generator mnist_acgan_discriminator mnist_acgan 实验结果1234567Testing for epoch 100:component | loss | generation_loss | auxiliary_loss-----------------------------------------------------------------generator (train) | 0.77 | 0.7664 | 0.0005generator (test) | 0.81 | 0.8105 | 0.0000discriminator (train) | 0.71 | 0.6947 | 0.0131discriminator (test) | 0.71 | 0.7032 | 0.0110 周期1 周期10 周期30 周期100 mnist_hierarchical_rnn.py Trains a Hierarchical RNN (HRNN) to classify MNIST digits. 网络结构 实验结果 mnist_siamese.pyTrains a Siamese multi-layer perceptron on pairs of digits from the MNIST dataset. mnist_swwae.py Trains a Stacked What-Where AutoEncoder built on residual blocks on the MNIST dataset. 12345678910111213141516171819202122232425262728293031323334353637383940'''Trains a stacked what-where autoencoder built on residual blocks on theMNIST dataset. It exemplifies two influential methods that have been developedin the past few years.The first is the idea of properly 'unpooling.' During any max pool, theexact location (the 'where') of the maximal value in a pooled receptive fieldis lost, however it can be very useful in the overall reconstruction of aninput image. Therefore, if the 'where' is handed from the encoderto the corresponding decoder layer, features being decoded can be 'placed' inthe right location, allowing for reconstructions of much higher fidelity.# References- Visualizing and Understanding Convolutional Networks Matthew D Zeiler, Rob Fergus https://arxiv.org/abs/1311.2901v3- Stacked What-Where Auto-encoders Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun https://arxiv.org/abs/1506.02351v8The second idea exploited here is that of residual learning. Residual blocksease the training process by allowing skip connections that give the networkthe ability to be as linear (or non-linear) as the data sees fit. This allowsfor much deep networks to be easily trained. The residual element seems tobe advantageous in the context of this example as it allows a nice symmetrybetween the encoder and decoder. Normally, in the decoder, the finalprojection to the space where the image is reconstructed is linear, howeverthis does not have to be the case for a residual block as the degree to whichits output is linear or non-linear is determined by the data it is fed.However, in order to cap the reconstruction in this example, a hard softmax isapplied as a bias because we know the MNIST digits are mapped to [0, 1].# References- Deep Residual Learning for Image Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1512.03385v1- Identity Mappings in Deep Residual Networks Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun https://arxiv.org/abs/1603.05027v3''' 网络结构 实验结果1epochs=5 loss: 0.0023 - val_loss: 0.0022 mnist_transfer_cnn.pyTransfer learning toy example on the MNIST dataset. ‘’’Transfer learning toy example.1 - Train a simple convnet on the MNIST dataset the first 5 digits [0..4].2 - Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9].Get to 99.8% test accuracy after 5 epochsfor the first five digits classifierand 99.2% for the last five digits after transfer + fine-tuning.‘’’ 网络结构 实验结果橙色的线是迁移前模型的数据，蓝色的线是迁移后模型的数据。 mnist_denoising_autoencoder.py Trains a denoising autoencoder on the MNIST dataset. 网络结构encoderdecoderautoencoder 实验结果模型去噪声结果 Text &amp; sequences examplesaddition_rnn.py Implementation of sequence to sequence learning for performing addition of two numbers (as strings). 1234567891011121314151617181920212223242526'''An implementation of sequence to sequence learning for performing additionInput: "535+61"Output: "596"Padding is handled by using a repeated sentinel character (space)Input may optionally be reversed, shown to increase performance in many tasks in:"Learning to Execute"http://arxiv.org/abs/1410.4615and"Sequence to Sequence Learning with Neural Networks"http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdfTheoretically it introduces shorter term dependencies between source and target.Two digits reversed:+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochsThree digits reversed:+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochsFour digits reversed:+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochsFive digits reversed:+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs''' 网络结构 实验结果12345678910111213epochs = 200 时的结果loss: 1.2168e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9997Q 6+909 T 915 ☑ 915Q 128+263 T 391 ☑ 391Q 104+0 T 104 ☑ 104Q 63+352 T 415 ☑ 415Q 624+8 T 632 ☑ 632Q 31+251 T 282 ☑ 282Q 758+445 T 1203 ☑ 1203Q 88+534 T 622 ☑ 622Q 315+624 T 939 ☑ 939Q 81+459 T 540 ☑ 540 babi_rnn.py Trains a two-branch recurrent network on the bAbI dataset for reading comprehension. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758'''Trains two recurrent neural networks based upon a story and a question.The resulting merged vector is then queried to answer a range of bAbI tasks.The results are comparable to those for an LSTM model provided in Weston et al.:"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"http://arxiv.org/abs/1502.05698Task Number | FB LSTM Baseline | Keras QA--- | --- | ---QA1 - Single Supporting Fact | 50 | 100.0QA2 - Two Supporting Facts | 20 | 50.0QA3 - Three Supporting Facts | 20 | 20.5QA4 - Two Arg. Relations | 61 | 62.9QA5 - Three Arg. Relations | 70 | 61.9QA6 - yes/No Questions | 48 | 50.7QA7 - Counting | 49 | 78.9QA8 - Lists/Sets | 45 | 77.2QA9 - Simple Negation | 64 | 64.0QA10 - Indefinite Knowledge | 44 | 47.7QA11 - Basic Coreference | 72 | 74.9QA12 - Conjunction | 74 | 76.4QA13 - Compound Coreference | 94 | 94.4QA14 - Time Reasoning | 27 | 34.8QA15 - Basic Deduction | 21 | 32.4QA16 - Basic Induction | 23 | 50.6QA17 - Positional Reasoning | 51 | 49.1QA18 - Size Reasoning | 52 | 90.8QA19 - Path Finding | 8 | 9.0QA20 - Agent's Motivations | 91 | 90.7For the resources related to the bAbI project, refer to:https://research.facebook.com/researchers/1543934539189348# Notes- With default word, sentence, and query vector sizes, the GRU model achieves: - 100% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU) - 50% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.- The task does not traditionally parse the question separately. This likelyimproves accuracy and is a good example of merging two RNNs.- The word vector embeddings are not shared between the story and question RNNs.- See how the accuracy changes given 10,000 training samples (en-10k) insteadof only 1000. 1000 was used in order to be comparable to the original paper.- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.- The length and noise (i.e. 'useless' story components) impact the ability forLSTMs / GRUs to provide the correct answer. Given only the supporting facts,these RNNs can achieve 100% accuracy on many tasks. Memory networks and neuralnetworks that use attentional processes can efficiently search through thisnoise to find the relevant statements, improving performance substantially.This becomes especially obvious on QA2 and QA3, both far longer than QA1.''' 网络结构 实验结果 仅仅把 only_supporting=False 改为 Ture 就可以显著的提高模型准确率和训练速度。原因可能是去除了与问题无关的信息，缩小了答案搜索空间（由x(None, 552) 变成了 (None, 14) ）。这也说明目前，神经网络从大段阅读理解文章段里面找答案还是比较困难的，由几句话中找答案还是还是比较准确的。效果如下： 123with tarfile.open(path) as tar: train = get_stories(tar.extractfile(challenge.format('train')), only_supporting=True) test = get_stories(tar.extractfile(challenge.format('test')), only_supporting=True) babi_rnn_v2.py Trains a two-branch recurrent network on the bAbI dataset for reading comprehension. 1234567'''change: merged = layers.add([encoded_sentence, encoded_question])to: merged = layers.multiply([encoded_sentence, encoded_question])将 encoded_sentence 句子张量与 encoded_question 问题张量由通过加法融合变成乘积融合。模型 v2 在训练集中的表现优于 v1，在测试集同样（未过拟合前）优于 v1，但过拟合后差于 v1。''' 网络结构 实验结果橙色的表示 v1，分红表示 v2。 babi_memnn.py Trains a memory network on the bAbI dataset for reading comprehension. 123456789101112131415'''Trains a memory network on the bAbI dataset.References:- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", http://arxiv.org/abs/1502.05698- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, "End-To-End Memory Networks", http://arxiv.org/abs/1503.08895Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.Time per epoch: 3s on CPU (core i7).''' 网络结构 实验结果 babi_rnn_vs_memnn_on_10k_qa1_single-supporting-fact babi_rnn.py 和 babi_memnn.py 在 Facebook bAbi 阅读理解数据集 qa1_single-supporting-fact 上的对比实验。 网络结构见 babi_rnn.py 和 babi_memnn.py 的网络结构 实验结果 可见 babi_rnn.py 比 babi_memnn.py 不仅模型更简单，而且训练更快、效果更好。 imdb_bidirectional_lstm.py Trains a Bidirectional LSTM on the IMDB sentiment classification task. 网络结构 实验结果 imdb_cnn.py Demonstrates the use of Convolution1D for text classification. 网络结构 实验结果epochs=10loss: 0.0293 - acc: 0.9894 - val_loss: 0.5514 - val_acc: 0.8855 imdb_cnn_lstm.py Trains a convolutional stack followed by a recurrent stack network on the IMDB sentiment classification task. 网络结构 实验结果epochs=10Test score: 0.8979291968528181Test accuracy: 0.825119995546341 imdb_fasttext.py Trains a FastText model on the IMDB sentiment classification task. 1234567891011'''This example demonstrates the use of fasttext for text classificationBased on Joulin et al's paper:Bags of Tricks for Efficient Text Classificationhttps://arxiv.org/abs/1607.01759Results on IMDB datasets with uni and bi-gram embeddings: Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu. Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.''' 网络结构 实验结果 imdb_lstm.py Trains an LSTM model on the IMDB sentiment classification task. 1234567891011121314'''Trains an LSTM model on the IMDB sentiment classification task.The dataset is actually too small for LSTM to be of any advantagecompared to simpler, much faster methods such as TF-IDF + LogReg.# Notes- RNNs are tricky. Choice of batch size is important,choice of loss and optimizer is critical, etc.Some configurations won't converge.- LSTM loss decrease patterns during training can be quite differentfrom what you see with CNNs/MLPs/etc.''' 网络结构 实验结果epochs=15Test accuracy: 0.81312 lstm_stateful.pyDemonstrates how to use stateful RNNs to model long sequences efficiently. 网络结构 实验结果 lstm_seq2seq.py Trains a basic character-level sequence-to-sequence model.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950'''Sequence to sequence example in Keras (character-level).This script demonstrates how to implement a basic character-levelsequence-to-sequence model. We apply it to translatingshort English sentences into short French sentences,character-by-character. Note that it is fairly unusual todo character-level machine translation, as word-levelmodels are more common in this domain.# Summary of the algorithm- We start with input sequences from a domain (e.g. English sentences) and corresponding target sequences from another domain (e.g. French sentences).- An encoder LSTM turns input sequences to 2 state vectors (we keep the last LSTM state and discard the outputs).- A decoder LSTM is trained to turn the target sequences into the same sequence but offset by one timestep in the future, a training process called "teacher forcing" in this context. Is uses as initial state the state vectors from the encoder. Effectively, the decoder learns to generate `targets[t+1...]` given `targets[...t]`, conditioned on the input sequence.- In inference mode, when we want to decode unknown input sequences, we: - Encode the input sequence into state vectors - Start with a target sequence of size 1 (just the start-of-sequence character) - Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character - Sample the next character using these predictions (we simply use argmax). - Append the sampled character to the target sequence - Repeat until we generate the end-of-sequence character or we hit the character limit.# Data downloadEnglish to French sentence pairs.http://www.manythings.org/anki/fra-eng.zipLots of neat sentence pairs datasets can be found at:http://www.manythings.org/anki/# References- Sequence to Sequence Learning with Neural Networks https://arxiv.org/abs/1409.3215- Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation https://arxiv.org/abs/1406.1078''' 网络结构 编码器 解码器 实验结果epochs=100loss: 0.0602 - val_loss: 0.7592 123456789-Input sentence: Come in.Decoded sentence: Entrez !-Input sentence: Come on!Decoded sentence: Viens !-Input sentence: Drop it!Decoded sentence: Laisse tomber ! lstm_seq2seq_restore.py Restores a character-level sequence to sequence model from disk (saved by lstm_seq2seq.py) and uses it to generate predictions. 网络结构同 lstm_seq2seq 实验结果12345678Input sentence: Fire!Decoded sentence: Au feu !Input sentence: Help!Decoded sentence: À l'aide !Input sentence: Jump.Decoded sentence: Saute. pretrained_word_embeddings.py Loads pre-trained word embeddings (GloVe embeddings) into a frozen Keras Embedding layer, and uses it to train a text classification model on the 20 Newsgroup dataset. 网络结构 实验结果epochs=30loss: 0.1113 - acc: 0.9586 - val_loss: 1.6334 - val_acc: 0.7179 reuters_mlp.py Trains and evaluate a simple MLP on the Reuters newswire topic classification task. 网络结构 实验结果 reuters_mlp_relu_vs_selu网络结构12345Network 1 ReluHyperparameters: &#123;'n_dense': 6, 'dense_units': 16, 'activation': 'relu', 'dropout': &lt;class 'keras.layers.core.Dropout'&gt;, 'dropout_rate': 0.5, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'sgd'&#125;Network 2 SeluHyperparameters: &#123;'n_dense': 6, 'dense_units': 16, 'activation': 'selu', 'dropout': &lt;class 'keras.layers.noise.AlphaDropout'&gt;, 'dropout_rate': 0.1, 'kernel_initializer': 'lecun_normal', 'optimizer': 'sgd'&#125; 实验结果123456Network 1 ReluTest score: 1.9769948146646827Test accuracy: 0.5204808548796103Network 2 SeluTest score: 1.530816549927872Test accuracy: 0.6714158504007124 Generative models exampleslstm_text_generation.py Generates text from Nietzsche’s writings. 1234567891011'''Example script to generate text from Nietzsche's writings.At least 20 epochs are required before the generated textstarts sounding coherent.It is recommended to run this script on GPU, as recurrentnetworks are quite computationally intensive.If you try this script on new data, make sure your corpushas at least ~100k characters. ~1M is better.''' 网络结构实验结果12345678910111213----- Generating text after Epoch: 99----- diversity: 0.2----- Generating with seed: " of his tolerance and humanity which pro" of his tolerance and humanity which proe ee e e e ee e ee et e ee te ee e" ! esov ehe eeted en enete ee e e ee q te e t eeqo a x een x qe n et" ! ex if i tewn x .o e v - ' e ! q jj j s t qhi xh x ! tetsa k ewetce e e of eeh e e et i e ee e x ee eetk( e e n e x xo = x ! xvekne x e ) x qnb z ehe e n x )edxd xx x xi x x tcnwet x- 'e o q e te eto e x eti '----- diversity: 0.5----- Generating with seed: " of his tolerance and humanity which pro" of his tolerance and humanity which pro ee e e ee ee ee e beeee ee e ' xx z e e ' en qengg x x evo-n x ! xn 'o xb i it k x n x xbe a wee x tix e i t q etvece e etoe o z eet i q h wete eo xe'e e egovee ! e eese e oe xe x e e zd ti q n x ni tqj x a n zxb x x " x we n n x j e et z v ! xj an ee xq " q styiete nxe! x et ! i qt ta xbn tx----- diversity: 1.0----- Generating with seed: " of his tolerance and humanity which pro" of his tolerance and humanity which proe eee ee eeeeee et eeeteeei ee (eeiiey qqe i n xxee x ue e jni " x xb z n n e in n e' ve te j e zw eeq(se t x q xve im:t x xxb tek x ehed te e't xheefe e-e xn e ebe eey zn xeg:ti xbs xvfete x !o ee- e e e o e we e ese eet -e oee e x (xb e ee necf e j e e ( ee je ie este)n ax q n . xjf z xi o t xxv x xnocese i----- diversity: 1.2----- Generating with seed: " of his tolerance and humanity which pro" of his tolerance and humanity which pro eeeee eee e e e tetee" e eteeee-e i eex o xhe ee x e on e te"( t x xg e i hek.tni etf xiecnne evet ecewe e e o ec ? e;e ,ee e e ee e;skyn xe e e x xte he et i q x jw w xn xn e x ' q= t o n nex e =tho xenwei ao? x zn x evq ety q x x et e es be d x xq ie n xetzo ke q y etx xt xsn inn eti e x ei eq t conv_filter_visualization.py Visualization of the filters of VGG16, via gradient ascent in input space. Results example: http://i.imgur.com/4nj4KjN.jpg 网络结构 实验结果 deep_dream.pyDeep Dreams in Keras. 网络结构实验结果 neural_doodle.pyNeural doodle. 网络结构实验结果 neural_style_transfer.pyNeural style transfer. 网络结构实验结果 variational_autoencoder.py Demonstrates how to build a variational autoencoder. 12345678910111213'''Example of VAE on MNIST dataset using MLPThe VAE has a modular design. The encoder, decoder and VAEare 3 models that share weights. After training the VAE model,the encoder can be used to generate latent vectors.The decoder can be used to generate MNIST digits by sampling thelatent vector from a Gaussian distribution with mean=0 and std=1.**Reference：**[1] Kingma, Diederik P., and Max Welling."Auto-encoding variational bayes."https://arxiv.org/abs/1312.6114''' 网络结构encoderdecoderautoencoder 实验结果模型生成手写数字的效果 digits_over_latent 模型 2 维隐藏变量的均值（z_mean）可视化后的效果 variational_autoencoder_deconv.py Demonstrates how to build a variational autoencoder with Keras using deconvolution layers. ‘’’Example of VAE on MNIST dataset using CNNThe VAE has a modular design. The encoder, decoder and VAEare 3 models that share weights. After training the VAE model,the encoder can be used to generate latent vectors.The decoder can be used to generate MNIST digits by sampling thelatent vector from a Gaussian distribution with mean=0 and std=1. Reference[1] Kingma, Diederik P., and Max Welling.“Auto-encoding variational bayes.”https://arxiv.org/abs/1312.6114‘’’ 网络结构encoderdecoderautoencoder 实验结果模型生成手写数字的效果 模型 2 维隐藏变量的均值（z_mean）可视化后的效果 Examples demonstrating specific Keras functionalityantirectifier.py Demonstrates how to write custom layers for Keras. 1234567891011'''The example demonstrates how to write custom layers for Keras.We build a custom activation layer called 'Antirectifier',which modifies the shape of the tensor that passes through it.We need to specify two methods: `compute_output_shape` and `call`.Note that the same result can also be achieved via a Lambda layer.Because our custom layer is written with primitives from the Kerasbackend (`K`), our code can run both on TensorFlow and Theano.''' 网络结构 实验结果12epochs = 40acc: 0.9985 - val_loss: 0.0901 - val_acc: 0.9809 灰色是 antirectifier，蓝色是 mnist_cnn，橙色是 mnist_mpl。 mnist_sklearn_wrapper.py Demonstrates how to use the sklearn wrapper. 1234&apos;&apos;&apos;Example of how to use sklearn wrapperBuilds simple CNN models on MNIST and uses sklearn&apos;s GridSearchCV to find best model&apos;&apos;&apos; 网络结构 实验结果1234The parameters of the best model are:&#123;'dense_layer_sizes': [64, 64], 'epochs': 6, 'filters': 8, 'kernel_size': 3, 'pool_size': 2&#125;loss : 0.042469549842912235acc : 0.9872 mnist_irnn.pyReproduction of the IRNN experiment with pixel-by-pixel sequential MNIST in “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units” by Le et al. mnist_net2net.pyReproduction of the Net2Net experiment with MNIST in “Net2Net: Accelerating Learning via Knowledge Transfer”. reuters_mlp_relu_vs_selu.pyCompares self-normalizing MLPs with regular MLPs. mnist_tfrecord.pyMNIST dataset with TFRecords, the standard TensorFlow data format. mnist_dataset_api.pyMNIST dataset with TensorFlow’s Dataset API. cifar10_cnn_tfaugment2d.py Trains a simple deep CNN on the CIFAR10 small images dataset using Tensorflow internal augmentation APIs. 12345678910111213141516171819202122232425262728293031323334'''Train a simple deep CNN on the CIFAR10 small images dataset.Using Tensorflow internal augmentation APIs by replacing ImageGenerator withan embedded AugmentLayer using LambdaLayer, which is faster on GPU.# Benchmark of `ImageGenerator` vs `AugmentLayer` both using augmentation 2D:(backend = Tensorflow-GPU, Nvidia Tesla P100-SXM2)Settings: horizontal_flip = True----------------------------------------------------------------------------Epoch | ImageGenerator | ImageGenerator | AugmentLayer | Augment LayerNumber | %Accuracy | Performance | %Accuracy | Performance----------------------------------------------------------------------------1 | 44.84 | 15ms/step | 45.54 | 358us/step2 | 52.34 | 8ms/step | 50.55 | 285us/step8 | 65.45 | 8ms/step | 65.59 | 281us/step25 | 76.74 | 8ms/step | 76.17 | 280us/step100 | 78.81 | 8ms/step | 78.70 | 285us/step---------------------------------------------------------------------------Settings: rotation = 30.0----------------------------------------------------------------------------Epoch | ImageGenerator | ImageGenerator | AugmentLayer | Augment LayerNumber | %Accuracy | Performance | %Accuracy | Performance----------------------------------------------------------------------------1 | 43.46 | 15ms/step | 42.21 | 334us/step2 | 48.95 | 11ms/step | 48.06 | 282us/step8 | 63.59 | 11ms/step | 61.35 | 290us/step25 | 72.25 | 12ms/step | 71.08 | 287us/step100 | 76.35 | 11ms/step | 74.62 | 286us/step---------------------------------------------------------------------------(Corner process and rotation precision by `ImageGenerator` and `AugmentLayer`are slightly different.)''' 网络结构 实验结果 tensorboard_embeddings_mnist.pyTrains a simple convnet on the MNIST dataset and embeds test data which can be later visualized using TensorBoard’s Embedding Projector. Keras API 部分说明LSTM理解LSTM在keras API中参数return_sequences和return_state Understand the Difference Between Return Sequences and Return States for LSTMs in Keras Kears LSTM API 中给出的两个参数描述 return_sequences：默认 False。在输出序列中，返回单个 hidden state值还是返回全部time step 的 hidden state值。 False 返回单个， true 返回全部。 return_state：默认 False。是否返回除输出之外的最后一个状态。 return_sequences return_state 式子 效果 False False 123456789101112131415161718192021222324252627282930313233343536373839404142434445|True|False|```LSTM(1, return_sequences=True)```|输出的hidden state 包含全部时间步的结果。||False|True|```lstm1, state_h, state_c = LSTM(1, return_state=True)```|lstm1 和 state_h 结果都是 hidden state。在这种参数设定下，它们俩的值相同。都是最后一个时间步的 hidden state。 state_c 是最后一个时间步 cell state结果。||True|True|```lstm1, state_h, state_c = LSTM(1, return_sequences=True, return_state=True)```|此时，我们既要输出全部时间步的 hidden state ，又要输出 cell state。lstm1 存放的就是全部时间步的 hidden state。state_h 存放的是最后一个时间步的 hidden state。state_c 存放的是最后一个时间步的 cell state|## AlphaDropout [噪声层Noise](https://keras-cn.readthedocs.io/en/latest/layers/noise_layer/)Alpha Dropout是一种保持输入均值和方差不变的Dropout，该层的作用是即使在dropout时也保持数据的自规范性。 通过随机对负的饱和值进行激活，Alphe Drpout与selu激活函数配合较好。参数rate: 浮点数，类似Dropout的Drop比例。乘性mask的标准差将保证为sqrt(rate / (1 - rate)).seed: 随机数种子输入shape任意，当使用该层为模型首层时需指定input_shape参数输出shape与输入相同参考文献[Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)## [文本Tokenizer与序列pad_sequences预处理](https://blog.csdn.net/zzulp/article/details/76146947)```pythonimport keras.preprocessing.text as Tfrom keras.preprocessing.text import Tokenizertext1='some thing to eat'text2='some thing to drink'texts=[text1,text2]print T.text_to_word_sequence(text1) #['some', 'thing', 'to', 'eat']print T.one_hot(text1,10) #[7, 9, 3, 4]print T.one_hot(text2,10) #[7, 9, 3, 1]tokenizer = Tokenizer(num_words=10)tokenzier.fit_on_text(texts)print tokenizer.word_count #[('some', 2), ('thing', 2), ('to', 2), ('eat', 1), ('drink', 1)]print tokenizer.word_index #&#123;'some': 1, 'thing': 2,'to': 3 ','eat': 4, drink': 5&#125;print tokenizer.word_docs #&#123;'some': 2, 'thing': 2, 'to': 2, 'drink': 1, 'eat': 1&#125;print tokenizer.index_docs #&#123;1: 2, 2: 2, 3: 2, 4: 1, 5: 1&#125;print tokenizer.text_to_sequences(texts) #[[1, 2, 3, 4], [1, 2, 3, 5]]print tokenizer.text_to_matrix(texts) #[[ 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.], [ 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.]] 123import keras.preprocessing.sequence as SS.pad_sequences([[1,2,3]],10,padding='post')# [[1, 2, 3, 0, 0, 0, 0, 0, 0, 0]]]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>深度学习框架</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F24%2F%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8BLanguageModel%2F</url>
    <content type="text"><![CDATA[自然语言处理中的语言模型预训练方法语言模型评价 语言模型构造完成后，如何确定好坏呢？ 目前主要有两种评价方法： 实用方法：通过查看该模型在实际应用（如拼写检查、机器翻译）中的表现来评价，优点是直观、实用，缺点是缺乏针对性、不够客观；理论方法：迷惑度/困惑度/混乱度（preplexity），其基本思想是给测试集的句子赋予较高概率值的语言模型较好,当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好。]]></content>
  </entry>
  <entry>
    <title><![CDATA[待读：SQuAD（Stanford Question Answering Dataset）综述]]></title>
    <url>%2F2018%2F10%2F23%2FSQuAD%EF%BC%88Stanford%20Question%20Answering%20Dataset%EF%BC%89%2F</url>
    <content type="text"><![CDATA[PaperWeekly 第38期 SQuAD综述]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>SQuAD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环神经网络RNN长短期记忆网络LSTM与门控循环网络GRU]]></title>
    <url>%2F2018%2F10%2F23%2F%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CRNN%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9CLSTM%E4%B8%8E%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9CGRU%2F</url>
    <content type="text"><![CDATA[RNN LSTM 最基本知识RNNCell1234567891011import tensorflow as tfimport numpy as npcell = tf.nn.rnn_cell.BasicRNNCell(num_units=128) # state_size = 128print(cell.state_size) # 128inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_sizeh0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态，形状为(batch_size, state_size)output, h1 = cell(inputs, h0) #调用函数print(h1.shape) # (32, 128) BasicLSTMCell123456789import tensorflow as tfimport numpy as nplstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_sizeh0 = lstm_cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态output, h1 = lstm_cell(inputs, h0)print(h1.h) # shape=(32, 128)print(h1.c) # shape=(32, 128) 堆叠RNNCell：MultiRNNCell12345678910111213141516171819202122232425262728293031323334import tensorflow as tfimport numpy as np# 每调用一次这个函数就返回一个BasicRNNCelldef get_a_cell(): return tf.nn.rnn_cell.BasicRNNCell(num_units=128)# 用tf.nn.rnn_cell MultiRNNCell创建3层RNNcell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)]) # 3层RNN# 得到的cell实际也是RNNCell的子类# 它的state_size是(128, 128, 128)# (128, 128, 128)并不是128x128x128的意思# 而是表示共有3个隐层状态，每个隐层状态的大小为128print(cell.state_size) # (128, 128, 128)# 使用对应的call函数inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_sizeh0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态, tuple中含有3个32x128的向量output, h1 = cell.call(inputs, h0)print(h1) # tuple中含有3个32x128的向量 符号说明 RNN 单层RNN12345678910111213141516171819202122232425262728293031import tensorflow as tfimport numpy as npn_steps = 2n_inputs = 3n_neurons = 5X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)seq_length = tf.placeholder(tf.int32, [None])outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length)init = tf.global_variables_initializer()X_batch = np.array([ # step 0 step 1 [[0, 1, 2], [9, 8, 7]], # instance 1 [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors) [[6, 7, 8], [6, 5, 4]], # instance 3 [[9, 0, 1], [3, 2, 1]], # instance 4 ])seq_length_batch = np.array([2, 1, 2, 2])with tf.Session() as sess: init.run() outputs_val, states_val = sess.run( [outputs, states], feed_dict=&#123;X: X_batch, seq_length: seq_length_batch&#125;) print("outputs_val.shape:", outputs_val.shape, "states_val.shape:", states_val.shape) print("outputs_val:", outputs_val, "states_val:", states_val) 123456789101112131415161718outputs_val.shape: (4, 2, 5) states_val.shape: (4, 5)outputs_val:[[[ 0.53073734 -0.61281306 -0.5437517 0.7320347 -0.6109526 ] [ 0.99996936 0.99990636 -0.9867181 0.99726075 -0.99999976]] [[ 0.9931584 0.5877845 -0.9100412 0.988892 -0.9982337 ] [ 0. 0. 0. 0. 0. ]] [[ 0.99992317 0.96815354 -0.985101 0.9995968 -0.9999936 ] [ 0.99948144 0.9998127 -0.57493806 0.91015154 -0.99998355]] [[ 0.99999255 0.9998929 0.26732785 0.36024097 -0.99991137] [ 0.98875254 0.9922327 0.6505734 0.4732064 -0.9957567 ]]]states_val: [[ 0.99996936 0.99990636 -0.9867181 0.99726075 -0.99999976] [ 0.9931584 0.5877845 -0.9100412 0.988892 -0.9982337 ] [ 0.99948144 0.9998127 -0.57493806 0.91015154 -0.99998355] [ 0.98875254 0.9922327 0.6505734 0.4732064 -0.9957567 ]] 首先输入X是一个 [batch_size，step，input_size] = [4，2，3] 的tensor，注意我们这里调用的是BasicRNNCell，只有一层循环网络，outputs是最后一层每个step的输出，它的结构是[batch_size，step，n_neurons] = [4，2，5]，states是每一层的最后那个step的输出，由于本例中，我们的循环网络只有一个隐藏层，所以它就代表这一层的最后那个step的输出，因此它和step的大小是没有关系的，我们的X有4个样本组成，输出神经元大小n_neurons是5，因此states的结构就是[batch_size，n_neurons] = [4，5]，最后我们观察数据，states的每条数据正好就是outputs的最后一个step的输出。 三层RNN1234567891011121314151617181920212223242526272829303132333435import tensorflow as tfimport numpy as npn_steps = 2n_inputs = 3n_neurons = 5n_layers = 3X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])seq_length = tf.placeholder(tf.int32, [None])layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu) for layer in range(n_layers)]multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32, sequence_length=seq_length)init = tf.global_variables_initializer()X_batch = np.array([ # step 0 step 1 [[0, 1, 2], [9, 8, 7]], # instance 1 [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors) [[6, 7, 8], [6, 5, 4]], # instance 3 [[9, 0, 1], [3, 2, 1]], # instance 4 ])seq_length_batch = np.array([2, 1, 2, 2])with tf.Session() as sess: init.run() outputs_val, states_val = sess.run( [outputs, states], feed_dict=&#123;X: X_batch, seq_length: seq_length_batch&#125;) print("outputs_val.shape:", outputs, "states_val.shape:", states) print("outputs_val:", outputs_val, "states_val:", states_val) 12345678910111213141516171819202122232425262728293031323334353637outputs_val.shape:Tensor("rnn/transpose_1:0", shape=(?, 2, 5), dtype=float32)states_val.shape:(&lt;tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 5) dtype=float32&gt;, &lt;tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 5) dtype=float32&gt;, &lt;tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 5) dtype=float32&gt;)outputs_val: [[[0. 0. 0. 0. 0. ] [0. 0.18740742 0. 0.2997518 0. ]] [[0. 0.07222144 0. 0.11551574 0. ] [0. 0. 0. 0. 0. ]] [[0. 0.13463384 0. 0.21534224 0. ] [0.03702604 0.18443246 0. 0.34539366 0. ]] [[0. 0.54511094 0. 0.8718864 0. ] [0.5382122 0. 0.04396425 0.4040263 0. ]]]states_val: (array([[0. , 0.83723307, 0. , 0. , 2.8518028 ], [0. , 0.1996038 , 0. , 0. , 1.5456247 ], [0. , 1.1372368 , 0. , 0. , 0.832613 ], [0. , 0.7904129 , 2.4675028 , 0. , 0.36980057]], dtype=float32), array([[0.6524607 , 0. , 0. , 0. , 0. ], [0.25143963, 0. , 0. , 0. , 0. ], [0.5010576 , 0. , 0. , 0. , 0. ], [0. , 0.3166597 , 0.4545995 , 0. , 0. ]], dtype=float32), array([[0. , 0.18740742, 0. , 0.2997518 , 0. ], [0. , 0.07222144, 0. , 0.11551574, 0. ], [0.03702604, 0.18443246, 0. , 0.34539366, 0. ], [0.5382122 , 0. , 0.04396425, 0.4040263 , 0. ]], dtype=float32)) outputs是最后一层的输出，即 [batch_size，step，n_neurons] = [4，2，5] states是每一层的最后一个step的输出，即三个结构为 [batch_size，n_neurons] = [4，5] 的tensor。 LSTM 三层LSTM下面我们继续讲当由BasicLSTMCell构造单元工厂的时候，只讲多层的情况，我们只需要将上面的BasicRNNCell替换成BasicLSTMCell就行了，打印信息如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970outputs_val.shape:Tensor("rnn/transpose_1:0", shape=(?, 2, 5), dtype=float32)states_val.shape:(LSTMStateTuple(c=&lt;tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 5) dtype=float32&gt;, h=&lt;tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 5) dtype=float32&gt;),LSTMStateTuple(c=&lt;tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 5) dtype=float32&gt;, h=&lt;tf.Tensor 'rnn/while/Exit_6:0' shape=(?, 5) dtype=float32&gt;),LSTMStateTuple(c=&lt;tf.Tensor 'rnn/while/Exit_7:0' shape=(?, 5) dtype=float32&gt;, h=&lt;tf.Tensor 'rnn/while/Exit_8:0' shape=(?, 5) dtype=float32&gt;))outputs_val:[[[1.2949290e-04 0.0000000e+00 2.7623639e-04 0.0000000e+00 0.0000000e+00] [9.4675866e-05 0.0000000e+00 2.0214770e-04 0.0000000e+00 0.0000000e+00]] [[4.3100454e-06 4.2123037e-07 1.4312843e-06 0.0000000e+00 0.0000000e+00] [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]] [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]] [[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00] [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]]]states_val:(LSTMStateTuple(c=array([[0. , 0. , 0.04676079, 0.04284539, 0. ], [0. , 0. , 0.0115245 , 0. , 0. ], [0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. ]], dtype=float32),h=array([[0. , 0. , 0.00035096, 0.04284406, 0. ], [0. , 0. , 0.00142574, 0. , 0. ], [0. , 0. , 0. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. ]], dtype=float32)),LSTMStateTuple(c=array([[0.0000000e+00, 1.0477135e-02, 4.9871090e-03, 8.2785974e-04, 0.0000000e+00], [0.0000000e+00, 2.3306280e-04, 0.0000000e+00, 9.9445322e-05, 5.9535629e-05], [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]], dtype=float32),h=array([[0.00000000e+00, 5.23016974e-03, 2.47756205e-03, 4.11730434e-04, 0.00000000e+00], [0.00000000e+00, 1.16522635e-04, 0.00000000e+00, 4.97301044e-05, 2.97713632e-05], [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]], dtype=float32)),LSTMStateTuple(c=array([[1.8937115e-04, 0.0000000e+00, 4.0442235e-04, 0.0000000e+00, 0.0000000e+00], [8.6200516e-06, 8.4243663e-07, 2.8625946e-06, 0.0000000e+00, 0.0000000e+00], [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]], dtype=float32),h=array([[9.4675866e-05, 0.0000000e+00, 2.0214770e-04, 0.0000000e+00, 0.0000000e+00], [4.3100454e-06, 4.2123037e-07, 1.4312843e-06, 0.0000000e+00, 0.0000000e+00], [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00], [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]], dtype=float32))) 如果您不查看框内的内容，LSTM单元看起来与常规单元格完全相同，除了它的状态分为两个向量：h（t）和c（t）。你可以将h（t）视为短期状态，将c（t）视为长期状态。 因此我们的states包含三个LSTMStateTuple，每一个表示每一层的最后一个step的输出，这个输出有两个信息，一个是h表示短期记忆信息，一个是c表示长期记忆信息。维度都是[batch_size，n_neurons] = [4，5]，states的最后一个LSTMStateTuple中的h就是outputs的最后一个step的输出。 GRU GRU 与RNN 代码基本一致，把basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons) 改为 basic_cell = tf.nn.rnn_cell.GRUCell(num_units=n_neurons)即可，结果分析同 RNN。 123456789101112131415161718192021222324252627282930import tensorflow as tfimport numpy as npn_steps = 2n_inputs = 3n_neurons = 5X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])basic_cell = tf.nn.rnn_cell.GRUCell(num_units=n_neurons)seq_length = tf.placeholder(tf.int32, [None])outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length)init = tf.global_variables_initializer()X_batch = np.array([ # step 0 step 1 [[0, 1, 2], [9, 8, 7]], # instance 1 [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors) [[6, 7, 8], [6, 5, 4]], # instance 3 [[9, 0, 1], [3, 2, 1]], # instance 4 ])seq_length_batch = np.array([2, 1, 2, 2])with tf.Session() as sess: init.run() outputs_val, states_val = sess.run( [outputs, states], feed_dict=&#123;X: X_batch, seq_length: seq_length_batch&#125;) print("outputs_val.shape:\n", outputs_val.shape, "\n", "states_val.shape:\n", states_val.shape) print("outputs_val:\n", outputs_val, "\n", "states_val:\n", states_val) 123456789101112131415161718192021outputs_val.shape: (4, 2, 5) states_val.shape: (4, 5)outputs_val: [[[ 0.22116265 -0.22573735 -0.14904192 0.37703517 -0.30065483] [-0.7885206 -0.98389083 -0.9981788 -0.69401765 -0.8436984 ]] [[-0.05911613 -0.8660758 -0.8889632 0.5718673 -0.53511584] [ 0. 0. 0. 0. 0. ]] [[-0.5683394 -0.9810551 -0.9934057 0.52230877 -0.7324221 ] [-0.78916883 -0.9947928 -0.9948283 -0.46843335 -0.8880266 ]] [[-0.23682797 -0.6044112 -0.93784267 -0.879141 -0.9476821 ] [-0.37557948 -0.73722005 -0.9297143 -0.5171338 -0.955366 ]]] states_val: [[-0.7885206 -0.98389083 -0.9981788 -0.69401765 -0.8436984 ] [-0.05911613 -0.8660758 -0.8889632 0.5718673 -0.53511584] [-0.78916883 -0.9947928 -0.9948283 -0.46843335 -0.8880266 ] [-0.37557948 -0.73722005 -0.9297143 -0.5171338 -0.955366 ]] 标题 说明 附加 看图理解长短期记忆网络与门控循环网络 翻译自 Illustrated Guide to LSTM’s and GRU’s: A step by step explanation 20181013 图解RNN、RNN变体、Seq2Seq、Attention机制 何之源 [译] 理解 LSTM 网络 译自 Christopher Olah 的博文 20180828 tf.nn.dynamic_rnn返回值详解 迗迗迗蘫 20180801]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>GRU</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注意力机制]]></title>
    <url>%2F2018%2F10%2F16%2F%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Attention是一种用于提升基于RNN（LSTM或GRU）的Encoder + Decoder模型的效果的的机制（Mechanism），一般称为Attention Mechanism。Attention Mechanism目前非常流行，广泛应用于机器翻译、语音识别、图像标注（Image Caption）等很多领域，之所以它这么受欢迎，是因为Attention给模型赋予了区分辨别的能力，例如，在机器翻译、语音识别应用中，为句子中的每个词赋予不同的权重，使神经网络模型的学习变得更加灵活（soft），同时Attention本身可以做为一种对齐关系，解释翻译输入/输出句子之间的对齐关系，解释模型到底学到了什么知识，为我们打开深度学习的黑箱，提供了一个窗口。 标题 说明 附加 模型汇总24 - 深度学习中Attention Mechanism详细介绍：原理、分类及应用 首推 知乎 2017 目前主流的attention方法都有哪些？ attention机制详解 知乎 2017 Neural Machine Translation (seq2seq) Tutorial GitHub 以机器翻译为例讲解注意力机制 长期更新 Attention_Network_With_Keras 注意力模型的代码的实现与分析 代码解析 简书 20180617 Attention_Network_With_Keras 代码实现 GitHub 2018 各种注意力机制窥探深度学习在NLP中的神威 综述 机器之心 20181008 Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures 为什么使用自注意力机制？ 实验结果证明：1）自注意力网络和 CNN 在建模长距离主谓一致时性能并不优于 RNN；2）自注意力网络在词义消歧方面显著优于 RNN 和 CNN。 20180827 以2014《Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation》抛砖引玉要介绍Attention Mechanism结构和原理，首先需要介绍下Seq2Seq模型的结构。基于RNN的Seq2Seq模型主要由两篇论文介绍，只是采用了不同的RNN模型。Ilya Sutskever等人与2014年在论文《Sequence to Sequence Learning with Neural Networks》中使用LSTM来搭建Seq2Seq模型。随后，2015年，Kyunghyun Cho等人在论文《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》提出了基于GRU的Seq2Seq模型。两篇文章所提出的Seq2Seq模型，想要解决的主要问题是，如何把机器翻译中，变长的输入X映射到一个变长输出Y的问题，其主要结构如图所示。 传统的Seq2Seq结构 其中，Encoder把一个变成的输入序列x1，x2，x3….xt编码成一个固定长度隐向量（背景向量，或上下文向量context）c，c有两个作用：1、做为初始向量初始化Decoder的模型，做为decoder模型预测y1的初始向量。2、做为背景向量，指导y序列中每一个step的y的产出。Decoder主要基于背景向量c和上一步的输出yt-1解码得到该时刻t的输出yt，直到碰到结束标志（$$）为止。 如上文所述，传统的Seq2Seq模型对输入序列X缺乏区分度，因此，2015年，Kyunghyun Cho等人在论文《Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation》中，引入了Attention Mechanism来解决这个问题，他们提出的模型结构如图所示。 Attention Mechanism模块图解 以Attention_Network_With_Keras 为例讲解一种Attention实现代码部分代码12345678910Tx = 50 # Max x sequence lengthTy = 5 # y sequence lengthX, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)# Split data 80-20 between training and testtrain_size = int(0.8*m)Xoh_train = Xoh[:train_size]Yoh_train = Yoh[:train_size]Xoh_test = Xoh[train_size:]Yoh_test = Yoh[train_size:] To be careful, let’s check that the code works: 1234567891011i = 5print("Input data point " + str(i) + ".")print("")print("The data input is: " + str(dataset[i][0]))print("The data output is: " + str(dataset[i][1]))print("")print("The tokenized input is:" + str(X[i]))print("The tokenized output is: " + str(Y[i]))print("")print("The one-hot input is:", Xoh[i])print("The one-hot output is:", Yoh[i]) Input data point 5. The data input is: 23 min after 20 p.m. The data output is: 20:23 The tokenized input is:[ 5 6 0 25 22 26 0 14 19 32 18 30 0 5 3 0 28 2 25 2 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40] The tokenized output is: [ 2 0 10 2 3] The one-hot input is: [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [1. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 1.] [0. 0. 0. ... 0. 0. 1.] [0. 0. 0. ... 0. 0. 1.]] The one-hot output is: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]] ModelOur next goal is to define our model. The important part will be defining the attention mechanism and then making sure to apply that correctly. Define some model metadata: 12layer1_size = 32layer2_size = 128 # Attention layer The next two code snippets defined the attention mechanism. This is split into two arcs: Calculating context Creating an attention layer As a refresher, an attention network pays attention to certain parts of the input at each output time step. _attention_ denotes which inputs are most relevant to the current output step. An input step will have attention weight ~1 if it is relevant, and ~0 otherwise. The _context_ is the “summary of the input”. The requirements are thus. The attention matrix should have shape $(T_x)$ and sum to 1. Additionally, the context should be calculated in the same manner for each time step. Beyond that, there is some flexibility. This notebook calculates both this way: attention = Softmax(Dense(Dense(x, y_{t-1}))) context = \sum_{i=1}^{m} ( attention_i * x_i )For safety, $y_0$ is defined as $\vec{0}$. 12345678910111213141516171819202122232425262728293031323334# Define part of the attention layer gloablly so as to# share the same layers for each attention step.def softmax(x): return K.softmax(x, axis=1)at_repeat = RepeatVector(Tx)at_concatenate = Concatenate(axis=-1)at_dense1 = Dense(8, activation="tanh")at_dense2 = Dense(1, activation="relu")at_softmax = Activation(softmax, name='attention_weights')at_dot = Dot(axes=1)def one_step_of_attention(h_prev, a): """ Get the context. Input: h_prev - Previous hidden state of a RNN layer (m, n_h) a - Input data, possibly processed (m, Tx, n_a) Output: context - Current context (m, Tx, n_a) """ # Repeat vector to match a's dimensions h_repeat = at_repeat(h_prev) # Calculate attention weights i = at_concatenate([a, h_repeat]) i = at_dense1(i) i = at_dense2(i) attention = at_softmax(i) # Calculate the context context = at_dot([attention, a]) return context 123456789101112131415161718192021222324252627282930def attention_layer(X, n_h, Ty): """ Creates an attention layer. Input: X - Layer input (m, Tx, x_vocab_size) n_h - Size of LSTM hidden layer Ty - Timesteps in output sequence Output: output - The output of the attention layer (m, Tx, n_h) """ # Define the default state for the LSTM layer h = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)), name='h_attention_layer')(X) c = Lambda(lambda X: K.zeros(shape=(K.shape(X)[0], n_h)), name='c_attention_layer')(X) # Messy, but the alternative is using more Input() at_LSTM = LSTM(n_h, return_state=True, name='at_LSTM_attention_layer') output = [] # Run attention step and RNN for each output time step for _ in range(Ty): context = one_step_of_attention(h, X) h, _, c = at_LSTM(context, initial_state=[h, c]) output.append(h) return output The sample model is organized as follows: BiLSTM Attention Layer Outputs Ty lists of activations. Dense Necessary to convert attention layer’s output to the correct y dimensions 12345678910111213141516171819202122232425262728293031layer3 = Dense(machine_vocab_size, activation=softmax)def get_model(Tx, Ty, layer1_size, layer2_size, x_vocab_size, y_vocab_size): """ Creates a model. input: Tx - Number of x timesteps Ty - Number of y timesteps size_layer1 - Number of neurons in BiLSTM size_layer2 - Number of neurons in attention LSTM hidden layer x_vocab_size - Number of possible token types for x y_vocab_size - Number of possible token types for y Output: model - A Keras Model. """ # Create layers one by one X = Input(shape=(Tx, x_vocab_size), name='X_Input') a1 = Bidirectional(LSTM(layer1_size, return_sequences=True), merge_mode='concat', name='Bid_LSTM')(X) a2 = attention_layer(a1, layer2_size, Ty) a3 = [layer3(timestep) for timestep in a2] # Create Keras model model = Model(inputs=[X], outputs=a3) return model The steps from here on out are for creating the model and training it. Simple as that. 12# Obtain a model instancemodel = get_model(Tx, Ty, layer1_size, layer2_size, human_vocab_size, machine_vocab_size) 1plot_model(model, to_file='Attention_tutorial_model_copy.png', show_shapes=True) 模型结构及说明 模型评估EvaluationThe final training loss should be in the range of 0.02 to 0.5 The test loss should be at a similar level. 1234# Evaluate the test performanceoutputs_test = list(Yoh_test.swapaxes(0,1))score = model.evaluate(Xoh_test, outputs_test)print('Test loss: ', score[0]) 2000/2000 [==============================] - 2s 1ms/step Test loss: 0.4966005325317383 Now that we’ve created this beautiful model, let’s see how it does in action. The below code finds a random example and runs it through our model. 1234567891011121314151617# Let's visually check model output.import random as randomi = random.randint(0, m)def get_prediction(model, x): prediction = model.predict(x) max_prediction = [y.argmax() for y in prediction] str_prediction = "".join(ids_to_keys(max_prediction, machine_vocab)) return (max_prediction, str_prediction)max_prediction, str_prediction = get_prediction(model, Xoh[i:i+1])print("Input: " + str(dataset[i][0]))print("Tokenized: " + str(X[i]))print("Prediction: " + str(max_prediction))print("Prediction text: " + str(str_prediction)) Input: 13.09 Tokenized: [ 4 6 2 3 12 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40] Prediction: [1, 3, 10, 0, 9] Prediction text: 13:09 Last but not least, all introductions to Attention networks require a little tour. The below graph shows what inputs the model was focusing on when writing each individual letter. 注意力机制图 注意力机制精要全局注意力机制 局部注意力机制 自注意力机制 隐藏向量 $h_t$ 首先会传递到全连接层。然后校准系数 $a_t$ 会对比全连接层的输出 $u_t$ 和可训练上下文向量 u（随机初始化），并通过 Softmax 归一化而得出。注意力向量 s 最后可以为所有隐藏向量的加权和。上下文向量可以解释为在平均上表征的最优单词。但模型面临新的样本时，它会使用这一知识以决定哪一个词需要更加注意。在训练中，模型会通过反向传播更新上下文向量，即它会调整内部表征以确定最优词是什么。 Self Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。因此，self Attention Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系 层级注意力机制 在该架构中，自注意力机制共使用了两次：在词层面与在句子层面。该方法因为两个原因而非常重要，首先是它匹配文档的自然层级结构（词——句子——文档）。其次在计算文档编码的过程中，它允许模型首先确定哪些单词在句子中是非常重要的，然后再确定哪个句子在文档中是非常重要的。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>注意力机制</tag>
        <tag>Attention Mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵形式的前馈神经网络]]></title>
    <url>%2F2018%2F10%2F12%2F%E7%9F%A9%E9%98%B5%E5%BD%A2%E5%BC%8F%E7%9A%84%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[前馈神经网络的符号定义 符号 意义 $m$ 样本个数。 $k$ 样本标签向量的维度。 $d^l$ 神经网络第 l 层神经元的个数。 $\sigma(·)$ 是某一种常用的激活函数。比如 sigmoid、tanh 和 ReLu 函数等。 $\rho(·)$ 神经网络输出层的变换函数。比如 logistic 和 Softmax 函数等。 $C$ 代价函数。 $Loss$ 损失函数，意义同 $C$ 。 $sum(·)$ 求和函数。可以根据矩阵行或列求和，比如 $sum(D, axis=0)$ 就是按照矩阵 $D$ 的行求和。 $where(·)$ 条件函数。可以根据矩阵元素满足的条件操作，比如 ${Y}=where(A&gt;=0.5, 1, 0)$ 就是若矩阵 $A$ 中元素 $a_{ij}$ 大于等于 0.5 则 $Y$ 矩阵中对应位置元素 $y_{ij}$ 的值取 1 否则取 0。 $*$ 按矩阵元素乘积。比如 $A * B$。 $\odot$ 按矩阵元素乘积。比如 $A \odot B$。 符号 向量形式 维度 意义 $x$ $[x_0,x_1,…,x_n]^T$ $(n, 1)$ $x$ 表示一个有 $n$ 维特征样本实例， $x_i$ 表示向量的第 $i$ 维特征。 $w_{jk}^{l}$ 标量 $(1, 1)$ 是从 $l-1$ 层的第 $k$ 个神经元到 $l$ 层的第 $j$ 个神经元的权重。 $b_j^l$ 标量 $(1, 1)$ 是第 $l$ 层的第 $j$ 个神经元的偏置。 $z_j^l$ 标量 $(1, 1)$ 是第 $l$ 层的第 $j$ 个神经元的带权输入。 $a_j^l$ 标量 $(1, 1)$ 是第 $l$ 层的第 $j$ 个神经元的激活值。 $y_j$ 标量 $(1, 1)$ 样本标签向量的第 $j$ 个值。 $\hat y_j$ 标量 $(1, 1)$ 估计的样本标签向量的第 $j$ 个值。 $\delta_j^l$ 标量 $(1, 1)$ 是第 $l$ 层的第 $j$ 个神经元的误差值。 $z^l$ $[z_0,z_1,…,z_{d^l}]^T$ $(d^l, 1)$ 表示第第 $l$ 层的带权输入。第 $j$ 个元素是 $z_j^l$。 $a^{l}$ $[a_0,a_1,…,a_{d^l}]^T$ $(d^l, 1)$ 是激活向量。第 $j$ 行的元素是 $a_j^l$。 $b^l$ $[b_0,b_1,…,b_{d^l}]^T$ $(d^l, 1)$ 偏置向量，第 $j$ 行的元素是 $b_j^l$ $W^{l}$ $\left[\begin{matrix} w_{i1}^l &amp; w_{i2}^l &amp;···&amp; w_{ik}^l &amp; \\ \\w_{j1}^l &amp; w_{j2}^l &amp;···&amp; w_{jk}^l\end{matrix} \right]$ $(d^l, d^{l-1})$ 权重矩阵，第 $j$ 行 $k$ 列的元素是 $w_{jk}^{l}$。 $\delta^l$ $[\delta_0,\delta_1,…,\delta_{d^l}]^T$ $(d^l, 1)$ 是误差向量，$\delta^l$ 的第 $j$ 个元素是 $\delta_j^l$。 $z^{v,l}$ $[z_0,z_1,…,z_{d^l}]^T$ $(d^l, 1)$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的带权输入向量。 $a^{v,l}$ $[a_0,a_1,…,a_{d^l}]^T$ $(d^l, 1)$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的激活向量。 $\delta^{v,l}$ $[\delta_0,\delta_1,…,\delta_{d^l}]^T$ $(d^l, 1)$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的误差。 $y$ $[y_0,y_1,…,y_{k}]^T$ $(k, 1)$ 标签向量，第 $j$ 个值是 $y_j$。 $\hat y$ $[\hat y_0,\hat y_1,…,\hat y_{k}]^T$ $(k, 1)$ 估计的标签向量，第 $j$ 个值是 $\hat y_j$。 $X$ $[{(x^{(1)})}^T,{(x^{(2)})}^T,…,{(x^{(m)})}^T]^T$ $(m, n)$ 样本矩阵，第 $i$ 行是第 $i$ 个样本向量 $x^{(i)}$ 的转置。 $Y$ $[{(y^{(1)})}^T,{(y^{(2)})}^T,…,{(y^{(m)})}^T]^T$ $(m, k)$ 标签矩阵，第 $i$ 行是第 $i$ 个标签向量 $y^{(i)}$ 的转置。 $\hat Y$ $[{(\hat y^{(1)})}^T,{(\hat y^{(2)})}^T,…,{(\hat y^{(m)})}^T]^T$ $(m, k)$ 估计的标签矩阵，第 $i$ 行是第 $i$ 个标签向量 $\hat y^{(i)}$ 的转置。 $Z^l$ $[{(z^{1,l})}^T,{(z^{2,l})}^T,…,{(z^{m,l})}^T]^T$ $(m, d^l)$ 带权输入矩阵，第 $v$ 行是第 $v$ 个样本在第 $l$ 层的带权输入向量 $z^{v,l}$ 的转置。 $A^l$ $[{(a^{1,l})}^T,{(a^{2,l})}^T,…,{(a^{m,l})}^T]^T$ $(m, d^l)$ 激活矩阵，第 $v$ 行是第 $v$ 个样本在第 $l$ 层的激活向量 $a^{v,l}$ 的转置。 $\Delta^l$ $[{(\delta^{1,l})}^T,{(\delta^{2,l})}^T,…,{(\delta^{m,l})}^T]^T$ $(m, d^l)$ 误差矩阵，第 $v$ 行是第 $v$ 个样本在第 $l$ 层的误差向量 $\delta^{v,l}$ 的转置。 前向传播算法的基本公式 计算公式 维度变化 $a^1=x$ $(n, 1) = (n, 1)$ $z^l = W^l a^{l-1}+b^l$ $({d}^l, 1) =({d}^{l}, {d}^{l-1}) (d^{l-1}, 1) + (d^l, 1)$ $a^{l} =\sigma{(z^{l})}$ $(d^l, 1) = (d^l, 1)$ $a^{L} =\varphi(z^{L})$ $(?, 1) \leftarrow (d^L, 1)$ $\hat{y}=\rho(a^L)$ $(?, 1) \leftarrow (?, 1)$ 前馈神经网络可以通过逐层信息传递，得到网络最后的输出 $a^L$。整个网络可以看做一个复合函数 $\varphi(x;W,b)$，将向量 $x$ 作为第一层的输入 $a^1$，将第 $L$ 层的输出 $a^L$ 作为整个函数的输出。 x = a^1 \rightarrow z^2 \rightarrow a^2 \rightarrow ··· \rightarrow a^{L-1} \rightarrow z^L \rightarrow a^L=\varphi(x;W,b)其中 $W, b$ 表示网络中所有层的连接权重和偏置。 矩阵形式前向传播算法的计算公式 计算公式 维度变化 $A^1=X$ $(m, n) = (m, n)$ $Z^l=A^{l-1}{(W^{l})}^T + {(b^l)}^T$ $(m, {d}^l) = (m, {d}^{l-1}) {({d}^{l}, {d}^{l-1})}^T + {(d^{l},1)}^T$ $A^l=\sigma(Z^l)$ $(m, d^l) = (m, d^l)$ $A^L=\varphi(Z^L)$ $(m, ?) \leftarrow (m, d^L)$ $\hat{Y}=\rho(A^L)$ $(m, ?) \leftarrow (m, ?)$ 矩阵形式前向传播算法的计算公式不过是把原来的向量 $x$ 装进了矩阵 $X$ 当中，网络最终的输出变成了 $A^L$。整个网络仍然可以看做一个复合函数 $\varphi(X;W,b)$。 X = A^1 \rightarrow Z^2 \rightarrow A^2 \rightarrow ··· \rightarrow A^{L-1} \rightarrow Z^L \rightarrow A^L=\varphi(X;W,b)其中 $W, b$ 表示网络中所有层的连接权重和偏置。 反向传播算法的四个基本公式有关反向传播算法的理解和证明见《反向传播算法》 计算公式 维度变化 $\delta^L =\nabla_a C\odot \dfrac{\partial \varphi^{(L)}}{\partial z^L} $ $(d^L, 1) = (d^L, 1) * (d^L, 1)$ $\delta^{(l)} =({(W^{(l+1)})}^T\delta^{(l+1)})\odot \sigma’(z^l) $ $(d^l, 1) = {({d}^{l+1}, {d}^{l})}^T (d^{l+1}, 1) * (d^l, 1)$ $\dfrac{\partial C}{\partial b^{(l)}} =\delta^{(l)}$ $(d^l, 1) = (d^l, 1)$ $\dfrac{\partial C}{\partial W^l} = \delta^l (a^{l-1})^T $ $(d^l, d^{l-1}) = (d^l,1) {(d^{l-1}, 1)}^T$ 对于代价函数函数有两种写法 $C$ 和 $Loss$，它两在本文中是一个意思。 矩阵形式反向传播四个基本公式反向传播算法对一个训练样本计算代价函数的梯度，$C=C_x$。在实践中，通常将反向传播算法和诸如随机梯度下降这样的学习算法进行组合使用，我们会对许多训练样本计算对应的梯度。特别地，给定一个大小为 m 的小批量数据，下面的算法在这个小批量数据的基础上应用梯度下降学习算法： 下面给出矩阵形式的反向传播算法： 计算公式 维度变化 $\Delta^L =\nabla_{A^L} C\odot \dfrac{\partial \varphi^{(L)}}{\partial Z^L}$ $(m, d^L) = (m, d^L) * (m, d^L)$ $\Delta^{l} =(\Delta^{l+1}W^{l+1})\odot \sigma’(Z^l)$ $(m, {d}^l) = (m, {d}^{l+1}) ({d}^{l+1}, {d}^l)$ $\dfrac{\partial C}{\partial b^{(l)}} =\dfrac{1}{m}{(sum(\Delta^l, axis=0))}^T$ $(d^l, 1) = {(1, d^l)}^T$ $\dfrac{\partial C}{\partial W^{(l)}} =\dfrac{1}{m}{(\Delta^l)}^TA^{l-1} $ $(d^{l}, d^{l-1}) = {(m, d^l)}^T(m, d^{l-1})$ 前馈神经网络与传统机器学习算法的关系从理论上的分析 目前大部分的机器学习算法是将特征处理（即前三步）和预测分开的，并且主要关注于最后一步，即构建预测函数。但是实际操作过程中，不同预测模型的性能相差不多，而前三步中的特征处理对最终系统的准确性有着十分关键的作用。由于特征处理一般都需要人工干预完成，利用人类的经验来选取好的“特征”，并最终提高机器学习系统的性能。因此，很多的模式识别问题变成了特征工程（Feature Engineering）问题。开发一个机器学习系统的主要工作量都消耗在了预处理和特提取以及征转换上。 在处理好特征后，传统机器学习模型主要关注于分类或预测。这类机器学习称为浅层学习（Shallow Learning）。浅层学习的一个重要特点是不涉及特征学习，其特征主要靠人工经验或特征转换方法来抽取。 为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的特征（feature），也叫表示（representation）。如果有一种算法可以自动地学习出有效的特征，并提高最终分类器的性能，那么这种学习就是可以叫做表示学习（Representation Learning）。 深度学习（Deep Learning，DL）是指如何从数据中学习一个“深度模型”的问题，是机器学习的一个子问题。通过构建具有一定“深度”的模型，可以让模型来自动学习好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测或识别的准确性。下图给出了深度学习的数据处理流程。 深度学习的主要目的是从数据中自动学习到有效的特征，即表示学习。深度学习技术在一定程度上可以看作是一个表示学习技术，通过多层的非线性转换，把原始数据变成为更高层次、更抽象的表示。这些学习到的表示可以替代人工设计的特征，从而避免“特征工程”。 从数学层面的分析前馈神经网络具有很强的拟合能力，常见的连续非线性函数都可以用前馈神经网络来近似。 根据通用近似定理，神经网络在某种程度上可以作为一个“万能”函数来使用，可以用来进行复杂的特征转换，或逼近一个复杂的条件分布。在机器学习中，输入样本的特征对分类器的影响很大。以监督学习为例，好的特征可以极大提高分类器的性能。因此，要取得好的分类效果，需要样本的原始特征向量x 转换到更有效的特征向量φ(x)，这个过程叫做特征抽取。 多层前馈神经网络可以看作是一个非线性复合函数$\varphi: R^p\rightarrow R^q$，将输入$x\in R^p$ 映射到输出 $\varphi(x)\in R^q$。因此，多层前馈神经网络也可以看成是一种特征转换方法，其输出 $\varphi(x)$ 作为分类器的输入进行分类。 给定一个训练样本(x, y)，先利用多层前馈神经网络将x 映射到φ(x)，然后再将φ(x) 输入到分类器 $\rho(·)$ 。其中 $\rho(·)$ 为线性或非线性的分类器，$\theta$ 为分类器 $\rho(·)$ 的参数，$\hat y$ 为分类器的输出。 \hat y = \rho(\varphi(x), \theta)特别地，如果分类器g(·) 为 logistic 回归分类器或softmax 回归分类器，那么 g(·) 也可以看成是网络的最后一层，即神经网络直接输出不同类别的后验概率。 对于两类分类问题y 2 {0, 1}，并采用logistic 回归，那么 logistic 回归分类器可以看成神经网络的最后一层。也就是说，网络的最后一层只用一个神经元，并且其激活函数为logistic 函数。网络的输出可以直接可以作为两个类别的后验概率。 p(y=1|x)=a^{(L)}其中 $a^{(L)}\in R$ 为第 L 层神经元的激活值。 对于多类分类问题 $y\in \{1,2,…,K \}$，如果使用 softmax 回归分类器，相当于网络最后一层设置 K 个神经元，其输出经过softmax 函数进行归一化后可以作为每个类的后验概率。 \hat y=softmax(z^{(L)})其中 $z^{(L)}\in R$ 为第 L 层神经元的带权输入，$\hat y\in R^K$ 为第 L 层神经元的激活值，分别表示不同类别标签的预测后验概率。 反之，logistic 回归或 softmax 回归也可以看作是只有一层的神经网络。 深度学习的优势在一些复杂的模式识别任务中，传统机器学习方法需要将一个模式识别任务的输入和输出之间人为地切割成很多子模块（或多个阶段），每个子模块分开学习。比如一个自然语言理解任务，一般需要分词、词性标注、句法分析、语义分析、语义推理等步骤。这种学习方式有两个问题：一是每一个模块都需要单独优化，并且其优化目标和任务总体目标并不能保证一致。二是错误传播，即前一步的错误会对后续的模型造成很大的影响。这样就增加了机器学习方法在实际应用的难度。 端到端学习（End-to-End Learning），也称端到端训练，是指在学习过程中不进行分模块或分阶段进行训练，直接优化任务的总体目标。在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预。端到端学习的训练数据为“输入-输出”对的形式，无需提供其它额外信息。因此，端到端学习和深度学习一样，都是要解决“贡献度分配”问题。 神经网络应用到逻辑回归机器学习中矩阵形式的逻辑回归模型计算公式如下： 模型名称 模型概率输出函数 模型的分类决策函数 模型损失函数 模型损失函数的梯度 线性回归模型|$\hat{p} = \sigma(\boldsymbol X\theta)$|$\hat{y}=where(\hat{p}&gt;=0.5, 1, 0)$|$J(\boldsymbol{\theta}) =-\dfrac{1}{m}[y^T log(\hat{p})+(1-y^T)log(1-\hat{p})]$|$\nabla_{W} Loss = \frac{1}{m}{X}^T(p-y)$| 关于上面内容的具体分析见《矩阵形式的逻辑回归模型》，下面根据神经网络的符号和定义对上面的符号做一些修改，本质上是一致的。 神经网络中的逻辑回归模型： 模型名称 模型概率输出函数 模型的分类决策函数 神经网络线性回归模型|$A^{L} = logistic(Z^{L})$|$\hat{Y}=where(A^{L}&gt;=0.5, 1, 0)$| 模型损失函数 模型输出层损失函数的梯度 $Loss =-\dfrac{1}{m}[Y^T log(A^{L})+(1-Y^T)log(1-A^{L})]$ $\Delta^{L} = A^{(L)}-Y$ 逻辑回归实例分析 针对该例子的符号说明如下： 符号 维度 意义 $X$ (m,2) 样本矩阵 $W^2$ (3,2) 第二层的权重矩阵 $W^3$ (1,3) 第三层的权重矩阵 $b^2$ (3,1) 第二层的偏置矩阵 $b^3$ (1,1) 第三层的偏置矩阵 $Z^2$ (m,3) 第二层的带权输入矩阵 $Z^3$ (m,1) 第三层的带权输入矩阵 $A^1$ (m,2) 第一层的激活矩阵 $A^2$ (m,3) 第二层的激活矩阵 $A^3$ (m,1) 第三层的激活矩阵 $\hat Y$ (m,1) 输出矩阵 $Y$ (m,1) 标签矩阵 前向传播过程 计算公式 维度变化 $A^1=X$ $(m, 2) = (m, 2)$ $Z^2=A^1{(W^2)}^T + {(b^2)}^T$ $(m, 3) = (m, 2) {(3, 2)}^T + {(3, 1)}^T$ $A^2=\sigma(Z^2)$ $(m, 3) = (m, 3)$ $Z^3=A^2{(W^3)}^T + {(b^3)}^T$ $(m, 1) = (m, 3) {(3, 1)}^T + {(1, 1)}^T$ $A^3=sigmoid(Z^3)$ $(m, 1) = (m, 1)$ $\hat{Y}=where(A^{(3)} &gt;= 0.5, 1, 0)$ $(m, 1) = (m, 1)$ 反向传播过程 计算公式 维度变化 $\Delta^3=A^3-Y$ $(m, 1) = (m, 1) - (m, 1)$ $\dfrac{\partial C}{\partial b^{3}} =\dfrac{1}{m}{(sum(\Delta^3, axis=0))}^T $ $(1, 1) = {(1, 1)}^T$ $\dfrac{\partial C}{\partial W^{3}} =\dfrac{1}{m}{(\Delta^3)}^TA^{2} $ $(1, 3) = {(m, 1)}^T (m, 3)$ $\Delta^{2} =(\Delta^{3}W^{3})\odot \sigma’(Z^2) $ $(m, 3) = ((m, 1) (1, 3)) * (m, 3)$ $\dfrac{\partial C}{\partial b^{2}} =\dfrac{1}{m}{(sum(\Delta^2, axis=0))}^T $ $(3, 1) = {(1, 3)}^T$ $\dfrac{\partial C}{\partial W^{2}} =\dfrac{1}{m}{(\Delta^2)}^TA^{1} $ $(3, 2) = {(m, 3)}^T (m, 2)$ 神经网络应用到 Softmax 多分类模型 模型名称 模型概率输出函数 模型分类决策函数 模型损失函数 模型损失函数的梯度 Softmax分类模型 $ A =softmax(X W)$ $\hat{Y} =argmax(softmax(XW), axis=0)$ $Loss = - \dfrac{1}{m}(sum(Y\odot log (A)))$ $\nabla_{W} Loss =\frac{1}{m}{X}^T(A-{Y})$ 关于上面内容的具体分析见《矩阵形式的Softmax多分类模型》，下面根据神经网络的符号和定义对上面的符号做一些修改，本质上是一致的。 神经网络应用到 Softmax 多分类模型， 模型名称 模型概率输出函数 模型分类决策函数 神经网络Softmax分类模型 $ A^L=softmax(Z^L)$ $\hat{Y} =argmax(A^L, axis=0)$ 模型损失函数 模型输出层损失函数的梯度 $Loss = - \dfrac{1}{m}(sum(Y\odot log (A^L)))$ $\Delta^{(L)} = A^{(L)}-Y$ Softmax 回归实例分析 针对该例子的符号说明如下： 符号 维度 意义 $X$ (m,3) 样本矩阵 $W^2$ (5,3) 第二层的权重矩阵 $W^3$ (4,5) 第三层的权重矩阵 $b^2$ (5,1) 第二层的偏置矩阵 $b^3$ (4,1) 第三层的偏置矩阵 $Z^2$ (m,5) 第二层的带权输入矩阵 $Z^3$ (m,4) 第三层的带权输入矩阵 $A^1$ (m,3) 第一层的激活矩阵 $A^2$ (m,5) 第二层的激活矩阵 $A^3$ (m,4) 第三层的激活矩阵 $\hat Y$ (m,1) 输出矩阵 $Y$ (m,1) 标签矩阵 前向传播过程 计算公式 维度变化 $A^1=X$ $(m, 3) = (m, 3)$ $Z^2=A^1{(W^2)}^T + {(b^2)}^T$ $(m, 5) = (m, 3) {(5, 3)}^T + {(5, 1)}^T$ $A^2=\sigma(Z^2)$ $(m, 5) = (m, 5)$ $Z^3=A^2{(W^3)}^T + {(b^3)}^T$ $(m, 4) = (m, 5){(4, 5)}^T + {(4, 1)}^T$ $A^3=softmax(Z^3)$ $(m, 4) = (m, 4)$ $\hat{Y} =argmax(A^3, axis=0)$ $(m, 1) = (m, 1)$ 反向传播过程 计算公式 维度变化 $\Delta^3=A^3-Y$ $(m, 4) = (m, 4) - (m, 4)$ $\dfrac{\partial C}{\partial b^{3}} =\dfrac{1}{m}{(sum(\Delta^3, axis=0))}^T $ $(4, 1) = {(1, 4)}^T$ $\dfrac{\partial C}{\partial W^{3}} =\dfrac{1}{m}{(\Delta^3)}^TA^{2} $ $(4, 5) = {(m, 4)}^T (m, 5)$ $\Delta^{2} =(\Delta^{3}W^{3})\odot \sigma’(Z^2) $ $(m, 5) = ((m, 4) (4, 5)) * (m, 5)$ $\dfrac{\partial C}{\partial b^{2}} =\dfrac{1}{m}{(sum(\Delta^2, axis=0))}^T $ $(5, 1) = {(1, 5)}^T$ $\dfrac{\partial C}{\partial W^{2}} =\dfrac{1}{m}{(\Delta^2)}^TA^{1} $ $(5, 3) = {(m, 5)}^T (m, 3)$ 神经网络应用到 logistic 多标签分类模型 模型名称 模型概率输出函数 模型分类决策函数 模型损失函数 模型损失函数的梯度 Sigmoid多标签分类模型 $A = \sigma(XW)$ $\hat{Y}=where(A &gt;=0.5, 1, 0)$ $Loss = - \dfrac{1}{m}(sum(Y\odot log (A)+(1-Y) \odot{A} ))$ $\nabla_{W} Loss =\frac{1}{m}{X}^T(A-{Y})$ 关于上面内容的具体分析见《矩阵形式的Sigmoid多标签分类模型》，下面根据神经网络的符号和定义对上面的符号做一些修改，本质上是一致的。 神经网络应用到 logistic 多标签分类模型， 模型名称 模型概率输出函数 模型分类决策函数 神经网络Logistic多标签分类模型 $ A^L=logistic(Z^L)$ $\hat{Y}=where(A^L &gt;=0.5, 1, 0), axis=0$ 模型损失函数 模型输出层损失函数的梯度 $Loss = - \dfrac{1}{m}(sum(Y\odot log (A^L)))$ $\Delta^{(L)} = A^{(L)}-Y$ logistic 多标签分类实例分析 针对该例子的符号说明如下： 符号 维度 意义 $X$ (m,3) 样本矩阵 $W^2$ (5,3) 第二层的权重矩阵 $W^3$ (4,5) 第三层的权重矩阵 $b^2$ (5,1) 第二层的偏置矩阵 $b^3$ (4,1) 第三层的偏置矩阵 $Z^2$ (m,5) 第二层的带权输入矩阵 $Z^3$ (m,4) 第三层的带权输入矩阵 $A^1$ (m,3) 第一层的激活矩阵 $A^2$ (m,5) 第二层的激活矩阵 $A^3$ (m,4) 第三层的激活矩阵 $\hat Y$ (m,1) 输出矩阵 $Y$ (m,1) 标签矩阵 前向传播过程 计算公式 维度变化 $A^1=X$ $(m, 3) = (m, 3)$ $Z^2=A^1{(W^2)}^T + {(b^2)}^T$ $(m, 5) = (m, 3) {(5, 3)}^T + {(5, 1)}^T$ $A^2=\sigma(Z^2)$ $(m, 5) = (m, 5)$ $Z^3=A^2{(W^3)}^T + {(b^3)}^T$ $(m, 4) = (m, 5){(4, 5)}^T + {(4, 1)}^T$ $A^3=logistic(Z^3), axis=0$ $(m, 4) = (m, 4)$ $\hat{Y}=where(A &gt;=0.5, 1, 0), axis=0$ $(m, 4) = (m, 4)$ 反向传播过程 计算公式 维度变化 $\Delta^3=A^3-Y$ $(m, 4) = (m, 4) - (m, 4)$ $\dfrac{\partial C}{\partial b^{3}} =\dfrac{1}{m}{(sum(\Delta^3, axis=0))}^T $ $(4, 1) = {(1, 4)}^T$ $\dfrac{\partial C}{\partial W^{3}} =\dfrac{1}{m}{(\Delta^3)}^TA^{2} $ $(4, 5) = {(m, 4)}^T (m, 5)$ $\Delta^{2} =(\Delta^{3}W^{3})\odot \sigma’(Z^2) $ $(m, 5) = ((m, 4) (4, 5)) * (m, 5)$ $\dfrac{\partial C}{\partial b^{2}} =\dfrac{1}{m}{(sum(\Delta^2, axis=0))}^T $ $(5, 1) = {(1, 5)}^T$ $\dfrac{\partial C}{\partial W^{2}} =\dfrac{1}{m}{(\Delta^2)}^TA^{1} $ $(5, 3) = {(m, 5)}^T (m, 3)$ 神经网络应用到多元回归模型 模型名称 模型输出函数 模型损失函数 模型损失函数的梯度 线性回归模型|$A=X\theta$|$Loss=\frac{1}{m}{\Vert A^L-Y \Vert}^2$|$\nabla_{\boldsymbol{\theta}} Loss= \dfrac{2}{m} {X}^T (A - Y)$| 关于上面内容的具体分析见《矩阵形式的线性回归模型》，下面根据神经网络的符号和定义对上面的符号做一些修改，本质上是一致的。有一点需要注意的是，由于神经网络中使用了非线性的激活函数，所以神经网络有了拟合非线性函数的能力，所以这里是神经网络应用到多元回归模型，而不仅仅是神经网络应用到多元线性回归模型。 神经网络应用到多元回归模型， 模型名称 模型输出函数 模型损失函数 模型输出层损失函数的梯度 神经网络多元回归模型 $\hat Y=A^L$ $Loss=\frac{1}{m}{\Vert A^L-Y \Vert}^2$ $\Delta^{(L)}= 2(A - Y)$ 多元回归模型实例分析 针对该例子的符号说明如下： 符号 维度 意义 $X$ (m,3) 样本矩阵 $W^2$ (5,3) 第二层的权重矩阵 $W^3$ (4,5) 第三层的权重矩阵 $b^2$ (5,1) 第二层的偏置矩阵 $b^3$ (4,1) 第三层的偏置矩阵 $Z^2$ (m,5) 第二层的带权输入矩阵 $Z^3$ (m,4) 第三层的带权输入矩阵 $A^1$ (m,3) 第一层的激活矩阵 $A^2$ (m,5) 第二层的激活矩阵 $A^3$ (m,4) 第三层的激活矩阵 $\hat Y$ (m,1) 输出矩阵 $Y$ (m,1) 标签矩阵 前向传播过程 计算公式 维度变化 $A^1=X$ $(m, 3) = (m, 3)$ $Z^2=A^1{(W^2)}^T + {(b^2)}^T$ $(m, 5) = (m, 3) {(5, 3)}^T + {(5, 1)}^T$ $A^2=\sigma(Z^2)$ $(m, 5) = (m, 5)$ $Z^3=A^2{(W^3)}^T + {(b^3)}^T$ $(m, 4) = (m, 5){(4, 5)}^T + {(4, 1)}^T$ $A^3=Z^3$ $(m, 4) = (m, 4)$ $\hat Y = A^3$ $(m, 4) = (m, 4)$ 反向传播过程 计算公式 维度变化 $\Delta^3=A^3-Y$ $(m, 4) = (m, 4) - (m, 4)$ $\dfrac{\partial C}{\partial b^{3}} =\dfrac{1}{m}{(sum(\Delta^3, axis=0))}^T $ $(4, 1) = {(1, 4)}^T$ $\dfrac{\partial C}{\partial W^{3}} =\dfrac{1}{m}{(\Delta^3)}^TA^{2} $ $(4, 5) = {(m, 4)}^T (m, 5)$ $\Delta^{2} =(\Delta^{3}W^{3})\odot \sigma’(Z^2) $ $(m, 5) = ((m, 4) (4, 5)) * (m, 5)$ $\dfrac{\partial C}{\partial b^{2}} =\dfrac{1}{m}{(sum(\Delta^2, axis=0))}^T $ $(5, 1) = {(1, 5)}^T$ $\dfrac{\partial C}{\partial W^{2}} =\dfrac{1}{m}{(\Delta^2)}^TA^{1} $ $(5, 3) = {(m, 5)}^T (m, 3)$]]></content>
      <categories>
        <category>机器学习</category>
        <category>talk is cheap show me the prove</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>前馈神经网络</tag>
        <tag>证明</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SMP2018中文人机对话技术评测（ECDT）]]></title>
    <url>%2F2018%2F10%2F11%2FSMP2018%E4%B8%AD%E6%96%87%E4%BA%BA%E6%9C%BA%E5%AF%B9%E8%AF%9D%E6%8A%80%E6%9C%AF%E8%AF%84%E6%B5%8B%EF%BC%88ECDT%EF%BC%89%2F</url>
    <content type="text"><![CDATA[点击查看GitHub SMP2018 完整代码和解析 标题 说明 时间 CodaLab评测主页 数据下载 20181010 CodaLab 评测教程 20181010 评测排行榜 SMP2018-ECDT评测主页 SMP2018-ECDT评测成绩公告链接 根据SMP2018中文人机对话模型创建的应用简单的说就是给用户的问题分类，效果如下： .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } query label 0 今天东莞天气如何 weather 1 从观音桥到重庆市图书馆怎么走 map 2 鸭蛋怎么腌？ cookbook 3 怎么治疗牛皮癣 health 4 唠什么 chat 用户意图领域分类 在人机对话系统的应用过程中，用户可能会有多种意图，相应地会触发人机对话系统中的多个领域（domain） ，其中包括任务型垂直领域（如查询机票、酒店、公交车等）、知识型问答以及闲聊等。因而，人机对话系统的一个关键任务就是正确地将用户的输入分类到相应的领域（domain）中，从而才能返回正确的回复结果。 分类的类别说明 包含闲聊和垂类两大类，其中垂类又细分为30个垂直领域。 本次评测任务1中，仅考虑针对单轮对话用户意图的领域分类，多轮对话整体意图的领域分类不在此次评测范围之内。类别 = [‘website’, ‘tvchannel’, ‘lottery’, ‘chat’, ‘match’, &#39;datetime&#39;, &#39;weather&#39;, &#39;bus&#39;, &#39;novel&#39;, &#39;video&#39;, &#39;riddle&#39;, &#39;calc&#39;, &#39;telephone&#39;, &#39;health&#39;, &#39;contacts&#39;, &#39;epg&#39;, &#39;app&#39;, &#39;music&#39;, &#39;cookbook&#39;, &#39;stock&#39;, &#39;map&#39;, &#39;message&#39;, &#39;poetry&#39;, &#39;cinemas&#39;, &#39;news&#39;, &#39;flight&#39;, &#39;translation&#39;, &#39;train&#39;, &#39;schedule&#39;, &#39;radio&#39;, &#39;email&#39;] 开始使用 1from app import query_2_label Using TensorFlow backend. 1query_2_label('我喜欢你') Building prefix dict from the default dictionary ... Loading model from cache /tmp/jieba.cache Loading model cost 0.945 seconds. Prefix dict has been built succesfully. &#39;chat&#39; 运行下面代码进行查询，输入 0 结束查询 12345678while True: your_query_sentence = input() print('-'*10) label = query_2_label(your_query_sentence) print('predict label:\t', label) print('-'*10) if your_query_sentence=='0': break 今天东莞天气如何 ---------- predict label: datetime ---------- 怎么治疗感冒？ ---------- predict label: health ---------- 你好？ ---------- predict label: chat ---------- 与实验部分的分割线 SMP2018中文人机对话技术评测（ECDT） 下面是一个完整的针对 SMP2018中文人机对话技术评测（ECDT） 的实验，由该实验训练的基线模型能达到评测排行榜的前三的水平。 通过本实验，可以掌握处理自然语言文本数据的一般方法。 推荐自己修改此文件，达到更好的实验效果，比如改变以下几个超参数 123456# 词嵌入的维度embedding_word_dims = 32# 批次大小batch_size = 30# 周期epochs = 20 本实验还可以改进的地方举例 预处理阶段使用其它的分词工具 采用字符向量和词向量结合的方式 使用预先训练好的词向量 改变模型结构 改变模型超参数 导入依赖库123456789101112131415161718import numpy as npimport pandas as pdimport collectionsimport jiebafrom keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequencesfrom keras.models import Sequentialfrom keras.layers import Embedding, LSTM, Densefrom keras.utils import to_categorical,plot_modelfrom keras.callbacks import TensorBoard, Callbackfrom sklearn.metrics import classification_reportimport requestsimport timeimport os Using TensorFlow backend. 辅助函数1234567891011121314151617181920212223242526272829303132from keras import backend as K# 计算 F1 值的函数def f1(y_true, y_pred): def recall(y_true, y_pred): """Recall metric. Only computes a batch-wise average of recall. Computes the recall, a metric for multi-label classification of how many relevant items are selected. """ true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) possible_positives = K.sum(K.round(K.clip(y_true, 0, 1))) recall = true_positives / (possible_positives + K.epsilon()) return recall def precision(y_true, y_pred): """Precision metric. Only computes a batch-wise average of precision. Computes the precision, a metric for multi-label classification of how many selected items are relevant. """ true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) precision = true_positives / (predicted_positives + K.epsilon()) return precision precision = precision(y_true, y_pred) recall = recall(y_true, y_pred) return 2*((precision*recall)/(precision+recall+K.epsilon())) 123456# 获取自定义时间格式的字符串def get_customization_time(): # return '2018_10_10_18_11_45' 年月日时分秒 time_tuple = time.localtime(time.time()) customization_time = "&#123;&#125;_&#123;&#125;_&#123;&#125;_&#123;&#125;_&#123;&#125;_&#123;&#125;".format(time_tuple[0], time_tuple[1], time_tuple[2], time_tuple[3], time_tuple[4], time_tuple[5]) return customization_time 准备数据下载SMP2018官方数据12345678910111213raw_train_data_url = "https://worksheets.codalab.org/rest/bundles/0x0161fd2fb40d4dd48541c2643d04b0b8/contents/blob/"raw_test_data_url = "https://worksheets.codalab.org/rest/bundles/0x1f96bc12222641209ad057e762910252/contents/blob/"# 如果不存在 SMP2018 数据，则下载if (not os.path.exists('./data/train.json')) or (not os.path.exists('./data/dev.json')): raw_train = requests.get(raw_train_data_url) raw_test = requests.get(raw_test_data_url) if not os.path.exists('./data'): os.makedirs('./data') with open("./data/train.json", "wb") as code: code.write(raw_train.content) with open("./data/dev.json", "wb") as code: code.write(raw_test.content) 12345678def get_json_data(path): # read data data_df = pd.read_json(path) # change row and colunm data_df = data_df.transpose() # change colunm order data_df = data_df[['query', 'label']] return data_df 123train_data_df = get_json_data(path="data/train.json")test_data_df = get_json_data(path="data/dev.json") 1train_data_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } query label 0 今天东莞天气如何 weather 1 从观音桥到重庆市图书馆怎么走 map 2 鸭蛋怎么腌？ cookbook 3 怎么治疗牛皮癣 health 4 唠什么 chat 结巴分词示例，下面将使用结巴分词对原数据进行处理12seg_list = jieba.cut("他来到了网易杭研大厦") # 默认是精确模式print(list(seg_list)) Building prefix dict from the default dictionary ... Loading model from cache /tmp/jieba.cache Loading model cost 1.022 seconds. Prefix dict has been built succesfully. [&#39;他&#39;, &#39;来到&#39;, &#39;了&#39;, &#39;网易&#39;, &#39;杭研&#39;, &#39;大厦&#39;] 序列化12345def use_jieba_cut(a_sentence): return list(jieba.cut(a_sentence))train_data_df['cut_query'] = train_data_df['query'].apply(use_jieba_cut)test_data_df['cut_query'] = test_data_df['query'].apply(use_jieba_cut) 1train_data_df.head(10) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } query label cut_query 0 今天东莞天气如何 weather [今天, 东莞, 天气, 如何] 1 从观音桥到重庆市图书馆怎么走 map [从, 观音桥, 到, 重庆市, 图书馆, 怎么, 走] 2 鸭蛋怎么腌？ cookbook [鸭蛋, 怎么, 腌, ？] 3 怎么治疗牛皮癣 health [怎么, 治疗, 牛皮癣] 4 唠什么 chat [唠, 什么] 5 阳澄湖大闸蟹的做法。 cookbook [阳澄湖, 大闸蟹, 的, 做法, 。] 6 昆山大润发在哪里 map [昆山, 大润发, 在, 哪里] 7 红烧肉怎么做？嗯？ cookbook [红烧肉, 怎么, 做, ？, 嗯, ？] 8 南京到厦门的火车票 train [南京, 到, 厦门, 的, 火车票] 9 6的平方 calc [6, 的, 平方] 处理特征1tokenizer = Tokenizer() 1tokenizer.fit_on_texts(train_data_df['cut_query']) 123max_features = len(tokenizer.index_word)len(tokenizer.index_word) 2883 123x_train = tokenizer.texts_to_sequences(train_data_df['cut_query'])x_test = tokenizer.texts_to_sequences(test_data_df['cut_query']) 1max_cut_query_lenth = 26 123x_train = pad_sequences(x_train, max_cut_query_lenth)x_test = pad_sequences(x_test, max_cut_query_lenth) 1x_train.shape (2299, 26) 1x_test.shape (770, 26) 处理标签1label_tokenizer = Tokenizer() 1label_tokenizer.fit_on_texts(train_data_df['label']) 1label_numbers = len(label_tokenizer.word_counts) 1NUM_CLASSES = len(label_tokenizer.word_counts) 1label_tokenizer.word_counts OrderedDict([(&#39;weather&#39;, 66), (&#39;map&#39;, 68), (&#39;cookbook&#39;, 269), (&#39;health&#39;, 55), (&#39;chat&#39;, 455), (&#39;train&#39;, 70), (&#39;calc&#39;, 24), (&#39;translation&#39;, 61), (&#39;music&#39;, 66), (&#39;tvchannel&#39;, 71), (&#39;poetry&#39;, 102), (&#39;telephone&#39;, 63), (&#39;stock&#39;, 71), (&#39;radio&#39;, 24), (&#39;contacts&#39;, 30), (&#39;lottery&#39;, 24), (&#39;website&#39;, 54), (&#39;video&#39;, 182), (&#39;news&#39;, 58), (&#39;bus&#39;, 24), (&#39;app&#39;, 53), (&#39;flight&#39;, 62), (&#39;epg&#39;, 107), (&#39;message&#39;, 63), (&#39;match&#39;, 24), (&#39;schedule&#39;, 29), (&#39;novel&#39;, 24), (&#39;riddle&#39;, 34), (&#39;email&#39;, 24), (&#39;datetime&#39;, 18), (&#39;cinemas&#39;, 24)]) 1y_train = label_tokenizer.texts_to_sequences(train_data_df['label']) 1y_train[:10] [[10], [9], [2], [17], [1], [2], [9], [2], [8], [23]] 1y_train = [[y[0]-1] for y in y_train] 1y_train[:10] [[9], [8], [1], [16], [0], [1], [8], [1], [7], [22]] 12y_train = to_categorical(y_train, label_numbers)y_train.shape (2299, 31) 1234y_test = label_tokenizer.texts_to_sequences(test_data_df['label'])y_test = [y[0]-1 for y in y_test]y_test = to_categorical(y_test, label_numbers)y_test.shape (770, 31) 1y_test[0] array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32) 设计模型12345678910111213def create_SMP2018_lstm_model(max_features, max_cut_query_lenth, label_numbers): model = Sequential() model.add(Embedding(input_dim=max_features+1, output_dim=32, input_length=max_cut_query_lenth)) model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(label_numbers, activation='softmax')) # try using different optimizers and different optimizer configs model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[f1]) plot_model(model, to_file='SMP2018_lstm_model.png', show_shapes=True) return model 训练模型123456789if 'max_features' not in dir(): max_features = 2888 print('not find max_features variable, use default max_features values:\t&#123;&#125;'.format(max_features))if 'max_cut_query_lenth' not in dir(): max_cut_query_lenth = 26 print('not find max_cut_query_lenth, use default max_features values:\t&#123;&#125;'.format(max_cut_query_lenth))if 'label_numbers' not in dir(): label_numbers = 31 print('not find label_numbers, use default max_features values:\t&#123;&#125;'.format(label_numbers)) 1model = create_SMP2018_lstm_model(max_features, max_cut_query_lenth, label_numbers) 12batch_size = 20epochs = 30 1print(x_train.shape, y_train.shape) (2299, 26) (2299, 31) 1print(x_test.shape, y_test.shape) (770, 26) (770, 31) 1234print('Train...')model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs) Train... Epoch 1/30 2299/2299 [==============================] - 16s 7ms/step - loss: 3.0916 - f1: 0.0000e+00 Epoch 2/30 2299/2299 [==============================] - 14s 6ms/step - loss: 2.6594 - f1: 0.1409 Epoch 3/30 2299/2299 [==============================] - 13s 6ms/step - loss: 2.0817 - f1: 0.4055 Epoch 4/30 2299/2299 [==============================] - 14s 6ms/step - loss: 1.6032 - f1: 0.4689 Epoch 5/30 2299/2299 [==============================] - 14s 6ms/step - loss: 1.1318 - f1: 0.6176 Epoch 6/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.8090 - f1: 0.7399 Epoch 7/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.5704 - f1: 0.8298 Epoch 8/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.4051 - f1: 0.8879 Epoch 9/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.3002 - f1: 0.9280 Epoch 10/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.2317 - f1: 0.9467 Epoch 11/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.1755 - f1: 0.9678 Epoch 12/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.1391 - f1: 0.9758 Epoch 13/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.1131 - f1: 0.9800 Epoch 14/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0883 - f1: 0.9861 Epoch 15/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0725 - f1: 0.9894 Epoch 16/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0615 - f1: 0.9929 Epoch 17/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0507 - f1: 0.9945 Epoch 18/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0455 - f1: 0.9963 Epoch 19/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0398 - f1: 0.9960 Epoch 20/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0313 - f1: 0.9978 Epoch 21/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0266 - f1: 0.9984 Epoch 22/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0279 - f1: 0.9965 Epoch 23/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0250 - f1: 0.9976 Epoch 24/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0219 - f1: 0.9982 Epoch 25/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0195 - f1: 0.9982 Epoch 26/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0179 - f1: 0.9989 Epoch 27/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0177 - f1: 0.9974 Epoch 28/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0139 - f1: 0.9987 Epoch 29/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0139 - f1: 0.9989 Epoch 30/30 2299/2299 [==============================] - 14s 6ms/step - loss: 0.0129 - f1: 0.9987 &lt;keras.callbacks.History at 0x7f84e87c5f28&gt; 评估模型12345score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)print('Test score:', score[0])print('Test f1:', score[1]) 770/770 [==============================] - 1s 1ms/step Test score: 0.6803552009068526 Test f1: 0.8464262740952628 1y_hat_test = model.predict(x_test) 1print(y_hat_test.shape) (770, 31) 将 one-hot 张量转换成对应的整数1y_pred = np.argmax(y_hat_test, axis=1).tolist() 1y_true = np.argmax(y_test, axis=1).tolist() 查看多分类的 准确率、召回率、F1 值1print(classification_report(y_true, y_pred)) precision recall f1-score support 0 0.78 0.93 0.85 154 1 0.92 0.97 0.95 89 2 0.67 0.62 0.64 60 3 0.83 0.83 0.83 36 4 0.79 1.00 0.88 34 5 0.83 0.65 0.73 23 6 1.00 0.83 0.91 24 7 1.00 1.00 1.00 24 8 0.68 0.65 0.67 23 9 0.90 0.86 0.88 22 10 0.85 0.50 0.63 22 11 0.88 1.00 0.93 21 12 1.00 0.90 0.95 21 13 0.91 0.95 0.93 21 14 1.00 0.95 0.98 21 15 0.79 0.95 0.86 20 16 0.90 0.47 0.62 19 17 0.79 0.61 0.69 18 18 0.63 0.67 0.65 18 19 0.90 0.82 0.86 11 20 1.00 0.70 0.82 10 21 1.00 0.67 0.80 9 22 1.00 0.88 0.93 8 23 1.00 0.62 0.77 8 24 1.00 1.00 1.00 8 25 1.00 0.88 0.93 8 26 0.88 0.88 0.88 8 27 0.86 0.75 0.80 8 28 1.00 1.00 1.00 8 29 0.75 0.75 0.75 8 30 0.75 1.00 0.86 6 micro avg 0.84 0.84 0.84 770 macro avg 0.88 0.82 0.84 770 weighted avg 0.85 0.84 0.84 770]]></content>
      <categories>
        <category>实验</category>
      </categories>
      <tags>
        <tag>文本分类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bag of Tricks for Efficient Text Classification]]></title>
    <url>%2F2018%2F10%2F10%2Ffasttext%2F</url>
    <content type="text"><![CDATA[N-gram features词袋模型不考虑词序的问题，若将词序信息添加进去又会造成过高的计算代价。文章取而代之使用bag of n-gram来将词序信息引入：比如 我来到颐和园参观，相应的bigram特征为：我来 来到 到颐 颐和 和园 园参 参观相应的trigram特征为：我来到 来到颐 到颐和 颐和园 和园参 园参观并使用哈希算法高效的存储n-gram信息。 标题 说明 附加 Bag of Tricks for Efficient Text Classification 原始论文 201607 脸书论文翻译《Bag of Tricks for Efficient Text Classification》 论文翻译 201707 #Paper Reading# Bag of Tricks for Efficient Text Classification 论文理解 20160710 论文引介 Bag of Tricks for Efficient Text Classification 论文 201607 如何评价Word2Vec作者提出的fastText算法？深度学习是否在文本分类等简单任务上没有优势？ 论文浅析和评价 Bag of Tricks for Efficient Text Classification Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov(Submitted on 6 Jul 2016 (v1), last revised 9 Aug 2016 (this version, v3))This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute. 本文探讨了一种简单有效的文本分类基线。我们的实验表明，我们的快速文本分类器 fastText在准确性方面通常与深度学习分类器相当，并且在训练和评估方面更快几个数量级。我们可以 使用标准的多核CPU在不到10分钟的时间内对超过10亿个单词进行fastText训练 ，并在不到一分钟的时间内对312K类中的50万个句子进行分类。 Subjects: Computation and Language (cs.CL)Cite as: arXiv:1607.01759 [cs.CL] (or arXiv:1607.01759v3 [cs.CL] for this version) 学习该论文的写作方法论文目录 论文内容 imdb_fasttext 与不使用 N-gram features 相比红色线是不使用 N-gram features，绿色线是使用，显然使用 N-gram features 有效果]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>fasttext</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy_Pandas_Matplotlib]]></title>
    <url>%2F2018%2F10%2F08%2FNumpy_Pandas_Matplotlib%2F</url>
    <content type="text"><![CDATA[Numpynp.append(sent_list, np.array([new_word]), axis=0) 12345sent_list = np.zeros([0,7])for _ in range(3): new_word = np.random.randint(0,10,size=7) sent_list = np.append(sent_list, np.array([new_word]), axis=0)print(sent_list) array_split(X, X.shape[0], axis=0)将数组切分时，会保留原始的数组维度。 concatenate(x, axis=0)用于将多个数组进行连接，这与stack函数很容易混淆，他们之间的区别是concatenate会把当前要匹配的元素降一维，即去掉最外面那一层括号。 Reshape、转置、旋转、轴对换区别1import numpy as np 12X = np.arange(0,15,1).reshape((3,5))X array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 1X.reshape((5,3)) array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) 1X.T array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12# 轴对换X.swapaxes(0,1) array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12# 顺时针旋转 1 个 90 度np.rot90(X, 1, axes=(1,0)) array([[10, 5, 0], [11, 6, 1], [12, 7, 2], [13, 8, 3], [14, 9, 4]]) 1np.rot90(X, 1, axes=(0,1)) array([[ 4, 9, 14], [ 3, 8, 13], [ 2, 7, 12], [ 1, 6, 11], [ 0, 5, 10]]) 12C2 = np.rot90(X, 2, axes=(1,0))C2 array([[14, 13, 12, 11, 10], [ 9, 8, 7, 6, 5], [ 4, 3, 2, 1, 0]]) 1np.rot90(C2, 2, axes=(0,1)) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 12X2 = np.arange(0,24,1).reshape((2,3,4))X2 array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) 1X2.swapaxes(1,2) array([[[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]], [[12, 16, 20], [13, 17, 21], [14, 18, 22], [15, 19, 23]]]) 1X2.transpose(0,2,1) array([[[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]], [[12, 16, 20], [13, 17, 21], [14, 18, 22], [15, 19, 23]]]) Pandas手把手：如何方便地使用Python和Pandas来匿名信息]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarkDown问题]]></title>
    <url>%2F2018%2F10%2F08%2FMarkDown%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hexo 公式渲染z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j 标题 内容 时间 如何在 hexo 中支持 Mathjax？ 20171129 Hexo博客next主题数学公式渲染问题 20180515 hexo 图片 标题 内容 时间 hexo博客图片问题 20160513]]></content>
      <categories>
        <category>MarkDown</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场 CRF]]></title>
    <url>%2F2018%2F10%2F04%2F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 如何轻松愉快地理解条件随机场（CRF）？ milter 浅析 2017 如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？ Scofield 详解 2018 zh-NER-keras 基于keras的BiLstm与CRF实现命名实体标注this project is a sample for Chinese Named Entity Recognition(NER) by Keras 2.1.4 2018 【中文分词】条件随机场CRF 科普 CRF逐标签Softmax 条件随机场逐标签softmax是一种简单有效的方法，但有时候会出现不合理的结果。比如我们用sbme来做4标签分词时，逐标签softmax无法排除出现bbbb这样的序列的可能性，但这个序列是违反了我们的解码规则（b后面只能接m或e）。 因此，有人说逐标签softmax不需要动态规划，那是不对的，这种情况下，我们至少需要一个“非0即1”的转移矩阵，直接把不合理的转移概率设为0（如p(b|b) =0，然后通过动态规划保证得到合理的序列。 线性链CRF bilsm_crf_model.py123456789101112131415161718192021222324252627from keras.models import Sequentialfrom keras.layers import Embedding, Bidirectional, LSTMfrom keras_contrib.layers import CRFimport process_dataimport pickleEMBED_DIM = 200BiRNN_UNITS = 200def create_model(train=True): if train: (train_x, train_y), (test_x, test_y), (vocab, chunk_tags) = process_data.load_data() else: with open('model/config.pkl', 'rb') as inp: (vocab, chunk_tags) = pickle.load(inp) model = Sequential() model.add(Embedding(len(vocab), EMBED_DIM, mask_zero=True)) # Random embedding model.add(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True))) crf = CRF(len(chunk_tags), sparse_target=True) model.add(crf) model.summary() model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy]) if train: return model, (train_x, train_y), (test_x, test_y) else: return model, (vocab, chunk_tags) 使用条件随机场 CRF 完成命名实体识别 NER]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>CRF</tag>
        <tag>条件随机场</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Auto-Encoding Variational Bayes]]></title>
    <url>%2F2018%2F09%2F30%2FAuto-Encoding%20Variational%20Bayes%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 Auto-Encoding Variational Bayes 原始论文 2013 Introduction to variational autoencoders 论文精要解读 备用链接 变分自编码器（Variational Auto-Encoder，VAE） 论文详解 Auto-Encoding Variational BayesDiederik P Kingma, Max Welling(Submitted on 20 Dec 2013 (v1), last revised 1 May 2014 (this version, v10)) How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results. Subjects: Machine Learning (stat.ML); Machine Learning (cs.LG)Cite as: arXiv:1312.6114 [stat.ML] (or arXiv:1312.6114v10 [stat.ML] for this version)]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>变分自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Self-Normalizing Neural Networks selu]]></title>
    <url>%2F2018%2F09%2F30%2FSELU%2F</url>
    <content type="text"><![CDATA[论文核心代码，TensorFlow 实现12345def selu(x): with ops.name_scope('elu') as scope: alpha = 1.6732632423543772848170429916717 scale = 1.0507009873554804934193349852946 return scale*tf.where(x&gt;0.0,x,alpha*tf.nn.elu(x)) 标题 说明 附加 《Self-Normalizing Neural Networks》 原始论文 2017 引爆机器学习圈：「自归一化神经网络」提出新型激活函数SELU 翻译 2017 【文献阅读】Self-Normalizing Neural Networks CSDN TensorSense 2017 如何评价 Self-Normalizing Neural Networks 这篇论文? 知乎评价 Compare SELUs (scaled exponential linear units) with other activations on MNIST, CIFAR10, etc. SELU 在不同数据集实验 2017 Self-Normalizing Neural NetworksGünter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter(Submitted on 8 Jun 2017 (v1), last revised 7 Sep 2017 (this version, v5)) Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are “scaled exponential linear units” (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance — even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs. 深度学习不仅通过卷积神经网络（CNN）变革了计算机视觉，同时还通过循环神经网络（RNN）变革了自然语言处理。然而，带有标准前馈神经网络（FNN）的深度学习很少有成功的案例。通常表现良好的 FNN 都只是浅层模型，因此不能挖掘多层的抽象表征。所以我们希望引入自归一化神经网络（self-normalizing neural networks/SNNs）以帮助挖掘高层次的抽象表征。虽然批归一化要求精确的归一化，但 SNN 的神经元激励值可以自动地收敛到零均值和单位方差。SNN 的激活函数即称之为「可缩放指数型线性单元（scaled exponential linear units/SELUs）」，该单元引入了自归一化的属性。使用 Banach 的不动点定理（fixed-point theorem），我们证明了激励值逼近于零均值和单位方差并且通过许多层的前向传播还是将收敛到零均值和单位方差，即使是存在噪声和扰动的情况下也是这样。这种 SNN 收敛属性就允许 (1) 训练许多层的深度神经网络，同时 (2) 采用强正则化、(3) 令学习更具鲁棒性。此外，对于不逼近单位方差的激励值，我们证明了其方差存在上确界和下确界，因此梯度消失和梯度爆炸是不可能出现的。同时我们采取了 (a) 来自 UCI 机器学习库的 121 个任务，并比较了其在 (b) 新药发现基准和 (c) 天文学任务上采用标准 FNN 和其他机器学习方法（如随机森林、支持向量机等）的性能。SNN 在 121 个 UCI 任务上显著地优于所有竞争的 FNN 方法，并在 Tox21 数据集上超过了所有的竞争方法，同时 SNN 还在天文数据集上达到了新纪录。该实现的 SNN 架构通常比较深，实现可以在以下链接获得：http://github.com/bioinf-jku/SNNs。 Comments: 9 pages (+ 93 pages appendix)Subjects: Machine Learning (cs.LG); Machine Learning (stat.ML)Journal reference: Advances in Neural Information Processing Systems 30 (NIPS 2017)Cite as: arXiv:1706.02515 [cs.LG] (or arXiv:1706.02515v5 [cs.LG] for this version) 实验结论我们提出了自归一化神经网络，并且已经证明了当神经元激励在网络中传播时是在朝零均值（zero mean）和单位方差（unit variance）的趋势发展的。而且，对于没有接近单位方差的激励，我们也证明了方差映射的上线和下限。于是 SNN 不会产梯度消失和梯度爆炸的问题。因此，SNN 非常适用于多层的结构，这使我们可以引入一个全新的正则化（regularization）机制，从而更稳健地进行学习。在 121UCI 基准数据集中，SNN 已经超过了其他一些包括或不包括归一化方法的 FNN，比如批归一化（batch）、层级归一化（layer）、权值归一化（weight normalization）或其它特殊结构（Highway network 或 Residual network）。SNN 也在药物研发和天文学任务中产生了完美的结果。和其他的 FNN 网络相比，高性能的 SNN 结构通常深度更深。 基于 SELUs 的模型的论文Models and architectures built on Self-Normalizing Networks 实验验证 SELU 的真实效果MNIST-MLP-SELU VS MNIST-MLP-RELU模型结构MNIST_Conv_SELU MNIST_Conv_RELU 实验结果蓝色的线代表 SELU ，橙色的线代表 RELU。 结果分析在 MNIST 数据集，多层前馈神经网络模型条件下，SELU 在训练集的效果差于 RELU，但是 SELU 在验证集效果与 RELU 几乎一致，且 SELU 训练时间更长。 MNIST-Conv-SELU VS MNIST-Conv-RELU模型结构MNIST_Conv_SELU MNIST_Conv_RELU 实验结果蓝色的线代表 SELU ，橙色的线代表 RELU 结果分析在 MNIST 数据集，多层卷积神经网络模型条件下，SELU 在训练集的效果优于 RELU，但是 SELU 在验证集效果差于 RELU ，且 SELU 训练时间更长。 CIFAR10_Conv_SELU VS CIFAR10_Conv_RELU模型结构CIFAR10_Conv_SELU CIFAR10_Conv_RELU 实验结果灰色线代表 SELU，绿色线代表 RELU。 结果分析在 CIFAR10 数据集，多层卷积神经网络模型条件下，SELU 在训练集的效果优于 RELU，但是 SELU 在验证集效果与 RELU 一致，且 RELU 更不易发生过拟合，SELU 训练时间更长。 reuters_mlp_relu_vs_selu模型结构reuters_mlp_selu reuters_mlp_relu 实验结果灰色的线代表 SELU，绿色的线代表 RELU。 结果分析在 reuters 数据集，多层前馈神经网络模型条件下，SELU 在训练集的效果差于 RELU，但是 SELU 在验证集效果稍稍优于 RELU ，但 SELU 训练时间更长。 本文结论SELU 只有在特殊限定的数据集和网络模型结构的条件下会优于 RELU，一般情况下 RELU 训练速度更快，且更不容易发生过拟合。所以根据现有证据，SELU 比 RELU 没有显著优势，一般情况下选择 RELU 更优。]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>Self-Normalizing</tag>
        <tag>缩放指数型线性单元</tag>
        <tag>selu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Conditional Image Synthesis with Auxiliary Classifier GANs]]></title>
    <url>%2F2018%2F09%2F27%2FACGAN%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 《Conditional Image Synthesis with Auxiliary Classifier GANs》 原始论文 2016 《Conditional Image Synthesis with Auxiliary Classifier GANs》阅读笔记 summer 2017 Conditional Image Synthesis With Auxiliary Classifier GANs 马小李 论文解读 2018 Auxiliary Classifier Generative Adversarial Networks in Keras 论文实现 2016 Conditional Image Synthesis with Auxiliary Classifier GANsConditional Image Synthesis With Auxiliary Classifier GANsAugustus Odena, Christopher Olah, Jonathon Shlens(Submitted on 30 Oct 2016 (v1), last revised 20 Jul 2017 (this version, v4)) Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. 合成高分辨率照片级真实感图像一直是机器学习中的长期挑战。 在本文中，我们介绍了用于图像合成的改进生成对抗网络（GAN）训练的新方法。 我们构建了采用标签调节的GAN变体，其产生128x128分辨率的图像样本，表现出全局一致性。 我们扩展了以前的图像质量评估工作，为评估类条件图像合成模型中样本的可辨性和多样性提供了两个新的分析。 这些分析表明，高分辨率样品提供了低分辨率样品中不存在的类信息。 在1000个ImageNet类中，128x128样本的差异是人工调整大小的32x32样本的两倍多。 此外，84.7％的类别的样本具有与真实ImageNet数据相当的多样性。 Subjects: Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV)Cite as: arXiv:1610.09585 [stat.ML] (or arXiv:1610.09585v4 [stat.ML] for this version) 以下内容参考谷歌翻译 摘要在本文中，我们介绍了用于图像合成的改进生成对抗网络（GAN）训练的新方法。我们构建了采用标签调节的GAN变体，其产生了展示全局一致性的 128 × 128 分辨率图像样本。我们扩展了以前的图像质量评估工作，为评估类条件图像合成模型中样本的可辨性和多样性提供了两个新的分析。这些分析表明，高分辨率样品提供了低分辨率样品中不存在的类信息。在1000个ImageNet类中，128 × 128 个样本比人工调整大小的 32 × 32 可辨别两倍多样本。此外，84.7％的类别的样本具有与真实ImageNet数据相当的多样性。 介绍表征自然图像的结构是一项丰富的研究工作。 自然图像遵循内在的不变性，并展示历史上难以量化的多尺度统计结构（Simoncelli＆Olshausen， 2001 ）。 机器学习的最新进展提供了显着改善图像模型质量的机会。 改进的图像模型推进了图像去噪（Ballé等， 2015 ） ，压缩（Toderici等， 2016 ） ，绘画（van den Oord等， 2016a ）和超级图像去噪方面的最新技术。 - 分辨率（Ledig等， 2016 ） 。 更好的自然图像模型也提高了半监督学习任务的性能（Kingma等， 2014 ; Springenberg， 2015 ; Odena， 2016 ; Salimans等， 2016 ）和强化学习问题（Blundell等， 2016 ） 。 理解自然图像统计的一种方法是构建从头合成图像的系统。 有几种有前景的方法可用于构建图像合成模型。 变分自动编码器（VAE）最大化训练数据对数似然的变化下界（Kingma＆Welling， 2013 ; Rezende等， 2014 ） 。 VAE很容易训练，但引入了关于近似后验分布的潜在限制性假设（但参见（Rezende＆Mohamed， 2015 ; Kingma等， 2016 ） ）。 自回归模型省去了潜在变量并直接模拟了像素上的条件分布（van den Oord et al。， 2016a ， b ） 。 这些模型产生令人信服的样品，但样品成本高，并且不提供潜在的表示。 可逆密度估计器使用一系列限制为可逆的参数化函数直接转换潜在变量（Dinh等， 2016 ） 。 该技术允许精确的对数似然计算和精确推理，但是可逆性约束是限制性的。 生成性对抗网络（GAN）提供了一种独特且有前景的方法，侧重于用于训练图像合成模型的游戏理论公式（Goodfellow等， 2014 ） 。 最近的研究表明，GAN可以在具有低可变性和低分辨率的数据集上生成令人信服的图像样本（Denton等， 2015 ; Radford等， 2015 ） 。 然而，GAN努力生成全局一致的高分辨率样本 - 特别是来自具有高可变性的数据集。 此外，对GAN的理论理解是一个持续的研究课题（Uehara等， 2016 ; Mohamed和Lakshminarayanan， 2016 ） 。 图1：来自在ImageNet数据集上训练的AC-GAN的5个等级的 128×128 分辨率样本。请注意，已选择显示的类以突出显示模型的成功，并且不具有代表性。 来自所有ImageNet类的样本将在本文后面链接。 在这项工作中，我们证明了向GAN潜在空间添加更多结构以及专门的成本函数可以获得更高质量的样本。 我们展示了来自ImageNet数据集各类的 128×128 像素样本（Russakovsky等， 2015 ） ，增强了全局一致性（图1 ）。 重要的是，我们定量地证明我们的高分辨率样品不仅仅是对低分辨率样品的天真抑制。 特别是，将 128×128 样本下采样到 32×32 会导致视觉可辨性降低50％。 我们还引入了一个新的度量标准，用于评估图像样本的可变性，并使用该度量标准来证明我们的合成图像显示出与大部分（84.7％）ImageNet类的训练数据相当的多样性。 更详细地说，这项工作是第一个： 以128x128空间分辨率（或任何空间分辨率 - 参见第3节）演示所有1000个ImageNet类的图像合成模型。 测量图像合成模型实际使用其输出分辨率的程度（第4.1节）。 使用快速，易于计算的度量来衡量GAN中的感知变异性和“崩溃”行为（第4.2节）。 需要注意的是，大量的类使GAN的ImageNet合成变得困难，并提供了明确的解决方案（第4.6节）。 通过实验证明，感性表现良好的GAN不是记忆少数例子的那些（第4.3节）。 在不使用任何技术（Salimans等， 2016 ） （第4.4节）的情况下，在CIFAR-10上进行训练时，在初始得分指标上实现最先进的技术水平 。 背景生成对抗网络（GAN）由两个彼此对立训练的神经网络组成。生成器 G 将随机噪声矢量 z 作为输入，并输出图像 $X_{fake}=G(z)$ 。鉴别器 D 接收来自发生器的训练图像或合成图像作为输入，并输出概率分布 $P(S|X)=D(X)$ 可能的图像源。训练鉴别器以最大化它分配给正确来源的对数似然性： L=E[logP(S=real|X_{real})]+E[logP(S=fake|X_{fake})]训练发生器以使等式1中的第二项最小化。 可以使用辅助信息来扩充基本GAN框架。一种策略是为生成器和鉴别器提供类标签，以生成类条件样本（Mirza＆Osindero，2014）。类条件合成可以显着提高生成样本的质量（van den Oord等，2016b）。更丰富的辅助信息（如图像标题和边界框定位）可以进一步提高样本质量（Reed等，2016a，b）。 不是将类别信息馈送到鉴别器，而是可以使鉴别器重建类别信息。这是通过修改鉴别器以包含辅助解码器网络来完成的1 输出训练数据的类别标签（Odena，2016 ; Salimans等，2016） 或生成样本的潜在变量的子集（Chen等，2016）。已知迫使模型执行附加任务以改善原始任务的性能（例如（Sutskever等人，2014 ; Szegedy等人，2014 ; Ramsundar等人，2016））。此外，辅助解码器可以利用预先训练的鉴别器（例如图像分类器）来进一步改善合成图像（Nguyen等，2016）。在这些考虑因素的推动下，我们引入了一种模型，该模型结合了利用类别信息的两种策略。也就是说，下面提出的模型是类条件的，但是具有辅助解码器，其负责重建类标签。 AC-GANs我们提出了GAN架构的变体，我们将其称为辅助分类器GAN（或AC-GAN）。在AC-GAN中，除了噪声z 之外，每个生成的样本都具有相应的类标签 $c \sim p_{c}$。鉴别器给出了源上的概率分布和类标签上的概率分布 $P(S|X), P(C|X)=D(X)$。目标函数有两个部分：正确源的对数似然，$L_s$ 和正确类的对数似然，$L_c$。 L_s = E[logP(S=real|X_{real})]+E[logP(S=fake|X_{fake})]L_c = E[logP(C=c|X_{real})]+E[logP(C=c|X_{fake})]训练 D 使 $L_s + L_c$ 最大化，同时训练 G 使 $L_c - L_s$ 最大化。AC-GAN学习 z 的表示，其独立于类标签（例如（Kingma等， 2014 ） ）。 在结构上，该模型与现有模型没有太大差别。 然而，对标准GAN配方的这种修改产生了优异的结果并且似乎稳定了训练。 此外，我们认为AC-GAN模型只是这项工作的技术贡献的一部分，以及我们提出的测量模型利用其给定输出分辨率的程度的方法，测量样品的感知变异性的方法。该模型，以及图像生成模型的全面实验分析，可从所有1000个ImageNet类中创建 128×128 个样本。 早期的实验证明，在固定模型的同时增加训练课程的数量会降低模型输出的质量。 AC-GAN 模型的结构允许按类别将大数据集分成子集，并为每个子集训练生成器和鉴别器。 所有ImageNet实验都是使用100个AC-GAN的集合进行的，每个AC-GAN都采用10级分割进行训练。 结果我们在ImageNet数据集上训练了几个AC-GAN模型（Russakovsky等， 2015 ） 。 从广义上讲，发生器 G 的结构是一系列“反卷积”层，它们将噪声 z 和类 c 转换为图像（Odena等， 2016 ） 。 我们训练两种模型架构的变体，用于生成 128×128 和 64×64 空间分辨率的图像。 鉴别器 D 是具有Leaky ReLU非线性的深度卷积神经网络（Maas等人， 2013 ） 。 如前所述，我们发现减少所有1000类ImageNet引入的可变性可显着提高培训质量。 我们培训了100个AC-GAN型号 - 每个型号只有10个类别 - 用于50000个100个小批量的批量生产。 由于概率标准的多样性（Theis等， 2015 ）以及缺乏感知上有意义的图像相似性度量，评估图像合成模型的质量具有挑战性。 尽管如此，在后面的部分中，我们尝试通过构建图像样本可辨性和多样性的若干临时措施来测量AC-GAN的质量。 我们希望这项工作可以提供可用于辅助图像合成模型的培训和后续开发的量化测量。 生成高分辨率图像可提高可辨别性图2：生成高分辨率图像可提高可辨性。 上图：来自斑马类的训练数据和合成图像调整为较低的空间分辨率（如上所示），然后人工调整大小为原始分辨率（红线和黑线为 128×128 ;蓝线为 64×64 ）。 初始精度显示在相应图像下方。 左下：对于 64×64 和 128×128 模型的训练数据和图像样本，不同空间分辨率的精度总结。 误差棒测量10个图像子集的标准偏差。 虚线突出了模型输出空间分辨率的精度。 训练数据（剪裁）在分辨率分别为32,64,128和256时分别达到24％，54％，81％和81％的精度。 右下：比较 128×128 和 32×32 空间分辨率（分别为 x 和 y 轴）的精度分数。 每个点代表一个ImageNet类。 84.4％的班级低于平等线。 绿点对应斑马类。 我们还将 128×128 和 64×64 图像人工调整为 256×256 ，作为一种健全性检查，以证明简单地增加像素数量不会增加可辨别性。 构建类条件图像合成模型需要测量合成图像看起来属于预期类的程度。 特别是，我们想知道高分辨率样本不仅仅是对低分辨率样本进行天真的调整。 考虑一个简单的实验：假装存在一个合成 32x32 图像的模型。 通过执行双线性插值，可以平凡地增加合成图像的分辨率。 这将产生更高分辨率的图像，但是这些图像将仅是不可辨别的低分辨率图像的模糊版本。 因此，图像合成模型的目标不仅仅是产生高分辨率图像，而是产生比低分辨率图像更易辨别的高分辨率图像。 为了测量可辨性，我们将合成图像提供给预先训练的Inception网络（Szegedy等， 2015 ）并报告Inception网络为其分配正确标签的样本部分。 我们计算了一系列真实和合成图像的精度测量值，这些图像的空间分辨率通过双线性插值人为地降低（图2 ，上图）。 请注意，随着空间分辨率的降低，精度会降低 - 表明生成的图像包含较少的类信息（图2 ，顶部面板下方的分数）。 我们在所有1000个ImageNet类中总结了这一发现，包括ImageNet训练数据（黑色），图2中的 128×128 分辨率AC-GAN（红色）和 64x64分辨率AC-GAN（蓝色）（左下图）。 黑色曲线（剪裁）提供了真实图像可辨性的上限。 该分析的目的是表明合成更高分辨率的图像会导致可识别性增加。 128×128 模型的精度达到10.1％ ±2.0％，而7.0％±2.0％，样品尺寸调整为 64x64和5.0％ ±2.0％，样品尺寸调整为 32x32 。 换句话说，将AC-GAN的输出缩小到 32x32 和 64x64 分别将视觉辨别率降低50％和38％。 此外，84.4％的ImageNet类在 128×128 处具有比在 32x32 处更高的精度（图2 ，左下）。 我们对训练为 64x64空间分辨率的AC-GAN进行了相同的分析。 与 128×128 AC-GAN模型相比，该模型的可辨识性较差。 来自 64x64模型平台的精度在 64x64空间分辨率下与先前的结果一致。 最后， 64x64分辨率模型在64空间分辨率下比 128×128 模型具有更小的可辨识性。 据我们所知，这项工作是第一次尝试测量图像合成模型“利用其给定的输出分辨率”的程度，实际上是第一个完全考虑该问题的工作。 我们认为这是一个重要的贡献，与提出一个合成来自所有1000个ImageNet类的图像的模型相同。 我们注意到，所提出的方法可以应用于任何可以构建“样本质量”度量的图像合成模型。 事实上，这种方法（广泛定义）可以应用于任何类型的合成模型，只要有一个容易计算的样本质量概念和一些“降低分辨率”的方法。 特别是，我们希望可以对音频合成进行类似的处理。 测量生成图像的多样性如果图像合成模型仅输出一个图像，则它不是很有趣。实际上，众所周知的GAN故障模式是生成器将崩溃并输出单个原型，最大限度地愚弄鉴别器（Goodfellow等， 2014 ; Salimans等， 2016 ） 。 如果每个类只输出一个图像，则图像的类条件模型不是很有趣。 初始精度无法测量模型是否已折叠。 简单地记住每个ImageNet类中的一个示例的模型可以很好地通过该度量。 因此，我们寻求补充度量以明确地评估由AC-GAN生成的样本的类内感知多样性。 存在几种通过尝试预测人类感知相似性判断来定量评估图像相似性的方法。 其中最成功的是多尺度结构相似性（MS-SSIM） （Wang等， 2004b ; Ma等， 2016 ）。 MS-SSIM是一种充分表征的感知相似性度量的多尺度变体，其试图折扣对人类感知不重要的图像的方面（Wang等， 2004a ） 。 MS-SSIM值介于0.0和1.0之间; 较高的MS-SSIM值对应于感知上更相似的图像。 作为图像多样性的代理，我们测量给定类别中100个随机选择的图像对之间的MS-SSIM分数。 来自具有较高多样性的类别的样本导致较低的平均MS-SSIM分数（图3 ，左栏）; 来自具有较低多样性的类别的样本具有较高的平均MS-SSIM分数（图3 ，右栏）。 来自ImageNet训练数据的训练图像包含各类的各种平均MS-SSIM分数，表明ImageNet类中图像多样性的可变性（图4 ，x轴）。 注意，对于训练数据，最高平均MS-SSIM得分（表示最小可变性）是0.25。 我们计算AC-GAN模型生成的所有1000个ImageNet类的平均MS-SSIM得分。 我们在训练期间跟踪此值以确定生成器是否已折叠（图5 ，红色曲线）。 我们还使用该度量来在训练完成后将训练图像的多样性与来自GAN模型的样本进行比较。 图4绘制了按类别划分的图像样本和训练数据的平均MS-SSIM值。 蓝线是平等的线。 在1000个类中，我们发现847的平均样本MS-SSIM分数低于训练数据的最大MS-SSIM。 换句话说，84.7％的类具有超过ImageNet训练数据中的最小变量类的样本可变性。 有两个与MS-SSIM指标相关的点以及我们对它的使用值得特别注意。 第一点是我们“滥用”指标：它最初用于使用参考“原始图像”来测量图像压缩算法的质量。 我们改为在两个可能不相关的图像上使用它。 我们认为这是可以接受的，原因如下：第一：视觉检查似乎表明该指标是有意义的 - 具有较高MS-SSIM的配对似乎与具有较低MS-SSIM的配对更相似。 第二：我们将比较限制为使用相同类标签合成的图像。 这限制了MS-SSIM的使用，使其更类似于通常使用的情况（哪个图像作为参考并不重要）。 第三：对于我们的用例，度量标准不是“饱和的”。 如果大多数分数大约为0，那么我们会更关注MS-SSIM的适用性。 最后：培训数据通过该指标实现更多可变性（如预期的那样）这一事实证明该指标正在按预期工作。 第二点是MS-SSIM度量不是作为像素空间中发生器分布的熵的代理，而是作为输出的感知多样性的度量。 生成器输出分布的熵难以计算，成对的MS-SSIM分数不是一个好的代理。 即使它很容易计算，我们仍然认为对感知多样性进行单独测量仍然是有用的。 要了解原因，请考虑生成器熵对对比度的微小变化以及输出的语义内容的变化敏感。 在许多应用中，我们并不关心这种对熵的贡献，并且考虑试图忽略我们认为“在感知上毫无意义”的图像的变化的措施是有用的，因此使用MS-SSIM。 生成的图像既多样又可辨别我们已经提出了量化指标，证明AC-GAN样本可能是多样的和可辨别的，但我们尚未研究这些指标如何相互作用。 图6显示了所有类别的初始准确度和MS-SSIM分数的联合分布。 初始精度和MS-SSIM是反相关的。 相比之下，Inception-v3模型在所有1000个类别中平均达到78.8％的准确度（Szegedy等， 2015 ） 。 AC-GAN样品的一小部分达到这种精度水平。 这表明未来图像合成模型的机会。 这些结果表明，降低模式的GAN最有可能产生低质量的图像。 这与关于GAN的流行假设形成鲜明对比，GAN是以可变性为代价获得高样本质量的。 我们希望这些发现有助于进一步研究GAN与其他图像合成模型之间不同样本质量的原因。 图6：所有1000个ImageNet类的初始精度与MS-SSIM（$r^2=-0.16$）。 每个数据点代表一个类的样本的平均MS-SSIM值。 如图4所示，红线标记所有ImageNet类的最大MS-SSIM值（用于训练数据）。 来自AC-GAN模型的样本不会以可辨别性为代价实现可变性。 与先前结果的比较在对数似然性方面报告了在ImageNet上训练的图像合成模型的先前定量结果（van den Oord等， 2016a ， b ） 。 对数似然是一种粗略且可能不准确的样本质量测量（Theis等， 2015 ）。 相反，我们使用较低的空间分辨率（32x32）与CIFAR-10上先前的最新结果进行比较。 按照（Salimans等， 2016 ）中的程序 ，我们计算初始分数 3 对于分辨率为（32x32）的AC-GAN的50000个样本，随机分成10组。 我们还计算了25000个额外样本的初始分数，随机分成5组。 我们根据第一个分数选择最佳模型并报告第二个分数。 在27个超参数配置中执行网格搜索，与现有技术8.09 ± 0.07相比，我们能够获得8.25 ± 0.07的分数（Salimans等， 2016 ） 。 此外，我们在不采用该工作中引入的任何新技术（即虚拟批量标准化，小批量区分和标签平滑）的情况下实现了这一点。 这提供了额外的证据，即AC-GAN即使没有类别分裂的好处也是有效的。 参见图7 ，对AC-GAN样品和模型样品进行定性比较（Salimans等， 2016 ） 。 图7：从ImageNet数据集生成的样本。 （左）从模型中生成的样本（Salimans等， 2016 ） 。 （右）从AC-GAN生成的随机选择的样本。 AC-GAN样品在早期模型的样品中缺乏全局一致性。 搜索过度拟合的证据必须研究的一种可能性是AC-GAN在训练数据上过度拟合。 作为网络不记忆训练数据的第一次检查，我们在像素空间中通过L1距离测量的训练数据中识别图像样本的最近邻居（图8 ）。 来自训练数据的最近邻居不与相应的样本相似。 这提供了AC-GAN不仅仅是记忆训练数据的证据。 图8：最近邻分析。 （顶部）来自单个ImageNet类的样本。 （下）每个样本的训练数据中对应的最近邻居（L1距离）。 用于理解模型中过度拟合程度的更复杂方法是​​通过插值来探索该模型的潜在空间。 在过拟合模型中，可以观察到插值图像中的离散跃迁和潜在空间中与有意义的图像不对应的区域（Bengio等人， 2012 ; Radford等人， 2015 ; Dinh等人， 2016 ） 。 图9 （顶部）突出显示了几个图像样本之间潜在空间中的插值。 值得注意的是，发生器了解到某些尺寸组合对应于语义上有意义的特征（例如拱的大小，鸟喙的长度），并且在潜在空间中没有离散的过渡或“洞”。 探索AC-GAN潜在空间的第二种方法是利用模型的结构。 AC-GAN将其表示分解为类信息和类独立的潜在表示z。 采用z固定但是改变类标签对AC-GAN进行采样对应于在多个类中生成具有相同“风格”的样本（Kingma等， 2014 ） 。 图9 （底部）显示了8个鸟类的样本。 同一行的元素具有相同的z。 尽管每列的类都在变化，但全局结构的元素（例如位置，布局，背景）仍然保留，表明AC-GAN可以表示某些类型的“组合性”。 图9：（顶部）所选ImageNet类的潜在空间插值。 最左侧和右侧列显示三对图像样本 - 每对来自不同的类。 中间列突出显示这三对图像之间潜在空间中的线性插值。 （底部）与类无关的信息包含关于合成图像的全局结构。 每列是一个独特的鸟类，而每一行对应一个固定的潜在代码z。 测量类别分裂对图像样本质量的影响类条件图像合成提供了基于图像标签划分数据集的机会。 在我们的最终模型中，我们将1000个ImageNet类划分为100个AC-GAN模型。 在本节中，我们将描述实验，这些实验强调了减少用于训练AC-GAN的类别的多样性的好处。 我们采用了标签的排序，并将其分为10个连续的组。这个排序可以在下面的部分中看到，我们在这里显示来自所有1000个类的样本。 分裂绩效讨论的两个方面：每个分裂的类数和分裂内多样性。我们发现在更多类别上训练固定模型会损害模型生成引人注目的样本的能力（图10 ）。通过为模型提供更多参数，可以提高较大分割的性能。但是，使用小分割不足以实现良好的性能。我们无法训练GAN （Goodfellow et al。，2014），即使对于1的分割大小也能可靠地收敛。这就提出了一个问题，即在不同的类别上训练模型是否比在类似的集合上训练更容易课程：我们无法找到确凿的证据表明分组中的课程选择会显着影响样本质量。 图10：针对训练期间使用的ImageNet类的数量绘制的10个ImageNet类的平均成对MS-SSIM值。我们使用10到100之间的值来修复除训练课程数量之外的所有内容。我们仅报告前10个课程的MS-SSIM值以保持分数可比。随着班级计数的增加，MS-SSIM迅速超过0.25（红线）。使用每个模型的相同数量的训练步骤，使用每个类计数9个随机重启来计算这些分数。由于我们观察到生成器不能从崩溃阶段恢复，因此在这种情况下使用固定数量的训练步骤似乎是合理的。 我们没有关于是什么原因导致这种对班级计数的敏感性的假设，这种假设在实验上得到了很好的支持。我们只能注意到，由于类计数增加时发生的故障情况是“生成器崩溃”，似乎有理由解决“生成器崩溃”的一般方法也可以解决这种敏感问题。 来自所有1000个ImageNet类的样本我们还从这里托管的1000个ImageNet类中的每一个中生成10个样本。据我们所知，没有其他图像合成工作包括类似的分析。 讨论这项工作引入了AC-GAN架构，并证明了AC-GAN可以生成全局一致的ImageNet样本。我们为图像可辨性提供了一种新的定量度量，作为空间分辨率的函数。使用此度量标准，我们证明了我们的样本比生成较低分辨率图像并执行简单调整大小操作的模型更具可辨识性。我们还分析了样本在训练数据方面的多样性，并提供了一些证据，证明大多数类别的图像样本在多样性方面与ImageNet数据具有可比性。 在这项工作的基础上存在几个方向。需要做很多工作来改善 128x128 分辨率模型的视觉可辨性。虽然一些合成图像类别具有较高的初始精度，但模型的平均初始精度（10.1 ％± 2.0 ％）仍远低于实际训练数据的81％。解决这个问题的一个直接机会是用预训练模型增强鉴别器以执行额外的监督任务（例如图像分割，（Ronneberger等，2015））。 提高GAN培训的可靠性是一个持续的研究课题。只有84.7％的ImageNet类表现出与真实训练数据相当的多样性。通过在100个AC-GAN模型中划分1000个ImageNet类，极大地帮助了训练稳定性。构建可以从所有1000个类生成样本的单个模型将是向前迈出的重要一步。 图像合成模型为执行半监督学习提供了独特的机会：这些模型构建了丰富的先前自然图像统计数据，可以通过分类器来利用这些统计数据来改进对少数标签存在的数据集的预测。当标签对于给定的训练图像不可用时，AC-GAN模型可以通过忽略由类标签引起的损失的分量来执行半监督学习。有趣的是，先前的工作表明，实现良好的样本质量可能与半监督学习的成功无关（Salimans等，2016）。]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>ACGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成式对抗网络 Generative Adversarial Network (GAN)]]></title>
    <url>%2F2018%2F09%2F25%2F%E7%94%9F%E6%88%90%E5%BC%8F%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[生成式对抗网络定义生成式对抗网络（GAN, Generative Adversarial Networks ）是一种深度学习模型，是近年来复杂分布上无监督学习最具前景的方法之一。模型通过框架中（至少）两个模块：生成模型（Generative Model）和判别模型（Discriminative Model）的互相博弈学习产生相当好的输出。原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。 生成式对抗网络资源列表2014 GAN《Generative Adversarial Networks》Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. Subjects: Machine Learning (stat.ML); Machine Learning (cs.LG)Cite as: arXiv:1406.2661 [stat.ML] (or arXiv:1406.2661v1 [stat.ML] for this version) GAN 极大极小博弈精髓 2016《NIPS 2016 Tutorial: Generative Adversarial Networks》Ian Goodfellow(Submitted on 31 Dec 2016 (v1), last revised 3 Apr 2017 (this version, v4)) This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises. Comments: v2-v4 are all typo fixes. No substantive changes relative to v1Subjects: Machine Learning (cs.LG)Cite as: arXiv:1701.00160 [cs.LG] (or arXiv:1701.00160v4 [cs.LG] for this version) 2016 ACGAN 《Conditional Image Synthesis With Auxiliary Classifier GANs》Conditional Image Synthesis With Auxiliary Classifier GANsAugustus Odena, Christopher Olah, Jonathon Shlens(Submitted on 30 Oct 2016 (v1), last revised 20 Jul 2017 (this version, v4)) Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. 合成高分辨率照片级真实感图像一直是机器学习中的长期挑战。 在本文中，我们介绍了用于图像合成的改进生成对抗网络（GAN）训练的新方法。 我们构建了采用标签调节的GAN变体，其产生128x128分辨率的图像样本，表现出全局一致性。 我们扩展了以前的图像质量评估工作，为评估类条件图像合成模型中样本的可辨性和多样性提供了两个新的分析。 这些分析表明，高分辨率样品提供了低分辨率样品中不存在的类信息。 在1000个ImageNet类中，128x128样本的差异是人工调整大小的32x32样本的两倍多。 此外，84.7％的类别的样本具有与真实ImageNet数据相当的多样性。 Subjects: Machine Learning (stat.ML); Computer Vision and Pattern Recognition (cs.CV)Cite as: arXiv:1610.09585 [stat.ML] (or arXiv:1610.09585v4 [stat.ML] for this version) 2019 BigGAN 《LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS》 尽管近期由于生成图像建模的研究进展，从复杂数据集例如 ImageNet 中生成高分辨率、多样性的样本仍然是很大的挑战。为此，在这篇提交到 ICLR 2019 的论文中，研究者尝试在最大规模的数据集中训练生成对抗网络，并研究在这种规模的训练下的不稳定性。研究者发现应用垂直正则化（orthogonal regularization）到生成器可以使其服从简单的「截断技巧」（truncation trick），从而允许通过截断隐空间来精调样本保真度和多样性的权衡。这种修改方法可以让模型在类条件的图像合成中达到当前最佳性能。当在 128x128 分辨率的 ImageNet 上训练时，本文提出的模型—BigGAN—可以达到 166.3 的 Inception 分数（IS），以及 9.6 的 Frechet Inception 距离（FID），而之前的最佳 IS 和 FID 仅为 52.52 和 18.65。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>生成式对抗网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sequence to Sequence Learning with Neural Networks]]></title>
    <url>%2F2018%2F09%2F25%2FSequence%20to%20Sequence%20Learning%20with%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[这是由谷歌的三位大神用seq2seq模型实现的基于神经网络的机器翻译（英法互译），基本的方法还是encoder-decoder,作为seq2seq的经典必读论文，本文与其他工作相比主要有创新点有： 利用了多层的LSTM（本文中是4层）。实验表明使用LSTM对长句子的翻译来说效果很好，主要是因为LSTM本身具有长期依赖的优势。 与attention不一样，它又回归到原始模型，在编码端将输入句子编码成一个固定的维度的向量。作者说这样可以迫使模型学习捕捉句子的意思，尽管句子的表达方式不同。 最重要的一点是，实验发现将句子逆序输入可以明显改善LSTM模型的表现。 一个猜测的解释（因为作者也不知道具体的原因）是这样做可以减小“minimal time lag”（最短时间间隔），举例，输入是“ABC”，对应输出是“XYZ”，“A”与对应的“X”的间隔是3，“B”和“C”与其对应的间隔也是3，所以最短时间间隔是3。如果将输入逆序，以“CAB”作为输入，“A”与“X”的间隔是1，最短时间间隔就减小为1。于是作者猜测将输入逆序虽然没有减少源句子（输入）与目标句子（输出）的平均间隔，但是源句子与目标句子是前几个词的距离减少了，于是句子的“最短时间间隔”减少了。通过后向传播可以更快地在源句子和目标句子之间“建立通信”，整体的性能也有了显着的改善。 标题 说明 附加 《Sequence to Sequence Learning with Neural Networks》 原始论文 2014 Sequence to Sequence Learning with Neural Networks mstar1992 论文解读 2017]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures]]></title>
    <url>%2F2018%2F09%2F21%2FSelf-Attention%20A%20Targeted%20Evaluation%20of%20Neural%20Machine%20Translation%20Architectures%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 《Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures》 原始论文 2018 为什么使用自注意力机制？ 机器之心 浅析 Why Self-Attention? A Targeted Evaluation of Neural Machine Translation ArchitecturesGongbo Tang, Mathias Müller, Annette Rios, Rico Sennrich(Submitted on 27 Aug 2018 (v1), last revised 28 Aug 2018 (this version, v2)) Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation. 近期，非循环架构（卷积、自注意力）在神经机器翻译任务中的表现优于 RNN。CNN 和自注意力网络连接远距离单词的路径比 RNN 短，有研究人员推测这正是其建模长距离依赖能力得到提高的原因。但是，这一理论论断并未得到实验验证，对这两种网络的强大性能也没有其他深入的解释。我们假设 CNN 和自注意力网络的强大性能也可能来自于其从源文本提取语义特征的能力。我们在两个任务（主谓一致任务和词义消歧任务）上评估了 RNN、CNN 和自注意力网络的性能。实验结果证明：1）自注意力网络和 CNN 在建模长距离主谓一致时性能并不优于 RNN；2）自注意力网络在词义消歧方面显著优于 RNN 和 CNN。Comments: 10 pages, 5 figures, accepted by EMNLP 2018 (v2: corrected author names)Subjects: Computation and Language (cs.CL)Cite as: arXiv:1808.08946 [cs.CL] (or arXiv:1808.08946v2 [cs.CL] for this version) 本论文的主要贡献如下： 检验了这一理论断言：具备更短路径的架构更擅长捕捉长距离依赖。研究者在建模长距离主谓一致任务上的实验结果并没有表明，Transformer 或 CNN 在这方面优于 RNN。 通过实验证明 Transformer 中注意力头的数量对其捕捉长距离依赖的能力有所影响。具体来说，多头注意力对使用自注意力机制建模长距离依赖是必要的。 通过实验证明 Transformer 擅长 WSD，这表明 Transformer 是强大的语义特征提取器。]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>Self-Attention</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[损失函数]]></title>
    <url>%2F2018%2F09%2F19%2F%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[通常机器学习每一个算法中都会有一个目标函数，算法的求解过程是通过对这个目标函数优化的过程。在分类或者回归问题中，通常使用损失函数（代价函数）作为其目标函数。损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的算法使用的损失函数不一样。损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。通常表示为如下： \theta ^{*}=argmin\frac{1}{N}\sum_{i=1}^{N}L(y_{i},f(x_{i};\theta_{i} ))+\lambda \Phi (\theta )常见的损失函数0-1 损失函数和绝对值损失函数0-1损失是指，预测值和目标值不相等为1，否则为0： L(Y,f(X))=\left\{\begin{matrix} 1, Y\neq f(X)\\ 0, Y=f(X) \end{matrix}\right.感知机就是用的这种损失函数。但是由于相等这个条件太过严格，因此我们可以放宽条件，即满足 |Y−f(X)|&lt;T 时认为相等。 L(Y,f(X))=\left\{\begin{matrix} 1, |Y-f(X)|\geq T\\ 0, |Y=f(X)|]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>损失函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Batch Normalization——Accelerating Deep Network Training by Reducing Internal]]></title>
    <url>%2F2018%2F09%2F17%2F%E6%89%B9%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》 原文 2015 Batch Normalization 论文翻译 DCD_Lin 深入理解Batch Normalization批标准化 郭耀华 优化深度神经网络（三）Batch Normalization 喵喵帕斯 Batch Normalization 学习笔记 hjimce 2016 白化我们已经了解了如何使用PCA降低数据维度。在一些算法中还需要一个与之相关的预处理步骤，这个预处理过程称为白化（一些文献中也叫sphering）。举例来说，假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的。白化的目的就是降低输入的冗余性；更正式的说，我们希望通过白化过程使得学习算法的输入具有如下性质：(i)特征之间相关性较低；(ii)所有特征具有相同的方差。s Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate ShiftSergey Ioffe, Christian Szegedy(Submitted on 11 Feb 2015 (v1), last revised 2 Mar 2015 (this version, v3)) Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters. 当前神经网络层之前的神经网络层的参数变化，引起神经网络每一层输入数据的分布产生了变化，这使得训练一个深度神经网络（Deep Neural Networks）变得复杂。这样就要求使用更小的学习率，参数初始化也需要更为谨慎的设置。并且由于非线性饱和（注：如sigmoid激活函数的非线性饱和问题），训练一个深度神经网络会非常困难。我们称这个现象为：internal covariate shif；同时利用归一化层输入解决这个问题。我们将归一化层输入作为神经网络的结构，并且对每一个小批量训练数据执行这一操作。Batch Normalization（BN） 能使用更高的学习率，并且不需要过多的注重参数初始化问题。BN 的过程与正则化相似，在某些情况下可以去除Dropout。将BN应用到一个state-of-the-art的图片分类模型中时，使用BN只要1/14的训练次数就能够达到同样的精度。使用含有BN神经网络模型能提升现有最好的ImageNet分类结果：在top-5 验证集中达到4.9%的错误率（测试集为4.8%），超出了人类的分类精度。 Subjects: Machine Learning (cs.LG)Cite as: arXiv:1502.03167 [cs.LG] (or arXiv:1502.03167v3 [cs.LG] for this version)]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>BN</tag>
        <tag>批正则化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Group Normalization]]></title>
    <url>%2F2018%2F09%2F15%2FGroup%20Normalization%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 《Group Normalization》 原文 2018 Group NormalizationYuxin Wu, Kaiming He(Submitted on 22 Mar 2018 (v1), last revised 11 Jun 2018 (this version, v3)) Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems —- BN’s error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN’s usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN’s computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries. Comments: v3: Update trained-from-scratch results in COCO to 41.0AP. Code and models at this https URLSubjects: Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)Cite as: arXiv:1803.08494 [cs.CV] (or arXiv:1803.08494v3 [cs.CV] for this version)]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>Group Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[残差网络]]></title>
    <url>%2F2018%2F09%2F14%2F%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[残差网络(百度百科)残差网络是2015年提出的深度卷积网络，一经出世，便在ImageNet中斩获图像分类、检测、定位三项的冠军。 残差网络更容易优化，并且能够通过增加相当的深度来提高准确率。核心是解决了增加深度带来的副作用（退化问题），这样能够通过单纯地增加网络深度，来提高网络性能。 残差网络背景 残差网络解决退化问题 残差网络资源列表Deep Residual Learning for Image RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun(Submitted on 10 Dec 2015) 更深的神经网络往往更难以训练，我们在此提出一个残差学习的框架，以减轻网络的训练负担，这是个比以往的网络要深的多的网络。我们明确地将层作为输入学习残差函数，而不是学习未知的函数。我们提供了非常全面的实验数据来证明，残差网络更容易被优化，并且可以在深度增加的情况下让精度也增加。在ImageNet的数据集上我们评测了一个深度152层（是VGG的8倍）的残差网络，但依旧拥有比VGG更低的复杂度。残差网络整体达成了3.57%的错误率，这个结果获得了ILSVRC2015的分类任务第一名，我们还用CIFAR-10数据集分析了100层和1000层的网络。 在一些计算机视觉识别方向的任务当中，深度表示往往是重点。我们极深的网络让我们得到了28%的相对提升（对COCO的对象检测数据集）。我们在深度残差网络的基础上做了提交的版本参加ILSVRC和COCO2015的比赛，我们还获得了ImageNet对象检测，Imagenet对象定位，COCO对象检测和COCO图像分割的第一名。 Comments: Tech reportSubjects: Computer Vision and Pattern Recognition (cs.CV)Cite as: arXiv:1512.03385 [cs.CV] (or arXiv:1512.03385v1 [cs.CV] for this version)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Residual Learning for Image Recognition]]></title>
    <url>%2F2018%2F09%2F14%2FDeep%20Residual%20Learning%20for%20Image%20Recognition%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 《Deep Residual Learning for Image Recognition》 原始论文 2015 解决了神经网络 “退化问题” ResNet网络，本文获得2016 CVPR best paper，获得了ILSVRC2015的分类任务第一名。 《Deep Residual Learning for Image Recognition》HTML 原始论文网页版 《Deep Residual Learning for Image Recognition（译）》 zhwhong 译文 2017 《Deep Residual Learning for Image Recognition（译）》 XlyPb 译文 2017 《论文理论解读》 作者 junlinhe@yeah.net Deep Residual Learning for Image RecognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun(Submitted on 10 Dec 2015) 更深的神经网络往往更难以训练，我们在此提出一个残差学习的框架，以减轻网络的训练负担，这是个比以往的网络要深的多的网络。我们明确地将层作为输入学习残差函数，而不是学习未知的函数。我们提供了非常全面的实验数据来证明，残差网络更容易被优化，并且可以在深度增加的情况下让精度也增加。在ImageNet的数据集上我们评测了一个深度152层（是VGG的8倍）的残差网络，但依旧拥有比VGG更低的复杂度。残差网络整体达成了3.57%的错误率，这个结果获得了ILSVRC2015的分类任务第一名，我们还用CIFAR-10数据集分析了100层和1000层的网络。 在一些计算机视觉识别方向的任务当中，深度表示往往是重点。我们极深的网络让我们得到了28%的相对提升（对COCO的对象检测数据集）。我们在深度残差网络的基础上做了提交的版本参加ILSVRC和COCO2015的比赛，我们还获得了ImageNet对象检测，Imagenet对象定位，COCO对象检测和COCO图像分割的第一名。 Comments: Tech reportSubjects: Computer Vision and Pattern Recognition (cs.CV)Cite as: arXiv:1512.03385 [cs.CV] (or arXiv:1512.03385v1 [cs.CV] for this version)]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>ResNet</tag>
        <tag>残差网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generative Adversarial Networks]]></title>
    <url>%2F2018%2F09%2F11%2FGenerative%20Adversarial%20Networks%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 《Generative Adversarial Networks》 原始论文 2014 《Generative Adversarial Networks》HTML 原始论文网页版 2014 Code and hyperparameters for the paper 作者提供代码 2014 Keras-GAN 实现代码 2018 Generative Adversarial Nets（译） XIyPb 翻译 2017 《GAN完整理论推导与实现》 论文证明详细解释 2017 《Generative Adversarial Networks》Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. Subjects: Machine Learning (stat.ML); Machine Learning (cs.LG)Cite as: arXiv:1406.2661 [stat.ML] (or arXiv:1406.2661v1 [stat.ML] for this version) Keras-GAN123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162from __future__ import print_function, divisionfrom keras.datasets import mnistfrom keras.layers import Input, Dense, Reshape, Flatten, Dropoutfrom keras.layers import BatchNormalization, Activation, ZeroPadding2Dfrom keras.layers.advanced_activations import LeakyReLUfrom keras.layers.convolutional import UpSampling2D, Conv2Dfrom keras.models import Sequential, Modelfrom keras.optimizers import Adamimport matplotlib.pyplot as pltimport sysimport numpy as npclass GAN(): def __init__(self): self.img_rows = 28 self.img_cols = 28 self.channels = 1 self.img_shape = (self.img_rows, self.img_cols, self.channels) self.latent_dim = 100 optimizer = Adam(0.0002, 0.5) # Build and compile the discriminator self.discriminator = self.build_discriminator() self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) # Build the generator self.generator = self.build_generator() # The generator takes noise as input and generates imgs z = Input(shape=(self.latent_dim,)) img = self.generator(z) # For the combined model we will only train the generator self.discriminator.trainable = False # The discriminator takes generated images as input and determines validity validity = self.discriminator(img) # The combined model (stacked generator and discriminator) # Trains the generator to fool the discriminator self.combined = Model(z, validity) self.combined.compile(loss='binary_crossentropy', optimizer=optimizer) def build_generator(self): model = Sequential() model.add(Dense(256, input_dim=self.latent_dim)) model.add(LeakyReLU(alpha=0.2)) model.add(BatchNormalization(momentum=0.8)) model.add(Dense(512)) model.add(LeakyReLU(alpha=0.2)) model.add(BatchNormalization(momentum=0.8)) model.add(Dense(1024)) model.add(LeakyReLU(alpha=0.2)) model.add(BatchNormalization(momentum=0.8)) model.add(Dense(np.prod(self.img_shape), activation='tanh')) model.add(Reshape(self.img_shape)) model.summary() noise = Input(shape=(self.latent_dim,)) img = model(noise) return Model(noise, img) def build_discriminator(self): model = Sequential() model.add(Flatten(input_shape=self.img_shape)) model.add(Dense(512)) model.add(LeakyReLU(alpha=0.2)) model.add(Dense(256)) model.add(LeakyReLU(alpha=0.2)) model.add(Dense(1, activation='sigmoid')) model.summary() img = Input(shape=self.img_shape) validity = model(img) return Model(img, validity) def train(self, epochs, batch_size=128, sample_interval=50): # Load the dataset (X_train, _), (_, _) = mnist.load_data() # Rescale -1 to 1 X_train = X_train / 127.5 - 1. X_train = np.expand_dims(X_train, axis=3) # Adversarial ground truths valid = np.ones((batch_size, 1)) fake = np.zeros((batch_size, 1)) for epoch in range(epochs): # --------------------- # Train Discriminator # --------------------- # Select a random batch of images idx = np.random.randint(0, X_train.shape[0], batch_size) imgs = X_train[idx] noise = np.random.normal(0, 1, (batch_size, self.latent_dim)) # Generate a batch of new images gen_imgs = self.generator.predict(noise) # Train the discriminator d_loss_real = self.discriminator.train_on_batch(imgs, valid) d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake) d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) # --------------------- # Train Generator # --------------------- noise = np.random.normal(0, 1, (batch_size, self.latent_dim)) # Train the generator (to have the discriminator label samples as valid) g_loss = self.combined.train_on_batch(noise, valid) # Plot the progress print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss)) # If at save interval =&gt; save generated image samples if epoch % sample_interval == 0: self.sample_images(epoch) def sample_images(self, epoch): r, c = 5, 5 noise = np.random.normal(0, 1, (r * c, self.latent_dim)) gen_imgs = self.generator.predict(noise) # Rescale images 0 - 1 gen_imgs = 0.5 * gen_imgs + 0.5 fig, axs = plt.subplots(r, c) cnt = 0 for i in range(r): for j in range(c): axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray') axs[i,j].axis('off') cnt += 1 fig.savefig("images/%d.png" % epoch) plt.close()if __name__ == '__main__': gan = GAN() gan.train(epochs=30000, batch_size=32, sample_interval=200)]]></content>
      <categories>
        <category>论文</category>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>生成式对抗网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[期望、似然函数、先验概率、后验概率以及共轭先验概念]]></title>
    <url>%2F2018%2F08%2F30%2F%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%E3%80%81%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E3%80%81%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BB%A5%E5%8F%8A%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[离散型数学期望和连续型数学期望 似然函数似然函数 $P(X|\theta)$ 表达了在不同的参数 $\theta$ 下，观测数据出现的可能性的大小。注意，似然函数不是参数 $\theta$ 的概率分布，并且似然函数关于参数 $\theta$ 的积分并不（一定）等于1。 似然与概率的区别 补充：L($\theta$|x)=f(x|$\theta$)这个等式表示的是对于事件发生的两种角度的看法。其实等式两遍都是表示的这个事件发生的概率或者说可能性。再给定一个样本x后，我们去想这个样本出现的可能性到底是多大。统计学的观点始终是认为样本的出现是基于一个分布的。那么我们去假设这个分布为f，里面有参数 $\theta$。对于不同的$\theta$，样本的分布不一样。f(x|$\theta$)表示的就是在给定参数$\theta$的情况下，x出现的可能性多大。L($\theta$|x)表示的是在给定样本x的时候，哪个参数$\theta$使得x出现的可能性多大。所以其实这个等式要表示的核心意思都是在给一个$\theta$和一个样本x的时候，整个事件发生的可能性多大。 似然与概率的联系 共轭先验分布在贝叶斯统计中，如果后验分布与先验分布属于同类，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。比如，高斯分布家族在高斯似然函数下与其自身共轭(自共轭)。这个概念，以及”共轭先验”这个说法，由霍华德·拉法拉和罗伯特·施莱弗尔在他们关于贝叶斯决策理论的工作中提出。类似的概念也曾由乔治·阿尔弗雷德·巴纳德独立提出。 具体地说，就是给定贝叶斯公式 $p(\theta|x)=\dfrac{p(x|\theta)p(\theta)}{\int p(x|\theta’)d(\theta’)}$，假定似然函数 p(x|theta) 是已知的，问题就是选取什么样的先验分布 $p(\theta)$ 会让后验分布与先验分布具有相同的数学形式。 共轭先验的好处主要在于代数上的方便性，可以直接给出后验分布的封闭形式，否则的话只能数值计算。共轭先验也有助于获得关于似然函数如何更新先验分布的直观印象。所有指数家族的分布都有共轭先验。 参考文献[1] 白马负金羁. 先验概率、后验概率以及共轭先验[DB/OL]. https://blog.csdn.net/baimafujinji/article/details/51374202, 2018-08-15. [2] Yeung Evan. 如何理解似然函数?[DB/OL]. https://www.zhihu.com/question/54082000, 2018-08-15. [3] 挧熙33. 共轭先验分布[DB/OL]. https://baike.baidu.com/item/%E5%85%B1%E8%BD%AD%E5%85%88%E9%AA%8C%E5%88%86%E5%B8%83/15696678, 2018-08-27.]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>期望</tag>
        <tag>似然函数</tag>
        <tag>先验概率</tag>
        <tag>后验概率</tag>
        <tag>共轭先验</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率分布距离度量函数]]></title>
    <url>%2F2018%2F08%2F29%2F%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[信息熵熵是传输一个随机变量状态值所需的比特位的下界。 信息论之父 C. E. Shannon 在 1948 年发表的论文“通信的数学理论（ A Mathematical Theory of Communication ）”中， Shannon 指出，任何信息都存在冗余，冗余大小与信息中每个符号（数字、字母或单词）的出现概率或者说不确定性有关。Shannon 借鉴了热力学的概念，把信息中排除了冗余后的平均信息量称为“信息熵”，并给出了计算信息熵的数学表达式。 信息熵基本内容通常，一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。概率大，出现机会多，不确定性小；反之就大。不确定性函数 f 是概率 P 的单调递降函数；两个独立符号所产生的不确定性应等于各自不确定性之和，即f（P1，P2）=f（P1）+ f（P2），这称为可加性。同时满足这两个条件的函数f是对数函数，即 $f(P)=log(\dfrac{1}{p})=-log(p)$。 在信源中，考虑的不是某一单个符号发生的不确定性，而是要考虑这个信源所有可能发生情况的平均不确定性。若信源符号有n种取值：U1…Ui…Un，对应概率为：P1…Pi…Pn，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性-logPi的统计平均值（E），可称为信息熵，即 $H(U)=E[-logp_i]=-\sum_{i=1}^N p_ilogp_i$，式中对数一般取2为底，单位为比特。 当所有的 $p(x_i)$ 值都相等，且值为 $p(x_i)=\dfrac{1}{N}$ 时，熵取得最大值。$H(U) = log(N)$。 交叉熵交叉熵（Cross Entropy）是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。语言模型的性能通常用交叉熵和复杂度（perplexity）来衡量。交叉熵的意义是用该模型对文本识别的难度，或者从压缩的角度来看，每个词平均要用几个位来编码。复杂度的意义是用该模型表示这一文本平均的分支数，其倒数可视为每个词的平均概率。 交叉熵的介绍在信息论中，交叉熵是表示两个概率分布p,q，其中p表示真实分布，q表示非真实分布，在相同的一组事件中，其中，用非真实分布q来表示某个事件发生所需要的平均比特数。从这个定义中，我们很难理解交叉熵的定义。下面举个例子来描述一下：假设现在有一个样本集中两个概率分布p,q，其中p为真实分布，q为非真实分布。假如，按照真实分布p来衡量识别一个样本所需要的编码长度的期望为： H(p)=\sum_i{p(i)}log(\dfrac{1}{p(i)})但是，如果采用错误的分布q来表示来自真实分布p的平均编码长度，则应该是： H(p,q)=\sum_i{p(i)}log(\dfrac{1}{q(i)})此时就将H(p,q)称之为交叉熵。交叉熵的计算方式如下： 对于离散变量采用以下的方式计算：$H(p,q)=\sum_x p(x)log(\dfrac{1}{q(x)})$ 对于连续变量采用以下的方式计算：$-\int _XP(x)logQ(x)dx=E_p[-logQ]$ 注意：E_{x\sim p(x)}[f(x)]=\int{f(x)p(x)}dx \approx \dfrac{1}{n}\sum_{i=1}^nf(x_i), x_i \sim p(x) 交叉熵的应用 交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。 在特征工程中，可以用来衡量两个随机变量之间的相似度。 在语言模型中（NLP）中，由于真实的分布p是未知的，在语言模型中，模型是通过训练集得到的，交叉熵就是衡量这个模型在测试集上的正确率。 相对熵相对熵，又称KL散度( Kullback–Leibler divergence)，是描述两个概率分布P和Q差异的一种方法。它是非对称的，这意味着D(P||Q) ≠ D(Q||P)。特别的，在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。有人将KL散度称为KL距离，但事实上，KL散度并不满足距离的概念，因为：(1)KL散度不是对称的；(2)KL散度不满足三角不等式。 相对熵的定义对熵（relative entropy）又称为KL散度（Kullback–Leibler divergence，简称KLD），信息散度（information divergence）。设 $P(x)$ 和 $Q(x)$ 是两个取值的两个离散概率分布，则 $P$ 对 $Q$ 的相对熵为： D(P||Q)=\sum P(x)log(\dfrac{P(x)}{Q(x)})对于连续的随机变量，定义为： D(P||Q)=\int P(x)log(\dfrac{P(x)}{Q(x)})dx相对熵是两个概率分布 $P(x)$ 和 $Q(x)$ 差别的非对称性的度量。 相对熵物理意义相对熵是用来度量使用基于 $Q$ 的编码来编码来自 $P$ 的样本平均所需的额外的比特个数。 典型情况下，$P$ 表示数据的真实分布，$Q$ 表示数据的理论分布，模型分布，或 $P$ 的近似分布。根据shannon的信息论，给定一个字符集的概率分布，我们可以设计一种编码，使得表示该字符集组成的字符串平均需要的比特数最少。假设这个字符集是 $X$ ，对 $x \in X$ ，其出现概率为 $P$，那么其最优编码平均需要的比特数等于这个字符集的熵： H(p)=\sum_{x \in X}{P(x)}log(\dfrac{1}{P(x)})在同样的字符集上，假设存在另一个概率分布 $Q$，如果用概率分布 $P$ 的最优编码（即字符 $x$ 的编码长度等于 $log(\dfrac{1}{p(x)})$），来为符合分布 $Q$ 的字符编码，那么表示这些字符就会比理想情况多用一些比特数。相对熵就是用来衡量这种情况下平均每个字符多用的比特数，因此可以用来衡量两个分布的距离，即： D_{KL}(P||Q)=-\sum_{x \in X}{P(x)}log(\dfrac{1}{P(x)}) + \sum_{x \in X}{P(x)}log(\dfrac{1}{Q(x)})=\sum_{x\in X} P(x)log(\dfrac{P(x)}{Q(x)})相对熵的性质相对熵（KL散度）有两个主要的性质，如下：（1）不对称性尽管KL散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即（2）非负性相对熵的值为非负值，即 ，证明可用吉布斯不等式。 吉布斯不等式若 $\sum_{i=1}^{n}p_i=\sum_{i=1}^{n}q_i=1$，且 $p_i, q_i \in (0,1]$，则有：$-\sum_{i=1}^{n}p_ilog(p_i)\leq -\sum_{i=1}^{n}p_ilog(q_i)$，等号当且仅当 $\forall i, p_i=q_i$ 相对熵的应用相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。所以相对熵（KL散度）可以用于比较文本的相似度，先统计出词的频率，然后计算相对熵。另外，在多指标系统评估中，指标权重分配 [2] 是一个重点和难点，也通过相对熵可以处理。 JS散度（Jensen–Shannon divergence）JS散度度量了两个概率分布的相似度，基于KL散度的变体，解决了KL散度非对称的问题。一般地，JS散度是对称的，其取值是0到1之间。定义如下： JSD(P_1||P_2)=\dfrac{1}{2}KL(P_1||\dfrac{P_1+P_2}{2})+\dfrac{1}{2}KL(P_2||\dfrac{P_1+P_2}{2})KL散度和JS散度度量的时候有一个问题：如果两个分配P,Q离得很远，完全没有重叠的时候，那么KL散度值是没有意义的，而JS散度值是一个常数。这在学习算法中是比较致命的，这就意味这这一点的梯度为0。梯度消失了。 Wasserstein距离 参考文献[1] ajiao_nihao‏. 交叉熵[DB/OL]. https://baike.baidu.com/item/%E4%BA%A4%E5%8F%89%E7%86%B5/8983241, 2018-08-28. [2] 蒙牛纯爷们‏. 相对熵[DB/OL]. https://baike.baidu.com/item/%E7%9B%B8%E5%AF%B9%E7%86%B5, 2018-08-28. [3] miaoweijun‏. 信息熵[DB/OL]. https://baike.baidu.com/item/%E4%BF%A1%E6%81%AF%E7%86%B5, 2018-08-28. [4] 维基百科. Jensen–Shannon divergence[DB/OL]. https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence, 2018-08-28. [5] CodeTutor. 概率论——Wasserstein距离[DB/OL]. https://blog.csdn.net/victoriaw/article/details/56674777, 2018-08-28.]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>交叉熵</tag>
        <tag>信息熵</tag>
        <tag>相对熵</tag>
        <tag>KL散度</tag>
        <tag>Wasserstein距离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主成成分分析、自编码器和变分自编码器解析]]></title>
    <url>%2F2018%2F08%2F26%2F%E4%B8%BB%E6%88%90%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E3%80%81%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E5%92%8C%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[介绍主成分分析（PCA）和自编码器（AutoEncoders, AE）是无监督学习中的两种代表性方法。 PCA 的地位不必多说，只要是讲到降维的书，一定会把 PCA 放到最前面，它与 LDA 同为机器学习中最基础的线性降维算法，SVM/Logistic Regression、PCA/LDA 也是最常被拿来作比较的两组算法。 自编码器虽然不像 PCA 那般在教科书上随处可见，但是在早期被拿来做深度网络的逐层预训练，其地位可见一斑。尽管在 ReLU、Dropout 等技术出现之后，人们不再使用 AutoEncoders 来预训练，但它延伸出的稀疏 AutoEncoders，降噪 AutoEncoders 等仍然被广泛用于表示学习。2017 年 Kaggle 比赛 Porto Seguro’s Safe Driver Prediction 的冠军就是使用了降噪 AutoEncoders 来做表示学习，最终以绝对优势击败了手工特征工程的选手们。 PCA 和 AutoEncoders 都是非概率的方法，它们分别有一种对应的概率形式叫做概率 PCA (Probabilistic PCA) 和变分自编码器（Variational AE, VAE），本文的主要目的就是整理一下 PCA、概率 PCA、AutoEncoders、变分 AutoEncoders 这四者的关系。 先放结论，后面就围绕这个表格展开： 降维方法 线性 非线性 生成式 概率PCA 变分自编码器 非生成式 PCA 自编码器 降维的线性方法和非线性方法降维分为线性降维和非线性降维，这是最普遍的分类方法。 PCA 和 LDA 是最常见的线性降维方法，它们按照某种准则为数据集找到一个最优投影方向 W 和截距 b，然后做变换得到降维后的数据集。因为是一个线性变换（严格来说叫仿射变换，因为有截距项），所以这两种方法叫做线性降维。 LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。 PCA 与 LDA 异同分析： 首先我们看看相同点： 两者均可以对数据进行降维。 两者在降维时均使用了矩阵特征分解的思想。 两者都假设数据符合高斯分布。 我们接着看看不同点： LDA是有监督的降维方法，而PCA是无监督的降维方法 LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 LDA除了可以用于降维，还可以用于分类。 LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。 非线性降维的两类代表方法是流形降维和 AutoEncoders，这两类方法也体现出了两种不同角度的“非线性”。流形方法的非线性体现在它认为数据分布在一个低维流形上，而流形本身就是非线性的，流形降维的代表方法是两篇 2000 年的 Science 论文提出的：多维放缩（multidimensional scaling, MDS）和局部线性嵌入（locally linear embedding, LLE）。两种流形方法发表在同一年的 Science 上。 AutoEncoders 的非线性和神经网络的非线性是一回事，都是利用堆叠非线性激活函数来近似任意连续函数。事实上，AutoEncoders 就是一种神经网络，只不过它的输入和输出相同，真正有意义的地方不在于网络的输出，而是在于网络的权重。 降维的生成式方法和非生成式方法两类方法 降维还可以分为生成式方法（概率方法）和非生成式方法（非概率方法）。 教科书对 PCA 的推导一般是基于最小化重建误差或者最大化可分性的，或者说是通过提取数据集的结构信息来建模一个约束最优化问题来推导的。事实上，PCA 还有一种概率形式的推导，那就是概率 PCA，PRML 里面有对概率 PCA 的详细讲解，感兴趣的读者可以去阅读。需要注意的是，概率 PCA 不是 PCA 的变体，它就是 PCA 本身，概率 PCA 是从另一种角度来推导和理解 PCA，它把 PCA 纳入了生成式的框架。 设 $X$ 是我们拿到的数据集，我们的目的是得到数据集中每个样本的低维表示 $Z$，其中 $dim(z) &lt; dim(x)$。 降维的非生成式方法不需要概率知识，而是直接利用数据集 $X$ 的结构信息建模一个最优化问题，然后求解这个问题得到 $X$ 对应的 $Z$。 降维的生成式方法认为数据集 $X$ 是对一个随机变量 x 的 n 次采样，而随机变量 x 依赖于随机变量 z ，对 z 进行建模： z \sim p_\theta(Z)再对这个依赖关系进行建模： x|z \sim p_\theta(x|z)有了这两个公式，我们就可以表达出随机变量 x 的分布： x \sim p_\theta(x)=\int_Zp_\theta(x|z)p_\theta(z)dz随后我们利用数据集对分布的参数 θ 进行估计，就得到这几个分布。好了，设定了这么多，可是降维降在哪里了呢，为什么没有看到？ 回想一下降维的定义：降维就是给定一个高维样本 x，给出对应的低维表示 z，这恰好就是 p(z|x) 的含义。所以我们只要应用 Bayes 定理求出这个概率即可： p_\theta(z|x)=\dfrac{p_\theta(x|z)p_\theta(z)dz}{\int_Zp_\theta(x|z)p_\theta(z)dz}这样我们就可以得到每个样本点 x 上的 z 的分布 p(z|X=x) ，可以选择这个分布的峰值点作为 z，降维就完成了。 Q：那么问题来了，生成式方法和非生成式方法哪个好呢？ A：或许是非生成式方法好，一两行就能设定完，君不见生成式方法你设定了一大段？ 应该会有很多人这样想吧？事实也的确如此，上面这个回答在一定意义上是正确的。如果你只是为了对现有的数据进行降维，而没有其他需求，那么简单粗暴的非生成式方法当然是更好的选择。 那么，在什么情况下，或者说什么需求下，生成式方法是更好的选择更好呢？答案就蕴含在“生成式”这个名称中：在需要生成新样本的情况下，生成式方法是更好的选择。 在需要生成新样本时，非生成式方法需要对 z 的概率分布进行代价巨大的数值逼近，然后才能从分布中采样；生成式方法本身就对 z 的概率分布进行了建模，因此可以直接从分布中进行采样。所以，在需要生成新样本时，生成式方法是更好的选择，甚至是必然的选择。 概率 PCA 和 VAE下面简单整理一下这四种降维方法。注意一些术语，编码=降维，解码=重建，原数据=观测变量，降维后的数据=隐变量。 PCA原数据： X=\{x|x\in R^d\}编码后的数据： z = W^T(x+b), z\in R^c解码后的数据： \hat x = Wz - b, x\in R^d重建误差： \sum||x-\hat x||^p_p最小化重建误差，就可以得到 W 和 b 的最优解和解析解，PCA 的求解就完成了。 补充说明： PCA 中的 p=2 ，即最小化二范数意义下的重建误差，如果 p=1 的话我们就得到了鲁棒 PCA (Robust PCA)。而最小化误差的二范数等价于对高斯噪声的 MLE，最小化误差的一范数等价于对拉普拉斯噪声的 MLE。 因此，PCA 其实是在假设存在高斯噪声的条件下对数据集进行重建，这个高斯误差就是我们将要在下面概率 PCA 一节中提到的 ϵ。你看，即使不是概率 PCA，其中也隐含着概率的思想。 编码和解码用到的 W 和 b 是一样的，即编码过程和解码过程是对称的，这一点与下面要讲的 AutoEncoders 是不同的。 求解上述最优化问题可以得到 b，这恰好是样本均值的相反数。也就是说，PCA 中截距项的含义是让每个样本都减去样本均值，这正是“样本中心化”的含义。 既然我们已经知道求出来的截距就是样本均值，所以干脆一开始就对样本进行中心化，这样在使用 PCA 的时候就可以忽略截距项 b 而直接使用，变量就只剩下 W 了。教科书上讲解 PCA 时一般都是上来就说“使用 PCA 之前需要进行样本中心化”，但是没有人告诉我们为什么要这样做，现在大家应该明白为什么要进行中心化了吧。 AutoEncoders原数据： X=\{x|x\in R^d\}编码后的数据： z = \sigma(Wx+b), z\in R^c解码后的数据： \hat x = \sigma(\hat Wz + \hat b), z\in R^d重建误差： \sum||x-\hat x||^p_p最小化重建误差，利用反向传播算法可以得到的局部最优解&amp;数值解，AutoEncoders 的求解完成。 补充说明： 这里可以使用任意范数，每一个范数都代表我们对数据的一种不同的假设。为了和 PCA 对应，我们也取 p=2。 σ(·) 是非线性激活函数。AutoEncoder 一般都会堆叠多层，方便起见我们只写了一层。 $W$ 和 $\hat W$ 是不同的权重矩阵，这是因为经过非线性变换之后我们已经无法将样本再用原来的基 W 进行表示了，必须要重新训练解码的基 。甚至，AutoEncoders 的编码器和解码器堆叠的层数都可以不同，例如可以用 4 层来编码，用 3 层来解码。 概率PCA隐变量边缘分布： p(z)=N(z|0,I)观测变量条件分布： p_\theta (x|z) = N(x|f(z;\theta),{\sigma}^2 I)均值函数： f(z;\theta)=Wz+\mux 的生成过程： x=f(z;\theta)+\epsilon, \epsilon \in N(0,{\sigma}^2 I))因为 $p(z)$ 和 $p_θ(x|z)$ 都是高斯分布，且 $p_θ(x|z)$ 的均值 $f(z;θ) = Wz+μ$ 是 z 的线性函数，所以这是一个线性高斯模型。 线性高斯模型有一个非常重要的性质： $p_θ(x)$ 和$p_θ(z|x)$ 也都是高斯分布。这个性质保证了我们能够使用极大似然估计或者EM算法来求解PCA。 如果没有这个性质的话，我们就只能借助变分法（变分 AE 采用的）或者对抗训练（GAN 采用的）来近似 $p_θ(x)$ 和 $p_θ(z|x)$ 了。有了这个优秀的性质之后，我们至少有三种方法可以求解概率 PCA： 是一个形式已知，仅参数未知的高斯分布，因此可以用极大似然估计来求解 θ。p_\theta(x)=\int_Zp_\theta(x|z)p_\theta(z)dz 也是一个形式已知，仅参数未知的高斯分布，因此可以用 EM 算法来求解 θ，顺便还能得到隐变量 z。 p_\theta(z|x)=\dfrac{p_\theta(x|z)p_\theta(z)dz}{\int_Zp_\theta(x|z)p_\theta(z)dz} 甚至也可以引入一个变分分布 $q_Φ(z|x)$ 来求解概率 PCA。 一旦求出了 θ，我们就得到了所有的四个概率： p(z), p_\theta(x|z), p_\theta(x), p_\theta(z|x)有了这四个概率，我们就可以做这些事情了： 降维：给定样本 x ，就得到了分布 $p_θ(z|X=x)$ ，取这个分布的峰值点 z 就是降维后的数据； 重建：给定降维后的样本 z ，就得到了分布 $p_θ(x|Z=z)$，取这个分布的峰值点 x 就是重建后的数据； 生成：从分布 p(z) 中采样一个，就得到了分布，取这个分布的峰值点就是新生成的数据； 密度估计：给定样本 x ，就得到了这一点的概率密度 pθ(X=x) 。 PCA 只能做到 1 和 2，对 3 和 4无力，这一点我们已经分析过了。 Q：为什么隐变量要取单位高斯分布（标准正态分布）？ A：这是两个问题。 subQ1：为什么要取高斯分布？ subA1：为了求解方便，如果不取高斯分布，那么 $p_θ(x)$ 有很大的可能没有解析解，这会给求解带来很大的麻烦。还有一个原因，回想生成新样本的过程，要首先从 p(z) 中采样一个，高斯分布采样简单。 subQ2：为什么是零均值单位方差的？ subA2：完全可以取任意均值和方差，但是我们要将 $p(z)$ 和 $p_θ(x|z)$ 相乘，均值和方差部分可以挪到 $f(z;θ)$ 中，所以 $p(z)$ 的均值和方差取多少都无所谓，方便起见就取单位均值方差了。 Q：$p_θ(x|z)$ 为什么选择了高斯分布呢？ A：因为简单，和上一个问题的一样。还有一个直觉的解释是 $p_θ(x|z)$ 认为 x 是由 f(z;θ) 和噪声 ϵ 加和而成的，如果 ϵ 是高斯分布的话，恰好和 PCA 的二范数重建误差相对应，这也算是一个佐证吧。 Q：$p_θ(x|z)$ 的方差为什么选择了各向同性的，而不是更一般的 ∑ 呢？ A：方差可以选择一般的 ∑ ，但是个参数一定会给求解带来困难，所导出的方法虽然也是线性降维，但它已经不是 PCA 了，而是另外的方法。方差也可以选择成一个的各向异性的对角阵 λ，这样只有 d 个参数，事实上这就是因子分析，另一种线性降维方法。只有当方差选择成各向同性的对角阵时，导出来的方法才叫主成分分析，这个地方 PRML 里有介绍。 变分AutoEncoders隐变量边缘分布： p(z)=N(z|0,I)观测变量条件分布： p_\theta (x|z) = N(x|f(z;\theta),{\sigma}^2 I)均值函数： f(z;\theta)=\sigma(Wz+\mu)x 的生成过程： x=f(z;\theta)+\epsilon, \epsilon \in N(0,{\sigma}^2 I))因为 f(z;θ) 是 z 的非线性函数，所以这不再是一个线性高斯模型。观测变量的边缘分布： p_\theta(x)=\int_Zp_\theta(x|z)p_\theta(z)dz没有解析形式。这就意味着我们无法直接使用极大似然估计来求解参数 θ。隐变量的后验分布： p_\theta(z|x)=\dfrac{p_\theta(x|z)p_\theta(z)dz}{\int_Zp_\theta(x|z)p_\theta(z)dz}也没有解析形式（这是当然，因为分母没有解析形式了）。这就意味着我们也无法通过 EM 算法来估计参数和求解隐变量。 那么，建出来的模型该怎么求解呢？这就需要上变分推断（Variational Inference），又称变分贝叶斯（Variational Bayes）了。本文不打算细讲变分推断，仅仅讲一下大致的流程。更多关于 VAE 的内容见《变分自编码器》。 变分推断会引入一个变分分布 $q_Φ(z|x)$ 来近似没有解析形式的后验概率 $p_θ(z|x)$ 。在变分 AE 的原文中，作者使用了 SGD 来同时优化参数 θ 和 Φ。一旦求出了这两个参数就可以得到这些概率： p(z), p_\theta(x|z), q_\phi(z|x)注意因为 $p_θ(x)$ 和 $p_θ(z|x)$ 没有解析形式，所以即使求出了 θ 我们也无法获得这两个概率。但是，正如上面说的， $q_Φ(z|x)$ 就是 $p_θ(z|x)$ 的近似，所以需要用 $p_θ(z|x)$ 的地方都可以用 $q_Φ(z|x)$ 代替。 有了这三个概率，我们就可以做这些事情了： 降维：给定样本 x，就得到了分布 $q_Φ(z|X=x)$ ，取这个分布的峰值点 z 就是降维后的数据； 重建：给定降维后的样本 z，就得到了分布 $p_θ(x|Z=z)$，取这个分布的峰值点 x 就是重建后的数据； 生成：从分布 $ p(z)$ 中采样一个，就得到了分布，取这个分布的峰值点就是新生成的数据。 与概率 PCA 不同的是，这里无法解析地得到 $p_θ(x)$ ，进行密度估计需要进行另外的设计，通过采样得到，计算代价还是比较大的，具体步骤变分 AE 的原文中有介绍。 AutoEncoders 只能做到 1 、2 和 3，对 4无力。 总结 从 PCA 和 AutoEncoders 这两节可以看出，PCA 实际上就是线性 Autoencoders。两者无论是编码解码形式还是重建误差形式都完全一致，只有是否线性的区别。线性与否给优化求解带来了不同性质：PCA 可以直接得到最优的解析解，而 AutoEncoders 只能通过反向传播得到局部最优的数值解。 从概率 PCA 和变分 AutoEncoders 这两节可以看出，概率 PCA 和变分 AutoEncoders 的唯一区别就是 f(z;θ) 是否是 z 的线性函数，但是这个区别给优化求解带来了巨大的影响。在概率 PCA 中，f(z;θ) 是线性的，所以我们得到了一个线性高斯模型，线性高斯模型的优秀性质是牵扯到的 4 个概率都是高斯分布，所以我们可以直接给出边缘分布和编码分布的解析形式，极大似然估计和 EM 算法都可以使用，一切处理都非常方便。在变分AutoEncoders中，f(z;θ) 是非线性的，所以边缘分布不再有解析形式，极大似然估计无法使用；编码分布也不再有解析形式，EM 算法无法使用，我们只能求助于变分推断，得到编码分布的近似 $q_Φ(z|x)$ ，再利用别的技巧得到边缘分布 $pθ(x)$ 的估计。 从 PCA 和概率 PCA 两小节可以看出，PCA 和概率 PCA 中 x 都是 z 的线性函数，只不过概率 PCA 显式地把高斯噪声 ϵ 写在了表达式中；PCA 没有显式写出噪声，而是把高斯噪声隐含在了二范数重建误差中。 从 AutoEncoders 和变分 AutoEncoders 这两节可以看出，AE 和 VAE 的最重要的区别在于 VAE 迫使隐变量 z 满足高斯分布 p(z)=N(z|0,I) ，而 AE 对 z 的分布没有做任何假设。 这个区别使得在生成新样本时，AE 需要先数值拟合 p(z) ，才能生成符合数据集分布的隐变量，而 VAE 直接从 N(z|0,I) 中采样一个 z ，它天然就符合数据集分布。事实上，这是因为在使用变分推断进行优化时，VAE 迫使 z 的分布向 N(z|0,I) 靠近，不过本文中没有讲优化细节，VAE 的原文中有详细的解释。 PCA 求解简单，但是都是线性降维，提取信息的能力有限；非线性的 AE 提取信息的能力强，但是求解复杂。要根据不同的场景选择不同的降维算法。 要生成新样本时，不能选择 PCA 或 AE，而是要选择概率 PCA 或 VAE。 对自编码器和变分自编码器的进一步理解《从自编码器到变分自编码器（其一）》《从自编码器到变分自编码器（其二）》 自编码器将数据作为输入并发现数据的一些潜在状态表示的模型（欠完备，稀疏，降噪，压缩）。也就是说，我们的输入数据被转换成一个编码向量，其中每个维度表示一些学到的关于数据的属性。在这里，最重要的细节是我们的编码器网络为每个编码维度输出单个值，而解码器网络随后接收这些值并尝试重构原始输入。 变分自编码器（VAE）以概率的方式描述潜在空间观察。因此，我们不会构建一个输出单个值来描述每个潜在状态属性的编码器，而是用编码器来描述每个潜在属性的概率分布。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>变分自编码器</tag>
        <tag>主成成分分析</tag>
        <tag>自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变分自编码器（Variational Auto-Encoder，VAE）]]></title>
    <url>%2F2018%2F08%2F26%2F%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%2F</url>
    <content type="text"><![CDATA[为什么要发明变分自编码器什么是变分自编码机？ 要理解变分自编码机(VAE)，我们要先从一个简单的网络开始，一步一步添加部件。 描述神经网络的常见方法，是把它解释成我们想要建模的功能的某种近似。然而，它们还能被理解为储存信息的某种数据结构。 假设有一个由数层解卷积层构成的神经网络，我们把输入设定为单位向量，然后训练该网络去降低其与目标图像之间的均方误差。这样，该图像的“数据”就包含在神经网络当前的参数之中了。 现在，我们用多张图像来尝试这一步骤。此时，输入不再是单位向量，而要改用独热向量。比如，输入 [1, 0, 0, 0] 可能是生成一张猫的图像，而输入 [0, 1, 0, 0] 则可能生成一张狗的图像。这是可行的，不过这样我们只能存储最多4张图像。让网络记住更多的图像则要使用更长的向量，同时也意味着越来越多的参数。 为此，我们需要使用实向量，而非独热向量。我们可以把它视为某个图像所对应的编码，比如用向量 [3.3, 4.5, 2.1, 9.8] 来表示猫的图像，而用向量 [3.4, 2.1, 6.7, 4.2] 来表示狗的图像，这就是 编码/解码 这一术语的来源。这一初始向量便是我们的潜在变量。 像我前面那样随机选择潜在变量，明显是个糟糕的做法。在自编码机中，我们加入了一个能自动把原始图像编码成向量的组件。上述解卷积层则能把这些向量“解码”回原始图像。 这样，我们的模型终于到了一个能有用武之地的阶段。根据需要，我们可以用尽可能多的图像来训练网络。如果保存了某张图像的编码向量，我们随时就能用解码组件来重建该图像，整个过程仅需一个标准的自编码机。 不过，这里我们想要的是构建一个生成式模型，而非仅仅是“记忆”图像数据的模糊结构。除了像前面那样从已有图像中编码出潜在向量，我们还不知道如何创造这些向量，也就无法凭空生成任何图像。 这里有个简单的办法。我们给编码网络增加一个约束，迫使它所生成的潜在向量大体上服从于单位高斯分布。该约束条件使得变分自编码机不同于标准自编码机。 现在，生成新的图像就变得容易了：我们只需从单位高斯分布中采样出一个潜在向量，并将其传到解码器即可。 实际操作中，我们需要仔细权衡网络的精确度与潜在变量在单位高斯分布上的契合程度。 神经网络可以自行决定这里的取舍。对于其中的误差项，我们归纳出独立的两种：生成误差，用以衡量网络重构图像精确度的均方误差；潜在误差，用以衡量潜在变量在单位高斯分布上的契合程度的KL散度。 generation_loss = mean(square(generated_image - real_image)) latent_loss = KL-Divergence(latent_variable, unit_gaussian) loss = generation_loss + latent_loss 为了优化KL散度，我们要用到重新参数化的一个简单技巧：生成一个均值向量一个标准差向量，而非直接生成实值向量。 我们的KL散度计算就变成这样： # z_mean and z_stddev are two vectors generated by encoder network latent_loss = 0.5 * tf.reduce_sum(tf.square(z_mean) + tf.square(z_stddev) - tf.log(tf.square(z_stddev)) - 1,1) 在计算解码网络的误差时，我们只需从标准差中取样，再加上均值向量，就能得到我们的潜在向量： samples = tf.random_normal([batchsize,n_z],0,1,dtype=tf.float32) sampled_z = z_mean + (z_stddev * samples) 除了能让我们生成随机的潜在变量，该约束还能提高VAE网络的泛化能力。 形象地说，我们可以把潜在变量视为数据的变换系数。 在[ 0, 10 ]的区间内，假定你有一系列的实数-名称对，一个实数代表一个物体的名字。例如，5.43表示苹果，5.44表示香蕉。当有人给你数字5.43时，你肯定知道他们是在谈论苹果。本质上，采用这一方式可以编码无限多的信息，毕竟[ 0, 10 ]之间的实数是有无数个。 然而，如果每当有人给告诉你一个新数字，它的高斯噪点也会增加一个时，情况会变成怎样？比如说，你收到数字是5.43，其原始数值则应在[4.4 ~ 6.4]之间，那其他人所说的真实数字就有可能是5.44（香蕉）。 所增噪点的标准差越大，其均值变量所能传递的信息就越少。 用此相同的逻辑，我们就能在编码器和解码器之间传递潜在变量。对原始图像的编码越有效，我们在高斯分布上所能取样的标准差就越大，直至为1（标准正态分布）。 这一约束迫使编码器变得非常高效，从而能创造出信息丰富的潜在变量。它所提升的泛化能力，让我们随机生成或从非训练图像编码而来的潜在变量，在解码时将能产生更好的结果。 VAE的效果有多好？ 我在MNIST手写数据集上做了一些测试，从中可以看出变分自编码机的效果有多好。 左：第1世代，中：第9世代，右：原始图像 看起来很不错！在我那没有显卡的笔记本上运行15分钟后，它就生成了一些很好的MNIST结果。 VAE的优点： 由于它们所遵循的是一种 编码-解码 模式，我们能直接把生成的图像同原始图像进行对比，这在使用GAN时是不可能的。 VAE的不足： 由于它是直接采用均方误差而非对抗网络，其神经网络倾向于生成更为模糊的图像。 也有一些需要结合VAE和GAN的研究工作：采用相同的 编码器-解码器 配置，但使用对抗网络来训练解码器。 研究详情参考论文 https://arxiv.org/pdf/1512.09300.pdf http://blog.otoro.net/2016/04/01/generating\-large\-images\-from\-latent\-vectors/ 变分自编码器经典论文《Auto-Encoding Variational Bayes》 | Diederik P Kingma, Max Welling | 2013 Abstract如何在有向概率模型中进行有效的推理和学习，在存在连续的潜在变量和难以处理的后验分布和大数据集的情况下？我们引入一个随机变分推理和学习算法，缩放到大数据集，并在一些温和的可微性条件下，甚至在棘手的情况下工作。我们的贡献是双重的。首先，我们表明，变分下界的重新参数化产生一个下界估计，可以直接使用标准随机梯度方法优化。第二，我们表明，对于I.I.D.数据集具有连续潜变量每个数据点，后推理可以特别有效的拟合近似推理模型（也称为识别模型）棘手的后部使用所提出的下界估计。理论优势反映在实验结果中。Subjects: Machine Learning (stat.ML); Machine Learning (cs.LG) 变分自编码器导论 Vivek Vyas 的《浅析变分自编码器VAE》 MoussaTintin 的《【Learning Notes】变分自编码器（Variational Auto-Encoder，VAE）》 变分自编码器基础首先阅读苏剑林 VAE 的第一篇文章《变分自编码器（一）：原来是这么一回事》。 下面示意 VAE 代码完整版 。 VAE 的示意图 VAE由三部分组成：编码器 $q(z|x)$，先验 $p(z)$，解码器 $p(x|z)$。 编码器将图像映射到针对该图像的代码的分布上。这种分布也被称为后验（posterior），因为它反映了我们关于代码应该用于给定图像之后的准确度。 1234567891011121314'''Example of VAE on MNIST dataset using MLPThe VAE has a modular design. The encoder, decoder and VAEare 3 models that share weights. After training the VAE model,the encoder can be used to generate latent vectors.The decoder can be used to generate MNIST digits by sampling thelatent vector from a Gaussian distribution with mean=0 and std=1.# Reference[1] Kingma, Diederik P., and Max Welling."Auto-encoding variational bayes."https://arxiv.org/abs/1312.6114''' VAE 网络结构1234567# network parametersinput_shape = (original_dim, )intermediate_dim = 512batch_size = 128latent_dim = 2 #可以是其它维度，这里选择二维只是为了方便可视化epochs = 50# VAE model = encoder + decoder 编码器123456789101112# build encoder modelinputs = Input(shape=input_shape, name='encoder_input')x = Dense(intermediate_dim, activation='relu')(inputs)z_mean = Dense(latent_dim, name='z_mean')(x)z_log_var = Dense(latent_dim, name='z_log_var')(x)# use reparameterization trick to push the sampling out as input# note that "output_shape" isn't necessary with the TensorFlow backendz = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])# instantiate encoder modelencoder = Model(inputs, [z_mean, z_log_var, z], name='encoder') 解码器1234567# build decoder modellatent_inputs = Input(shape=(latent_dim,), name='z_sampling')x = Dense(intermediate_dim, activation='relu')(latent_inputs)outputs = Dense(original_dim, activation='sigmoid')(x)# instantiate decoder modeldecoder = Model(latent_inputs, outputs, name='decoder') VAE123# instantiate VAE modeloutputs = decoder(encoder(inputs)[2]) #前两维度分别是均值和“方差”vae = Model(inputs, outputs, name='vae_mlp') 重参数技巧 1234567891011121314151617181920# reparameterization trick# instead of sampling from Q(z|X), sample eps = N(0,I)# z = z_mean + sqrt(var)*epsdef sampling(args): """Reparameterization trick by sampling fr an isotropic unit Gaussian. # Arguments: args (tensor): mean and log of variance of Q(z|X) # Returns: z (tensor): sampled latent vector """ z_mean, z_log_var = args # K is the keras backend batch = K.shape(z_mean)[0] dim = K.int_shape(z_mean)[1] # by default, random_normal has mean=0 and std=1.0 epsilon = K.random_normal(shape=(batch, dim)) return z_mean + K.exp(0.5 * z_log_var) * epsilon VAE 本质结构 网络输出结果网络输入时 MNIST 数据集123456789# MNIST dataset(x_train, y_train), (x_test, y_test) = mnist.load_data()image_size = x_train.shape[1]original_dim = image_size * image_sizex_train = np.reshape(x_train, [-1, original_dim])x_test = np.reshape(x_test, [-1, original_dim])x_train = x_train.astype('float32') / 255x_test = x_test.astype('float32') / 255 继续深入研究 VAE《变分自编码器（二）：从贝叶斯观点出发》《变分自编码器（三）：这样做为什么能成？》《what-is-variational-autoencoder-vae-tutorial》 条件变分自编码器Conditional Variational Autoencoders | Isaac Dykeman 参考文献[1] 苏剑林‏. 变分自编码器（一）：原来是这么一回事[DB/OL]. https://spaces.ac.cn/archives/5253, 2018-08-14. [2] 苏剑林‏. 变分自编码器（二）：从贝叶斯观点出发[DB/OL]. https://spaces.ac.cn/archives/5343, 2018-08-15. [3] 苏剑林‏. 变分自编码器（三）：这样做为什么能成？[DB/OL]. https://spaces.ac.cn/archives/5383, 2018-08-15. [4] Kingma D P, Welling M. Auto-Encoding Variational Bayes[J]. 2013. [5] Isaac Dykeman‏. Conditional Variational Autoencoders[DB/OL]. http://ijdykeman.github.io/ml/2016/12/21/cvae.html, 2018-08-15. [6] MoussaTintin. Learning Notes】变分自编码器（Variational Auto-Encoder，VAE）[DB/OL]. https://blog.csdn.net/JackyTintin/article/details/53641885, 2018-08-15. [7] AI科技大本营. 什么！你竟然还不懂变分自编码机？这个16岁的OpenAI天才实习生讲得可透彻了[DB/OL]. https://www.sohu.com/a/162863895_697750, 2018-08-26.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>变分自编码器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[胶囊网络（Capsule Network）]]></title>
    <url>%2F2018%2F08%2F24%2F%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[标题 说明 附加 《Dynamic Routing Between Capsules》 原始论文 2017 《胶囊间的动态路由》 论文翻译 AI研习社 cifar10_cnn_capsule Keras 实现 2018 CapsNet-Tensorflow TensorFlow实现 2018 Dynamic Routing Between CapsulesSara Sabour, Nicholas Frosst, Geoffrey E Hinton(Submitted on 26 Oct 2017 (v1), last revised 7 Nov 2017 (this version, v2)) A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule. Subjects: Computer Vision and Pattern Recognition (cs.CV)Cite as: arXiv:1710.09829 [cs.CV] (or arXiv:1710.09829v2 [cs.CV] for this version) Hinton大神的胶囊网络理解胶囊网络基本计算公式将 Capsule 称作向量神经元 (vector neuron, VN)，而普通的人工神经元叫做标量神经元 (scalar neuron, SN)，下表总结了 VN 和 SN 之间的差异： SN 从其他神经元接收输入标量，然后乘以标量权重再求和，然后将这个总和传递给某个非线性激活函数 (比如 sigmoid, tanh, Relu)，生出一个输出标量。该标量将作为下一层的输入变量。实质上，SN 可以用以下三个步骤来描述： 将输入标量 x 乘上权重 w 对加权的输入标量求和成标量 a 用非线性函数将标量 a 转化成标量 h VN 的步骤在 SN 的三个步骤前加一步： 将输入向量 u 用矩阵 W 加工成新的输入向量 U 将输入向量 U 乘上权重 c 对加权的输入向量求和成向量 s 用非线性函数将向量 s 转化成向量 v VN 和 SN 的过程总结如下图所示： 理解胶囊网络工作原理这一小节分析计算公式的工作原理，为了使问题具体化，假设： 上一层的 VN 代表眼睛 (u1), 鼻子 (u2) 和嘴巴 (u3)，称为低层特征。 下一层第 j 个的 VN 代表脸，称为高层特征。注意下一层可能还有很多别的高层特征，脸是最直观的一个。 第一步：矩阵转化 Uj|1 是根据眼睛位置来检测脸的位置 Uj|2 是根据鼻子位置来检测脸的位置 Uj|3 是根据嘴巴位置来检测脸的位置 现在，直觉应该是这样的：如果这三个低层特征 (眼睛，鼻子和嘴) 的预测指向相同的脸的位置和状态，那么出现在那个地方的必定是一张脸。如下图所示： 第二步：输入加权 这个步骤和标量神经元 SN 的加权形式有点类似。在 SN 的情况下，这些权重是通过反向传播 (backward propagation) 确定的，但是在 VN 的情况下，这些权重是使用动态路由 (dynamic routing) 确定的，具体算法见下面的动态路由小节 。本节只从高层面来解释动态路由，如下图： 在上图中，我们有一个较低级别 VNi需要“决定”它将发送输出给哪个更高级别 VN1和 VN2。它通过调整权重 ci1和 ci2来做出决定。 现在，高级别 VN1和 VN2已经接收到来自其他低级别 VN 的许多输入向量，所有这些输入都以红点和蓝点表示。 红点聚集在一起，意味着低级别 VN 的预测彼此接近 蓝点聚集在一起，意味着低级别 VN 的预测相差很远 那么，低别级 VNi应该输出到高级别 VN1还是 VN2？这个问题的答案就是动态路由的本质。由上图看出 VNi 的输出远离高级别 VN1 中的“正确”预测的红色簇 VNi 的输出靠近高级别 VN2 中的“正确”预测的红色簇 而动态路由会根据以上结果产生一种机制，来自动调整其权重，即调高 VN2相对的权重 ci2，而调低 VN1相对的权重 ci1。 第三步：加权求和这一步类似于普通的神经元的加权求和步骤，除了总和是向量而不是标量。加权求和的真正含义就是计算出第二步里面讲的红色簇心 (cluster centroid)。 第四步：非线性激活 这个公式的确是 VN 的一个创新，采用向量的新型非线性激活函数，又叫 squash 函数，姑且翻译成“压缩”函数。这个函数主要功能是使得 vj 的长度不超过 1，而且保持 vj和 sj同方向。 公式第一项压扁函数 如果 sj 很长，第一项约等于 1 如果 sj 很短，第一项约等于 0 公式第二项单位化向量 sj，因此第二项长度为 1 这样一来，输出向量 vj的长度是在 0 和 1 之间的一个数，因此该长度可以解释为 VN 具有给定特征的概率。 动态路由低级别 VNi 需要决定如何将其输出向量发送到高级别 VNj，它是通过改变权重 cij而实现的。首先来看看 cij的性质： 每个权重是一个非负值 对于每个低级别 VNi，所有权重 cij 的总和等于 1 对于每个低级别 VNi，权重的个数等于高级别 VN 的数量 权重由迭代动态路由 (iterative dynamic routing) 算法确定 前两个性质说明 c 符合概率概念。VN 的长度被解释为它的存在概率。VN 的方向是其特征的参数化状态。因此，对于每个低级别 VNi，其权重 cij定义了属于每个高级别 VNj 的输出的概率分布。 一言以蔽之，低级别 VN 会将其输出发送到“同意”该输出的某个高级别 VN。这是动态路由算法的本质。很绕口是吧？分析完 Hinton 论文中的动态路由算法就懂了，见截图： 算法字面解释如下： 第 1 行：这个过程用到的所有输入 - l 层的输出 Uj|i，路由迭代次数 r第 2 行：定义 bij 是 l 层 VNi 应该连接 l+1 层 VNj 的可能性，初始值为 0第 3 行：执行第 4-7 行 r 次第 4 行：对 l 层的 VNi，将 bij 用 softmax 转化成概率 cij第 5 行：对 l+1 层的 VNj，加权求和 sj第 6 行：对 l+1 层的 VNj，压缩 sj 得到 vj第 7 行：根据 Uj|i 和 vj 的关系来更新 bij算法逻辑解释如下： 第 1 行无需说明，唯一要指出的是迭代次数为 3 次，Hinton 在他论文里这样说道第 2 行初始化所有 b 为零，这是合理的。因为从第 4 行可看出，只有这样 c 才是均匀分布的，暗指“l 层 VN 到底要传送输出到 l+1 层哪个 VN 是最不确定的”第 4 行的 softmax 函数产出是非负数而且总和为 1，致使 c 是一组概率变量第 5 行的 sj 就是小节 2.3 第二步里面讲的红色簇心，可以认为是低层所有 VN 的“共识”输出第 6 行的 squash 确保向量 sj 的方向不变，但长度不超过 1，因为长度代表 VN 具有给定特征的概率第 7 行是动态路由的精华，用 Uj|i 和 vj 的点积 (dot product) 更新 bij，其中前者是 l 层 VNi对 l+1 层 VNj 的“个人”预测，而后者是所有 l 层 VN 对 l+1 层 VNj 的“共识”预测：当两者相似，点积就大，bij 就变大，低层 VNi 连接高层 VNj 的可能性就变大当两者相异，点积就小，bij 就变小，低层 VNi 连接高层 VNj 的可能性就变小下面两幅图帮助进一步理解第 7 行的含义，第一幅讲的是点积，论文中用点积来度量两个向量的相似性，当然还有很多别的度量方式。 损失函数由于 Capsule 允许多个分类同时存在，所以不能直接用传统的交叉熵 (cross-entropy) 损失，一种替代方案是用间隔损失 (margin loss) 其中， k 是分类 Tk 是分类的指示函数 (k 类存在为 1，不存在为 0) m+ 为上界，惩罚假阳性 (false positive) ，即预测 k 类存在但真实不存在，识别出来但错了 m- 为下界，惩罚假阴性 (false negative) ，即预测 k 类不存在但真实存在，没识别出来 λ 是比例系数，调整两者比重 总的损失是各个样例损失之和。论文中 m+= 0.9, m-= 0.1, λ = 0.5，用大白话说就是 如果 k 类存在，||vk|| 不会小于 0.9 如果 k 类不存在，||vk|| 不会大于 0.1 惩罚假阳性的重要性大概是惩罚假阴性的重要性的 2 倍 # 参考文献[1] 王圣元‏.看完这篇，别说你还不懂Hinton大神的胶囊网络[DB/OL]. http://www.sohu.com/a/226611009_633698, 2018-08-24.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>胶囊网络</tag>
        <tag>Capsule Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beginning Application Development with TensorFlow and Keras（路易斯卡佩罗）]]></title>
    <url>%2F2018%2F08%2F09%2FBeginning_Application_Development_with_TensorFlow_and_Keras%2F</url>
    <content type="text"><![CDATA[阅读和下载地址PDF 书籍配套代码GitHub 代码整理Jupyter nbviewer 购买地址《Beginning Application Development with TensorFlow and Keras》| Luis Capelo | May 2018 | Packt 读书笔记 前言这本书是你的指南将TensorFlow和Keras模型部署到实际应用程序中。 本书首先介绍了如何构建应用程序的专用蓝图产生预测。每个后续课程都会解决一个问题模型的类型，例如神经网络，配置深度学习环境，使用Keras并着重于三个重要问题：该模型如何工作，如何提高我们的预测准确性，以及如何使用它来衡量和评估其性能现实世界的应用程序。在本书中，您将学习如何创建生成的应用程序来自深度学习的预测。这个学习之旅从探索开始神经网络的共同组成部分及其必要条件性能。在课程结束时，您将探索训练有素的神经使用TensorFlow创建的网络。在剩下的课程中，你会学习构建一个包含不同组件的深度学习模型并测量他们在预测中的表现。最后，我们将能够部署一个有效的Web应用程序到本书结束时。 本书内容Lesson 1, Introduction to Neural Networks and Deep Learning, helps you set up and configure deep learning environment and start looking at individual models and case studies. It also discusses neural networks and its idea along with their origins and explores their power. Lesson 2, Model Architecture, shows how to predict Bitcoin prices using deep learning model. Lesson 3, Model Evaluation and Optimization, shows on how to evaluate a neural network model. We will modify the network’s hyperparameters to improve its performance. Lesson 4, Productization explains how to productize a deep learning model and also provides an exercise of how to deploy a model as a web application. Chapter 1. Introduction to Neural Networks and Deep LearningWhat are Neural Networks?Neural networks—also known as Artificial Neural Networks—were first proposed in the 40s by MIT professors Warren McCullough and Walter Pitts. For more information refer, Explained: Neural networks. MIT News Office, April 14, 2017. Available at: http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414. Successful ApplicationsTranslating text: In 2017, Google announced that it was releasing a new algorithm for its translation service called Transformer. The algorithm consisted of a recurrent neural network (LSTM) that is trained used bilingual text. Google showed that its algorithm had gained notable accuracy when comparing to industry standards (BLEU) and was also computationally efficient. Google Research Blog. Transformer: A Novel Neural Network Architecture for Language Understanding. August 31, 2017. Available at: https://research.googleblog.com/2017/08/transformer-novel-neural-network.html. Self-driving vehicles: Uber, NVIDIA, and Waymo are believed to be using deep learning models to control different vehicle functions that control driving. Alexis C. Madrigal: Inside Waymo’s Secret World for Training Se Driving Cars. The Atlantic. August 23, 2017. Available https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/“&gt;lities/537648/.NVIDIA: End-to-End Deep Learning for Self-Driving Cars. Augu 17, 2016. Available https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/.Dave Gershgorn: Uber’s new AI team is looking for the shorte route to self-driving cars. Quartz. December 5, 2016. Available https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/. Image recognition: Facebook and Google use deep learning models to identify entities in images and automatically tag these entities as persons from a set of contacts. Why Do Neural Networks Work So Well?Neural networks are powerful because they can be used to predict any given function with reasonable approximation. If one is able to represent a problem as a mathematical function and also has data that represents that function correctly, then a deep learning model can, in principle—and given enough resources—be able to approximate that function. This is typically called the universality principle of neural networks. For more information refer, Michael Nielsen: Neural Networks and Deep Learning: A visual proof that neural nets can compute any function. Available at: http://neuralnetworksanddeeplearning.com/chap4.html. Representation Learningneural networks are computation graphs in which each step computes higher abstraction representations from input data. Each one of these steps represents a progression into a different abstraction layer. Data progresses through these layers, building continuously higher-level representations. The process finishes with the highest representation possible: the one the model is trying to predict. Function ApproximationWhen neural networks learn new representations of data, they do so by combining weights and biases with neurons from different layers. However, there are many reasons why a neural network may not be able to predict a function with perfection, chief among them being that: Many functions contain stochastic properties (that is, random properties) There may be overfitting to peculiarities from the training data There may be a lack of training data Limitations of Deep LearningDeep learning techniques are best suited to problems that can be defined with formal mathematical rules (that is, as data representations). If a problem is hard to define this way, then it is likely that deep learning will not provide a useful solution. Remember that deep learning algorithms are learning different representations of data to approximate a given function. If data does not represent a function appropriately, it is likely that a function will be incorrectly represented by a neural network. To avoid this problem, make sure that the data used to train a model represents the problem the model is trying to address as accurately as possible. Inherent Bias and Ethical Considerations Researchers have suggested that the use of the deep learning model without considering the inherent bias in the training data can lead not only to poor performing solutions, but also to ethical complications. Common Components and Operations of Neural NetworksNeural networks have two key components: layers and nodes.Nodes are responsible for specific operations, and layers are groups of nodes used to differentiate different stages of the system.节点负责特定的操作，而层是用来区分系统不同阶段的节点组。 Nodes are where data is represented in the network. There are two values associated with nodes: biases and weights. Both of these values affect how data is represented by the nodes and passed on to other nodes. Unfortunately, there isn’t a clear rule for determining how many layers or nodes a network should have. Configuring a Deep Learning EnvironmentActivity 1 – Verifying Software Components 函数名 作用 启发 __separator 打印规整的分隔符 test_python 测试 Python 版本是否符合要求 test_tensorflow 测试 TensorFlow 版本是否符合要求 测试其它第三方库时也可以用此方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def __separator(c): """ Prints a pretty separator. Parameters ---------- c: str Character to use. """ print(c * 65)def test_python(): """ Tests if Python 3 is installed. """ message = None if sys.version_info[0] == 3: success = True log = """ PASS: Python 3.0 (or higher) is installed. """ else: success = False log = """ FAIL: Python 3.0 (or higher) not detected. """ message = """ https://www.python.org/downloads/ """ print(log) if message: print(message) __separator('~') return successdef test_tensorflow(): """ Tests if TensorFlow is installed. """ message = None try: import tensorflow if tensorflow.__version__ &gt;= '1.4.0': success = True log = """ PASS: TensorFlow 1.4.0 (or higher) is installed. """ else: success = False log = """ FAIL: TensorFlow 1.4.0 (or higher) not detected. """ message = """ https://www.tensorflow.org/install/ """ except ModuleNotFoundError: success = False log = """ FAIL: TensorFlow 1.4.0 (or higher) not detected. """ message = """ https://www.tensorflow.org/install/ """ print(log) if message: print(message) __separator('~') return success Activity_2_mnistmnist-demo Chapter 2. Model ArchitectureOlder architectures have been used to solve a large array of problems and are generally considered the right choice when starting a new project. Newer architectures have shown great successes in specific problems, but are harder to generalize. The latter are interesting as references of what to explore next, but are hardly a good choice when starting a project. Choosing the Right Model Architecture Table 1: Different neural network architectures have shown success indifferent fields. The networks’ architecture is typically related to the Data NormalizationBefore building a deep learning model, one more step is necessary: data normalization. Data normalization is a common practice in machine learning systems. Particularly regarding neural networks, researchers have proposed that normalization is an essential technique for training RNNs (and LSTMs), mainly because it decreases the network’s training time and increases the network’s overall performance. For more information refer, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift by Sergey Ioffe et. al., arXiv, March 2015. Available at: https://arxiv.org/abs/1502.03167. Z-scoreWhen data is normally distributed (that is, Gaussian), one can compute the distance between each observation as a standard deviation from its mean. This normalization is useful when identifying how distant data points are from more likely occurrences in the distribution. The Z-score is defined by: Z_i=\dfrac{x_i-\mu}{\sigma}Here, $x_i$ is the $i^{th}$ observation, $\mu$ is the mean, and $\sigma$ is the stand deviation of the series. Point-Relative NormalizationThis normalization computes the difference of a given observation in relation to the first observation of the series. This kind of normalization is useful to identify trends in relation to a starting point. The point-relative normalization is defined by: n_i=(\dfrac{O_i}{O_o})-1Here, $O_i$ is the $i^{th}$ observation, $O_o$ is the first observation of the series. Maximum and Minimum NormalizationThis normalization computes the distance between a given observation and the maximum and minimum values of the series. This normalization is useful when working with series in which the maximum and minimum values are not outliers and are important for future predictions. This normalization technique can be applied with: n_i=\dfrac{O_i - min(O}{max(O)-min(O}Here, $O_i$ is the $i^{th}$ observation,ation, $O$ represents a vector with all $O$ values, and the functions $min(O)$ and $max(O)$ represent the minimum and maximum values of the series, respectively. Structuring Your ProblemCompared to researchers, practitioners spend much less time determining which architecture to choose when starting a new deep learning project. Acquiring data that represents a given problem correctly is the most important factor to consider when developing these systems, followed by the understanding of the dataset’s inherent biases and limitations. Figure 5: Decision-tree of key reflection questions to be made at the beginning of a deep learning project Activity 3 – Exploring the Bitcoin Dataset and Preparing Data for Model1bitcoin = pd.read_csv('data/bitcoin_historical_prices.csv') Our dataset contains 7 variables (i.e. columns). Here’s what each one of them represents: date: date of the observation. iso_week: week number of a given year. open: open value of a single Bitcoin coin. high: highest value achieved during a given day period. low: lowest value achieved during a given day period. close: value at the close of the transaction day. volume: what is the total volume of Bitcoin that was exchanged during that day. market_capitalization: as described in CoinMarketCap’s FAQ page, this is calculated by Market Cap = Price X Circulating Supply. All values are in USD. ExplorationWe will now explore the dataset timeseries to understand its patterns. Let’s first explore two variables: close price and volume. Volume only contains data starting in November 2013, while close prices start earlier in April of that year. However, both show similar spiking patterns starting at the beginning of 2017. 1bitcoin.set_index('date')['close'].plot(linewidth=2, figsize=(14, 4), color='#d35400') &lt;matplotlib.axes._subplots.AxesSubplot at 0x8869048&gt; Now let’s explore the yera of 2017 only. This is the year where the price of bitcoin has risen significantly. 12bitcoin[bitcoin['date'] &gt;= '2017-01-01'].set_index('date')['close'].plot( linewidth=2, figsize=(14, 4), color='#d35400') &lt;matplotlib.axes._subplots.AxesSubplot at 0x8b50048&gt; Preparing Dataset for ModelNeural networks typically work with either matrices) or tensors. Our data needs to fit that structure before it can be used by either keras (or tensorflow). Also, it is common practice to normalize data before using it to train a neural network. We will be using a normalization technique the evaluates each observation into a range between 0 and 1 in relation to the first observation in each week. 1bitcoin.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date iso_week open high low close volume market_capitalization 0 2013-04-28 2013-17 135.30 135.98 132.10 134.21 NaN 1.500520e+09 1 2013-04-29 2013-17 134.44 147.49 134.00 144.54 NaN 1.491160e+09 2 2013-04-30 2013-17 144.00 146.93 134.05 139.00 NaN 1.597780e+09 3 2013-05-01 2013-17 139.00 139.89 107.72 116.99 NaN 1.542820e+09 4 2013-05-02 2013-17 116.38 125.60 92.28 105.21 NaN 1.292190e+09 First, let’s remove data from older periods. We will keep only data from 2016 until the latest observation of 2017. Older observations may be useful to understand current prices. However, Bitcoin has gained so much popularity in recent years that including older data would require a more laborious treatment. We will leave that for a future exploration. 1bitcoin_recent = bitcoin[bitcoin['date'] &gt;= '2016-01-01'] Let’s keep only the close and volume variables. We can use the other variables in another time. 1bitcoin_recent = bitcoin_recent[['date', 'iso_week', 'close', 'volume']] Now, let’s normalize our data for both the close and volume variables. 12bitcoin_recent['close_point_relative_normalization'] = bitcoin_recent.groupby('iso_week')['close'].apply( lambda x: normalizations.point_relative_normalization(x)) 12bitcoin_recent.set_index('date')['close_point_relative_normalization'].plot( linewidth=2, figsize=(14, 4), color='#d35400') &lt;matplotlib.axes._subplots.AxesSubplot at 0xb81d160&gt; Using Keras as a TensorFlow InterfaceKeras simplifies the interface for working with different architectures by using three components - network architecture, fit, and predict: Figure 15: The Keras neural network paradigm: A. design a neural network architecture, B. Train a neural network (or Fit), and C. Make predictions Activity 4 – Creating a TensorFlow Model Using Keras1234567891011121314# build_modelmodel = Sequential()model.add(LSTM( units=period_length, batch_input_shape=(batch_size, number_of_periods, period_length), input_shape=(number_of_periods, period_length), return_sequences=False, stateful=False))model.add(Dense(units=period_length))model.add(Activation("linear"))model.compile(loss="mse", optimizer="rmsprop")## saving modelmodel.save('bitcoin_lstm_v0.h5') Activity 5 – Assembling a Deep Learning System123456789# Shaping DataX_train = data[:-1,:].reshape(1, 76, 7)Y_validation = data[-1].reshape(1, 7)# Load Modelmodel = load_model('bitcoin_lstm_v0.h5')# Make Predictions%%timehistory = model.fit(x=X_train, y=Y_validation, batch_size=32, epochs=100) Chapter 3. Model Evaluation and OptimizationParameter and HyperparameterParameters are properties that affect how a model makes predictions from data. Hyperparameters refer to how a model learns from data. Parameters can be learned from the data and modified dynamically. Hyperparameters are higher-level properties and are not typically learned from data. Table 1: Common loss functions used for classification and regression problems We learned that loss functions are key elements of neural networks, as they evaluate the performance of a network at each epoch and are the starting point for the propagation of adjustments back into layers and nodes. We also explored why some loss functions can be difficult to interpret (for instance, the MSE) and developed a strategy using two other functions—RMSE and MAPE—to interpret the predicted results from our LSTM model. Activity 6 – Creating an Active Training EnvironmentLayers and Nodes - Adding More Layersthe more layers you add, the more hyperparameters you have to tune—and the longer your network will take to train. If your model is performing fairly well and not overfitting your data, experiment with the other strategies outlined in this lesson before adding new layers to your network. Epochshe larger the date used to train your model, the more epochs it will need to achieve good performance. Activation Functions Understanding Activation Functions in Neural Networks by Avinash Sharma V, available at: https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0. L2 RegularizationL2 regularization (or weight decay) is a common technique for dealing with overfitting models. In some models, certain parameters vary in great magnitudes. The L2 regularization penalizes such parameters, reducing the effect of these parameters on the network. DropoutDropout is a regularization technique based on a simple question: if one randomly takes away a proportion of nodes from layers, how will the other node adapt? It turns out that the remaining neurons adapt, learning to represent patterns that were previously handled by those neurons that are missing. Activity 7: Optimizing a deep learning model12345678910111213141516171819202122232425262728293031323334# 使用 tensorboard 辅助训练的函数def train_model(model, X, Y, epochs=100, version=0, run_number=0): """ Shorthand function for training a new model. This function names each run of the model using the TensorBoard naming conventions. Parameters ---------- model: Keras model instance Compiled Keras model. X, Y: np.array Series of observations to be used in the training process. version: int Version of the model to run. run_number: int The number of the run. Used in case the same model version is run again. """ hash = random.getrandbits(128) hex_code = '%032x' % hash model_name = 'bitcoin_lstm_v&#123;version&#125;_run_&#123;run_number&#125;_&#123;hex_code&#125;'.format( version=version, run_number=run_number, hex_code=hex_code[:6]) tensorboard = TensorBoard(log_dir='./logs/&#123;&#125;'.format(model_name)) model_history = model.fit( x=X, y=Y, batch_size=1, epochs=epochs, verbose=0, callbacks=[tensorboard], shuffle=False) return model_history Chapter 4. ProductizationThis lesson focuses on how to productize a deep learning model. We use the word productize to define the creation of a software product from a deep learning model that can be used by other people and applications.We are interested in models that use new data when it becomes available, continuously learning patterns from new data and, consequently, making better predictions. We study two strategies to deal with new data: one that re-trains an existing model, and another that creates a completely new model. Then, we implement the latter strategy in our Bitcoin prices prediction model so that it can continuously predict new Bitcoin prices. Figure 1: System architecture for the web application built in this project Handling New DataModels can be trained once in a set of data and can then be used to make predictions. Such static models can be very useful, but it is often the case that we want our model to continuously learn from new data—and to continuously get better as it does so.In this section, we will discuss two strategies on how to re-train a deep learning model and how to implement them in Python. Separating Data and ModelWhen building a deep learning application, the two most important areas are data and model. From an architectural point of view, we suggest that these two areas be separate. We believe that is a good suggestion because each of these areas include functions inherently separated from each other. Data is often required to be collected, cleaned, organized, and normalized; and models need to be trained, evaluated, and able to make predictions. Both of these areas are dependent, but are better dealt with separately.As a matter of following that suggestion, we will be using two classes to help us build our web application: CoinMarketCap() and Model(): CoinMarketCap(): This is a class designed for fetching Bitcoin prices from the following website: http://www.coinmarketcap.com. This is the same place where our original Bitcoin data comes from. This class makes it easy to retrieve that data on a regular schedule, returning a Pandas DataFrame with the parsed records and all available historical data. Model(): This class implements all the code we have written so far into a single class. That class provides facilities for interacting with our previously trained models, and also allows for the making of predictions using de-normalized data—which is much easier to understand. The Model() class is our model component. These two classes are used extensively throughout our example application and define the data and model components. Activity 8: Re-training a model dynamicallyAchievements]]></content>
      <categories>
        <category>深度学习</category>
        <category>Beginning Application Development with TensorFlow and Keras（路易斯卡佩罗）</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>书籍</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文资源]]></title>
    <url>%2F2018%2F08%2F07%2F%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[论文实现谷歌全attention机器翻译模型Transformer的TensorFlow实现 强化学习Deepmind | AlphaGo Zero: Learning from scratch | 2017 论文翻译 自然语言处理NLP 研究灵感库 GitHub | 进展 | NLP-progress跟踪自然语言处理进展，包括数据集和最常见的NLP任务的当前最先进的状态。机器之心 | 翻译 | NLP-progress Sentiance | 2018-05-03 | Loc2Vec: Learning location embeddings with triplet-loss networks 机器之心 | 翻译 | 使用三重损失网络学习位置嵌入：让位置数据也能进行算术运算 以自监督的方式学习位置数据并从中提取见解。Sentiance 开发了一款能接收加速度计、陀螺仪和位置信息等智能手机传感器数据并从中提取出行为见解的平台。该平台能学习用户的模式，并能预测和解释事情发生的原因和时间。 2018-07-09-9 | NLP领域的ImageNet时代到来：词嵌入「已死」，语言模型当立 计算机视觉领域常使用在 ImageNet 上预训练的模型，它们可以进一步用于目标检测、语义分割等不同的 CV 任务。而在自然语言处理领域中，我们通常只会使用预训练词嵌入向量编码词汇间的关系，因此也就没有一个能用于整体模型的预训练方法。Sebastian Ruder 表示语言模型有作为整体预训练模型的潜质，它能由浅到深抽取语言的各种特征，并用于机器翻译、问答系统和自动摘要等广泛的 NLP 任务。Ruder 同样展示了用语言模型做预训练模型的效果，并表示 NLP 领域中的「ImageNet」终要到来。 达观数据 | 2018-07-25 | NLP概述和文本自动分类算法详解 图像处理知乎 | 进展 | 深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读 2018-07-10-6 | OpenAI提出可逆生成模型Glow 目前，生成对抗网络 GAN 被认为是在图像生成等任务上最为有效的方法，越来越多的学者正朝着这一方向努力：在计算机视觉顶会 CVPR 2018 上甚至有 8% 的论文标题中包含 GAN。近日来自 OpenAI 的研究科学家 Diederik Kingma 与 Prafulla Dhariwal 却另辟蹊径，提出了基于流的生成模型 Glow。据介绍，该模型不同于 GAN 与 VAE，而在生成图像任务上也达到了令人惊艳的效果。 知识图谱2018-01-25 | DKN：深度知识网络新闻推荐在线新闻推荐系统旨在解决新闻的信息爆炸并为用户提供个性化推荐。一般来说，新闻语言高度浓缩，充满知识实体和常识。然而，现有方法不知道这种外部知识，并且不能完全发现新闻之间的潜在知识级连接。因此，用户的推荐结果仅限于简单模式，无法合理扩展。此外，新闻推荐还面临着新闻时间敏感度高，用户兴趣多样化的挑战。为了解决上述问题，本文提出了一种深度知识感知网络（DKN），它将知识图谱表示结合到新闻推荐中。&gt;]]></content>
      <categories>
        <category>论文</category>
        <category>论文资源</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hands-on Machine Learning 课后习题解答]]></title>
    <url>%2F2018%2F08%2F01%2FHands-on%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow%20%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94%2F</url>
    <content type="text"><![CDATA[本文是 《Hands-on Machine Learning with Scikit-Learn and TensorFlow 》的课后习题解答！ 把练习和答案分开是为了督促学习和思考，提倡独立思考，主动学习，而不是背答案！文章前半段中文翻译有问题的地方欢迎评论指正，英文原文在后面。 CHAPTER 1 The Machine Learning Landscape练习本章中，我们学习了一些机器学习中最为重要的概念。下一章，我们会更加深入，并写一些代码。开始下章之前，确保你能回答下面的问题： 如何定义机器学习？ 机器学习可以解决的四类问题？ 什么是带标签的训练集？ 最常见的两个监督任务是什么？ 指出四个常见的非监督任务？ 要让一个机器人能在各种未知地形行走，你会采用什么机器学习算法？ 要对你的顾客进行分组，你会采用哪类算法？ 垃圾邮件检测是监督学习问题，还是非监督学习问题？ 什么是在线学习系统？ 什么是核外学习？ 什么学习算法是用相似度做预测？ 模型参数和学习算法的超参数的区别是什么？ 基于模型学习的算法搜寻的是什么？最成功的策略是什么？基于模型学习如何做预测？ 机器学习主要的挑战是什么？ 如果模型在训练集上表现好，但推广到新实例表现差，问题是什么？给出三个可能的解决方案。 什么是测试集，为什么要使用它？ 验证集的目的是什么？ 如果用测试集调节超参数，会发生什么？ 什么是交叉验证，为什么它比验证集好？ 练习解答 机器学习是关于构建可以从数据中学习的模型。学习意味着在某些任务中，根据一些绩效衡量，模型可以更好。 机器学习非常适用于： 我们没有算法解决方案的复杂问题； 可以替换手工调整规则的长列表； 构建适应波动环境的系统； 最后帮助人类学习（例如，数据挖掘） 。提示：不要把所以问题往机器学习上套，比如做网页，比如已经有高效算法的（图联通判断）。 标记的训练集是一个训练集，其中包含每个实例的所需解决方案（例如标签）。 两个最常见的监督任务是回归和分类。 常见的无监督任务包括聚类，可视化，降维和关联规则学习。 强化学习如果我们希望机器人学会在各种未知的地形中行走，那么学习可能会表现得最好，因为这通常是强化学习所解决的问题类型。有可能将问题表达为监督或半监督学习问题，但这种解决方式不太自然。 如果您不知道如何定义组，则可以使用聚类算法（无监督学习）将客户划分为类似客户的集群。但是，如果您知道您希望拥有哪些组，那么您可以将每个组的许多示例提供给分类算法（监督学习），并将所有客户分类到这些组中。 垃圾邮件检测是一种典型的监督学习问题：算法会输入许多电子邮件及其标签（垃圾邮件或非垃圾邮件）。 在线学习系统可以逐步学习，而不是批量学习系统。这使它能够快速适应不断变化的数据和自治系统，以及对大量数据的培训。 核外算法可以处理大量无法容纳在计算机主存中的数据。核心学习算法将数据分成小批量，并使用在线学习技术从这些小批量中学习。 基于实例的学习系统用心学习训练数据;然后，当给定一个新实例时，它使用相似性度量来查找最相似的学习实例并使用它们进行预测。 模型具有一个或多个模型参数，其确定在给定新实例的情况下它将预测什么（例如，线性模型的斜率）。学习算法试图找到这些参数的最佳值，以便模型很好地推广到新实例。超参数是学习算法本身的参数，而不是模型的参数（例如，要应用的正则化的量）。 基于模型的学习算法搜索模型参数的最佳值，使得模型将很好地推广到新实例。我们通常通过最小化成本函数来训练这样的系统，该成本函数测量系统在对训练数据进行预测时的糟糕程度，以及如果模型正规化则对模型复杂性的惩罚。为了进行预测，我们使用学习算法找到的参数值将新实例的特征提供给模型的预测函数。 机器学习中的一些主要挑战是缺乏数据，数据质量差，非代表性数据，无法提供信息的特征，过于简单的模型以及过度拟合训练数据的模型，以及过度复杂的模型过度拟合数据。 如果一个模型在训练数据上表现很好，但对新实例表现不佳，那么该模型可能会过度拟合训练数据（或者我们对训练数据非常幸运）。过度拟合的可能解决方案是获得更多数据，简化模型（选择更简单的算法，减少所使用的参数或特征的数量，或使模型正规化），或减少训练数据中的噪声。 测试集用于估计模型在生产中启动之前模型将对新实例进行的泛化错误。 验证集用于比较模型。它可以选择最佳模型并调整超参数。 如果使用测试集调整超参数，则存在过度拟合测试集的风险，并且您测量的泛化错误将是乐观的（您可能会启动比预期更差的模型）。 交叉验证是一种技术，可以比较模型（模型选择和超参数调整），而无需单独的验证集。这节省了宝贵的培训数据。 ExercisesIn this chapter we have covered some of the most important concepts in Machine Learning. In the next chapters we will dive deeper and write more code, but before we do, make sure you know how to answer the following questions: How would you define Machine Learning? Can you name four types of problems where it shines? What is a labeled training set? What are the two most common supervised tasks? Can you name four common unsupervised tasks? What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? What type of algorithm would you use to segment your customers into multiple groups? Would you frame the problem of spam detection as a supervised learning prob‐lem or an unsupervised learning problem? What is an online learning system? What is out-of-core learning? What type of learning algorithm relies on a similarity measure to make predic‐tions? What is the difference between a model parameter and a learning algorithm’s hyperparameter? What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions? Can you name four of the main challenges in Machine Learning? If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions? What is a test set and why would you want to use it? What is the purpose of a validation set? What can go wrong if you tune hyperparameters using the test set? What is cross-validation and why would you prefer it to a validation set? Exercise Solutions Machine Learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure. Machine Learning is great for complex problems for which we have no algorith‐mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. The two most common supervised tasks are regression and classification. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning. Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to express the problem as a supervised or semisupervised learning problem, but it would be less natural. If you don’t know how to define the groups, then you can use a clustering algo‐rithm (unsupervised learning) to segment your customers into clusters of similar customers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups. Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam). An online learning system can learn incrementally, as opposed to a batch learn‐ing system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data. Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer’s main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini-batches. An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned instances and uses them to make predictions. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the sys‐tem is at making predictions on the training data, plus a penalty for model com‐plexity if the model is regularized. To make predictions, we feed the new instance’s features into the model’s prediction function, using the parameter val‐ues found by the learning algorithm. Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple mod‐els that underfit the training data, and excessively complex models that overfit the data. If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters. If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect). Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate vali‐dation set. This saves precious training data. Chapter 13: Convolutional Neural Networks练习 CNN相对于完全连接的DNN有什么优势可用于图像分类？ 考虑由三个卷积层组成的CNN，每个卷积层具有3×3个内核，步长为2，以及SAME填充。最下层输出100个特征图，中间一个输出200，顶部输出400.输入图像是200×300像素的RGB图像。 CNN中的参数总数是多少？如果我们使用32位浮点数，那么在对单个实例进行预测时，该网络至少需要多少RAM？什么时候对50个图像的小批量培训？ 如果您的GPU在训练CNN时内存不足，您可以尝试解决问题的五件事情是什么？ 为什么要添加最大池化层而不是具有相同步幅的卷积层？ 您希望何时添加本地响应规范化层？ 与LeNet-5相比，您能说出AlexNet的主要创新吗？ GoogLeNet和ResNet的主要创新如何？ 练习解答1.这是CNN相对于完全连接的DNN进行图像分类的主要优点： 因为连续的层只是部分连接，并且因为它重复使用其权重，所以CNN的参数比完全连接的DNN少得多，这使得训练速度更快，降低了过度拟合的风险，并且需要的训练数据要少得多。 当CNN学习了可以检测特定功能的内核时，它可以在图像的任何位置检测到该功能。相反，当DNN在一个位置学习一个特征时，它只能在该特定位置检测到它。由于图像通常具有非常重复的特征，因此使用较少的训练示例，CNN能够比DNN更好地用于图像处理任务（例如分类）。 最后，DNN没有关于如何组织像素的先验知识;它不知道附近的像素是否接近。 CNN的架构嵌入了这一先验知识。较低层通常识别图像的小区域中的特征，而较高层将较低层特征组合成较大特征。这适用于大多数自然图像，使CNN与DNN相比具有决定性的先机性。 让我们计算CNN有多少参数。由于其第一个卷积层具有3×3个内核，并且输入具有三个通道（红色，绿色和蓝色），因此每个特征图具有3×3×3个权重，加上偏置项。这是每个功能图的28个参数。由于该第一卷积层具有100个特征映射，因此它具有总共2,800个参数。第二卷积层具有3×3个核，其输入是前一层的100个特征映射的集合，因此每个特征映射具有3×3×100 = 900个权重，加上偏差项。由于它有200个特征图，因此该层具有901×200 = 180,200个参数。最后，第三个和最后一个卷积层也有3×3个核，其输入是前一个层的200个特征映射的集合，因此每个特征映射具有3×3×200 = 1,800个权重，加上一个偏置项。由于它有400个特征图，因此该图层总共有1,801×400 = 720,400个参数。总而言之，CNN有2,800 + 180,200 + 720,400 = 903,400个参数。现在让我们计算这个神经网络在对单个实例进行预测时需要多少RAM（至少）。首先让我们计算每一层的特征图大小。由于我们使用2和SAME填充的步幅，因此要素图的水平和垂直尺寸在每一层被除以2（必要时向上舍入），因此输入通道为200×300像素，第一层的特征地图是100×150，第二层的特征地图是50×75，第三层的特征地图是25×38。因为32位是4个字节而第一个卷积层有100个特征地图，所以第一层需要4 x 100×150×100 = 600万字节（约5.7 MB，考虑到1 MB = 1,024 KB和1 KB = 1,024字节）。第二层占用4×50×75×200 = 300万字节（约2.9MB）。最后，第三层占用4×25×38×400 = 1,520,000字节（约1.4MB）。但是，一旦计算了一个层，就可以释放前一层占用的内存，因此如果一切都经过优化，只需要6 + 9 = 1500万字节（约14.3 MB）的RAM（第二层时）刚刚计算过，但第一层占用的内存尚未释放）。但是等等，你还需要添加CNN参数占用的内存。我们之前计算过它有903,400个参数，每个参数使用4个字节，所以这增加了3,613,600个字节（大约3.4 MB）。所需的总RAM是（至少）18,613,600字节（约17.8 MB）。最后，让我们计算在50个图像的小批量训练CNN时所需的最小RAM量。在训练期间，TensorFlow使用反向传播，这需要保留在前向传递期间计算的所有值，直到反向传递开始。因此，我们必须计算单个实例的所有层所需的总RAM，并将其乘以50。那时让我们开始以兆字节而不是字节计数。我们之前计算过，每个实例的三层分别需要5.7,2.9和1.4 MB。每个实例总共10.0 MB。因此，对于50个实例，总RAM为500 MB。再加上输入图像所需的RAM，即50×4×200×300×3 = 36百万字节（约34.3 MB），加上模型参数所需的RAM，大约3.4 MB（之前计算过）加上一些用于渐变的RAM（我们将忽略它们，因为它们可以逐渐释放，因为反向传播在反向传递过程中向下传播）。我们总共大约500.0 + 34.3 + 3.4 = 537.7 MB。这真的是一个乐观的最低限度。 如果您的GPU在训练CNN时内存不足，可以尝试解决问题的五件事情（除了购买具有更多RAM的GPU）： 减少小批量。 在一个或多个图层中使用更大的步幅减少维度。 删除一个或多个图层。 使用16位浮点数而不是32位浮点数。 在多个设备上分发CNN。 最大池层根本没有参数，而卷积层有很多参数（参见前面的问题）。 局部响应归一化层使得最强烈激活的神经元在相同位置但在相邻特征图中抑制神经元，这促使不同的特征图专门化并将它们分开，迫使它们探索更广泛的特征。 它通常在较低层中使用，以具有较大的低级特征池，上层可以构建在其上。 与LeNet-5相比，AlexNet的主要创新是：（1）它更大更深，（2）它将卷积层直接叠加在一起，而不是在每个卷积层的顶部堆叠汇集层。 GoogLeNet的主要创新是引入了初始模块，这使得有可能拥有比以前的CNN架构更深的网络，参数更少。 最后，ResNet的主要创新是跳过连接的引入，这使得它可以超越100层。 可以说，它的简洁性和一致性也相当具有创新性。 Exercises What are the advantages of a CNN over a fully connected DNN for image classi‐fication? Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN?If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images? If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem? Why would you want to add a max pooling layer rather than a convolutional layer with the same stride? When would you want to add a local response normalization layer? Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet and ResNet? Exercise Solutions These are the main advantages of a CNN over a fully connected DNN for image classification: Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data. When a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples. Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN’s architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start com‐pared to DNNs. Let’s compute how many parameters the CNN has. Since its first convolutional layer has 3 × 3 kernels, and the input has three channels (red, green, and blue), then each feature map has 3 × 3 × 3 weights, plus a bias term. That’s 28 parame‐ters per feature map. Since this first convolutional layer has 100 feature maps, it has a total of 2,800 parameters. The second convolutional layer has 3 × 3 kernels, and its input is the set of 100 feature maps of the previous layer, so each feature map has 3 × 3 × 100 = 900 weights, plus a bias term. Since it has 200 feature maps, this layer has 901 × 200 = 180,200 parameters. Finally, the third and last convolutional layer also has 3 × 3 kernels, and its input is the set of 200 feature maps of the previous layers, so each feature map has 3 × 3 × 200 = 1,800 weights, plus a bias term. Since it has 400 feature maps, this layer has a total of 1,801 × 400 = 720,400 parameters. All in all, the CNN has 2,800 + 180,200 + 720,400 = 903,400 parameters.Now let’s compute how much RAM this neural network will require (at least) when making a prediction for a single instance. First let’s compute the feature map size for each layer. Since we are using a stride of 2 and SAME padding, the horizontal and vertical size of the feature maps are divided by 2 at each layer (rounding up if necessary), so as the input channels are 200 × 300 pixels, the first layer’s feature maps are 100 × 150, the second layer’s feature maps are 50 × 75, and the third layer’s feature maps are 25 × 38. Since 32 bits is 4 bytes and the first convolutional layer has 100 feature maps, this first layer takes up 4 x 100 × 150 × 100 = 6 million bytes (about 5.7 MB, considering that 1 MB = 1,024 KB and 1 KB = 1,024 bytes). The second layer takes up 4 × 50 × 75 × 200 = 3 million bytes (about 2.9 MB). Finally, the third layer takes up 4 × 25 × 38 × 400 = 1,520,000 bytes (about 1.4 MB). However, once a layer has been computed, the memory occupied by the previous layer can be released, so if everything is well optimized, only 6 + 9 = 15 million bytes (about 14.3 MB) of RAM will be required (when the second layer has just been computed, but the memory occupied by the first layer is not released yet). But wait, you also need to add the memory occupied by the CNN’s parameters. We computed earlier that it has 903,400 parameters, each using up 4 bytes, so this adds 3,613,600 bytes (about 3.4 MB). The total RAM required is (at least) 18,613,600 bytes (about 17.8 MB).Lastly, let’s compute the minimum amount of RAM required when training the CNN on a mini-batch of 50 images. During training TensorFlow uses backpropa‐gation, which requires keeping all values computed during the forward pass until the reverse pass begins. So we must compute the total RAM required by all layers for a single instance and multiply that by 50! At that point let’s start counting in megabytes rather than bytes. We computed before that the three layers require respectively 5.7, 2.9, and 1.4 MB for each instance. That’s a total of 10.0 MB per instance. So for 50 instances the total RAM is 500 MB. Add to that the RAM required by the input images, which is 50 × 4 × 200 × 300 × 3 = 36 million bytes (about 34.3 MB), plus the RAM required for the model parameters, which is about 3.4 MB (computed earlier), plus some RAM for the gradients (we will neglect them since they can be released gradually as backpropagation goes down the layers during the reverse pass). We are up to a total of roughly 500.0 + 34.3 + 3.4 = 537.7 MB. And that’s really an optimistic bare minimum. If your GPU runs out of memory while training a CNN, here are five things you could try to solve the problem (other than purchasing a GPU with more RAM): Reduce the mini-batch size. Reduce dimensionality using a larger stride in one or more layers. Remove one or more layers. Use 16-bit floats instead of 32-bit floats. Distribute the CNN across multiple devices. A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few (see the previous questions). A local response normalization layer makes the neurons that most strongly acti‐vate inhibit neurons at the same location but in neighboring feature maps, which encourages different feature maps to specialize and pushes them apart, forcing them to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon. The main innovations in AlexNet compared to LeNet-5 are (1) it is much larger and deeper, and (2) it stacks convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. The main innovation in GoogLeNet is the introduction of inception modules, which make it possible to have a much deeper net than previous CNN architectures, with fewer parameters. Finally, ResNet’s main innovation is the introduction of skip connec‐tions, which make it possible to go well beyond 100 layers. Arguably, its simplic‐ity and consistency are also rather innovative. Chapter 14: Recurrent Neural Networks练习 你能想象 seq2seq RNN 的几个应用吗？ seq2vec 的 RNN 呢？vex2seq 的 RNN 呢？ 为什么人们使用编解码器 RNN 而不是简单的 seq2seq RNN 来自动翻译？ 如何将卷积神经网络与 RNN 结合，来对视频进行分类？ 使用 dynamic_rnn() 而不是 static_rnn() 构建 RNN 有什么好处？ 你如何处理长度可变的输入序列？ 那么长度可变输出序列呢？ 在多个 GPU 上分配深层 RNN 的训练和执行的常见方式是什么？ 练习解答 以下是一些RNN应用程序： 对于序列到序列的RNN：预测天气（或任何其他时间序列），机器翻译（使用编码器 - 解码器架构），视频字幕，语音到文本，音乐生成（或其他序列生成），识别 一首歌的和弦。 对于序列到矢量RNN：按音乐类型对音乐样本进行分类，分析书评的情绪，根据大脑植入物的读数预测失语症患者正在考虑的单词，预测概率 用户希望根据她的观看历史观看电影（这是协作过滤的许多可能实现之一）。 对于矢量到序列RNN：图像字幕，基于当前艺术家的嵌入创建音乐播放列表，基于一组参数生成旋律，在图片中定位行人。 一般来说，如果你一次翻译一个单词，结果将是非常可怕的。 例如，法语句子“Je vous en prie”的意思是“欢迎你”，但如果你一次翻译一个词，你会得到“我在祷告。”嗯？ 首先阅读整个句子然后翻译它会好得多。 普通的序列到序列RNN将在读取第一个字之后立即开始翻译句子，而编码器 - 解码器RNN将首先读取整个句子然后翻译它。 也就是说，人们可以想象一个简单的序列到序列的RNN，只要不确定接下来要说什么就会输出静音（就像人类翻译者必须翻译直播时那样）。 为了基于视觉内容对视频进行分类，一种可能的架构可以是（比方说）每秒一帧，然后通过卷积神经网络运行每一帧，将CNN的输出馈送到序列到矢量RNN ，最后通过softmax层运行其输出，为您提供所有类概率。 对于培训，您只需使用交叉熵作为成本函数。 如果您也想将音频用于分类，您可以将每秒音频转换为摄谱仪，将此摄谱仪输入CNN，并将此CNN的输出馈送到RNN（以及其他CNN的相应输出））。 使用dynamic_rnn（）而不是static_rnn（）构建RNN具有以下几个优点： 它基于while_loop（）操作，该操作能够在反向传播期间将GPU的内存交换到CPU的内存，从而避免内存不足错误。 它可以说更容易使用，因为它可以直接将单个张量作为输入和输出（涵盖所有时间步骤），而不是张量列表（每个时间步长一个）。 无需堆叠，取消堆叠或转置。 它生成一个较小的图形，更容易在TensorBoard中可视化。 5.要处理可变长度输入序列，最简单的选项是在调用static_rnn（）或dynamic_rnn（）函数时设置sequence_length参数。 另一种选择是填充较小的输入（例如，用零）以使它们与最大输入相同（如果输入序列都具有非常相似的长度，则这可能比第一选项快）。 要处理可变长度输出序列，如果事先知道每个输出序列的长度，可以使用sequence_length参数（例如，考虑序列到序列的RNN，用暴力标记视频中的每一帧 得分：输出序列与输入序列的长度完全相同）。 如果您事先不知道输出序列的长度，则可以使用填充技巧：始终输出相同大小的序列，但忽略序列结束标记之后的任何输出（通过在计算时忽略它们） 成本函数）。 要在多个GPU上分发深度RNN的训练和执行，常见的技术就是将每个层放在不同的GPU上（参见第12章）。 Exercises Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN? Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation? How could you combine a convolutional neural network with an RNN to classify videos? What are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()? How can you deal with variable-length input sequences? What about variable-length output sequences? What is a common way to distribute training and execution of a deep RNN across multiple GPUs? Exercise Solutions Here are a few RNN applications: For a sequence-to-sequence RNN: predicting the weather (or any other time series), machine translation (using an encoder–decoder architecture), video captioning, speech to text, music generation (or other sequence generation), identifying the chords of a song. For a sequence-to-vector RNN: classifying music samples by music genre, ana‐lyzing the sentiment of a book review, predicting what word an aphasic patient is thinking of based on readings from brain implants, predicting the probabil‐ity that a user will want to watch a movie based on her watch history (this is one of many possible implementations of collaborative filtering). For a vector-to-sequence RNN: image captioning, creating a music playlist based on an embedding of the current artist, generating a melody based on a set of parameters, locating pedestrians in a picture (e.g., a video frame from a self-driving car’s camera). In general, if you translate a sentence one word at a time, the result will be terri‐ble. For example, the French sentence “Je vous en prie” means “You are welcome,” but if you translate it one word at a time, you get “I you in pray.” Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an encoder–decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast). To classify videos based on the visual content, one possible architecture could be to take (say) one frame per second, then run each frame through a convolutional neural network, feed the output of the CNN to a sequence-to-vector RNN, and finally run its output through a softmax layer, giving you all the class probabili‐ties. For training you would just use cross entropy as the cost function. If you wanted to use the audio for classification as well, you could convert every second of audio to a spectrograph, feed this spectrograph to a CNN, and feed the output of this CNN to the RNN (along with the corresponding output of the other CNN). Building an RNN using dynamic_rnn() rather than static_rnn() offers several advantages: It is based on a while_loop() operation that is able to swap the GPU’s memory to the CPU’s memory during backpropagation, avoiding out-of-memory errors. It is arguably easier to use, as it can directly take a single tensor as input and output (covering all time steps), rather than a list of tensors (one per time step). No need to stack, unstack, or transpose. It generates a smaller graph, easier to visualize in TensorBoard. To handle variable length input sequences, the simplest option is to set the sequence_length parameter when calling the static_rnn() or dynamic_rnn() functions. Another option is to pad the smaller inputs (e.g., with zeros) to make them the same size as the largest input (this may be faster than the first option if the input sequences all have very similar lengths). To handle variable-length out‐put sequences, if you know in advance the length of each output sequence, you can use the sequence_length parameter (for example, consider a sequence-to-sequence RNN that labels every frame in a video with a violence score: the output sequence will be exactly the same length as the input sequence). If you don’t know in advance the length of the output sequence, you can use the padding trick: always output the same size sequence, but ignore any outputs that come after the end-of-sequence token (by ignoring them when computing the cost function). To distribute training and execution of a deep RNN across multiple GPUs, a common technique is simply to place each layer on a different GPU (see Chap‐ter 12). Chapter 15: Autoencoders练习 自动编码器使用的主要任务是什么？ 假设你想训练一个分类器，你有很多未标记的训练数据，但是只有几千个标记的实例。自动编码器如何帮助？你将如何进行？ 如果一个自动编码器完美地重建输入，它一定是好的吗？自动编码器？如何评价自动编码器的性能？ 什么是欠完备和过完备的自动编码器？过度完备的自动编码器的主要风险是什么？超完备自动编码器的主要风险是什么？ 如何在堆叠式自动编码器中系紧砝码？这样做有什么意义呢？ 什么是一种常见的技术来可视化的特点，学习下层的堆叠自动编码器？更高层怎么办？ 什么是生成模型？你能说出一种生成式自动编码器吗？ 练习解答 以下是自动编码器用于的一些主要任务： 特征提取 无人监督的预训练 维度降低 生成模型 异常检测（自动编码器通常不利于重建异常值） 如果你想训练一个分类器，你有大量的未标记的训练数据，但是只有几千个标记的实例，那么你可以首先在完整的数据集（标记和未标记）上训练一个深度的自动编码器，然后再将其下半部分用于分类器（即，重复使用编码层）。编码层，包括使用标记数据训练分类器。如果您的标记数据很少，则可能需要在训练分类器时冻结重复使用的层。 自动编码器完美地重建其输入的事实并不一定如此意味着它是一个很好的自动编码器;也许它只是一个过度完整的自动编码器学会了将其输入复制到编码层然后再输出到输出。实际上，即使编码层包含单个神经元，也是可能的对于一个非常深的自动编码器来学习将每个训练实例映射到不同的编码（例如，第一个实例可以映射到0.001，第二个实例可以映射到0.002，即第三到0.003，等等），它可以“用心”学习重建右边每个编码的训练实例。它将完美地重建其输入没有真正学习数据中任何有用的模式。在实践中这样的映射不太可能发生，但它说明了完美的重建不是这样的事实保证自动编码器学到了什么有用的东西。但是，如果它产生非常糟糕的重建，然后它几乎保证是一个糟糕的自动编码器。为了评估自动编码器的性能，一种选择是测量重建损失（例如，计算MSE，输出的均方值）减去输入）。再次，高重建损失是一个很好的迹象自动编码器很糟糕，但重建损失很小并不能保证好。您还应该根据它将使用的内容来评估自动编码器对于。例如，如果您将其用于无人监督的分类器预训练，那么你还应该评估分类器的性能。 欠完全自动编码器是一种编码层小于编码层的编码器输入和输出层。 如果它更大，那么它是一个过完备的自动编码器。欠完全自动编码器的主要风险是它可能无法完成重建输入。 过度完整的自动编码器的主要风险是它可能只是将输入复制到输出，而不学习任何有用的功能。 要将编码器层及其相应解码器层的权重联系起来简单地使解码器权重等于编码器权重的转置。这会将模型中的参数数量减少一半，通常会进行培训通过较少的训练数据更快地收敛，并降低过度拟合的风险训练集。 为了可视化由堆叠自动编码器的下层学习的特征，通常的技术是通过将每个权重向量重新整形为输入图像的大小来简单地绘制每个神经元的权重（例如，对于MNIST，重塑一个权重向量）。 形状[784]至[28,28]）。 为了可视化更高层学习的特征，一种技术是显示最能激活每个神经元的训练实例。 生成模型是能够随机生成类似于训练实例的输出的模型。 例如，一旦在MNIST数据集上成功训练，生成模型可用于随机生成数字的真实图像。 输出分布通常类似于训练数据。 例如，由于MNIST包含每个数字的许多图像，因此生成模型将输出大致相同数量的每个数字的图像。 一些生成模型可以参数化。例如，仅生成某种输出。 生成自动编码器的一个例子是变分自动编码器。 Exercise What are the main tasks that autoencoders are used for? Suppose you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances. How can autoencoders help? How would you proceed? If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder? What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder? How do you tie weights in a stacked autoencoder? What is the point of doing so? What is a common technique to visualize features learned by the lower layer of a stacked autoencoder? What about higher layers? What is a generative model? Can you name a type of generative autoencoder? Exercise Solutions Here are some of the main tasks that autoencoders are used for: Feature extraction Unsupervised pretraining Dimensionality reduction Generative models Anomaly detection (an autoencoder is generally bad at reconstructing outliers) If you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances, then you could first train a deep autoencoder on the full dataset (labeled + unlabeled), then reuse its lower half for the classifier (i.e., reuse the layers up to the codings layer, included) and train the classifier using the labeled data. If you have little labeled data, you probably want to freeze the reused layers when training the classifier. The fact that an autoencoder perfectly reconstructs its inputs does not necessarily mean that it is a good autoencoder; perhaps it is simply an overcomplete autoen‐coder that learned to copy its inputs to the codings layer and then to the outputs. In fact, even if the codings layer contained a single neuron, it would be possible for a very deep autoencoder to learn to map each training instance to a different coding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the third to 0.003, and so on), and it could learn “by heart” to reconstruct the right training instance for each coding. It would perfectly reconstruct its inputs without really learning any useful pattern in the data. In practice such a mapping is unlikely to happen, but it illustrates the fact that perfect reconstructions are not a guarantee that the autoencoder learned anything useful. However, if it produces very bad reconstructions, then it is almost guaranteed to be a bad autoencoder. To evaluate the performance of an autoencoder, one option is to measure the reconstruction loss (e.g., compute the MSE, the mean square of the outputs minus the inputs). Again, a high reconstruction loss is a good sign that the autoencoder is bad, but a low reconstruction loss is not a guarantee that it is good. You should also evaluate the autoencoder according to what it will be used for. For example, if you are using it for unsupervised pretraining of a classifier, then you should also evaluate the classifier’s performance. An undercomplete autoencoder is one whose codings layer is smaller than the input and output layers. If it is larger, then it is an overcomplete autoencoder. The main risk of an excessively undercomplete autoencoder is that it may fail to reconstruct the inputs. The main risk of an overcomplete autoencoder is that it may just copy the inputs to the outputs, without learning any useful feature. To tie the weights of an encoder layer and its corresponding decoder layer, you simply make the decoder weights equal to the transpose of the encoder weights. This reduces the number of parameters in the model by half, often making train‐ing converge faster with less training data, and reducing the risk of overfitting the training set. To visualize the features learned by the lower layer of a stacked autoencoder, a common technique is simply to plot the weights of each neuron, by reshaping each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape [784] to [28, 28]). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic images of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative model would output roughly the same number of images of each digit. Some generative models can be parametrized—for example, to generate only some kinds of outputs. An example of a generative autoencoder is the variational autoencoder. Chapter 16: Reinforcement Learning练习1.您如何定义强化学习？ 它与常规监督或无监督学习有什么不同？2.您能想到本章未提及的RL的三种可能应用吗？ 对于他们每个人来说，环境是什么？ 代理商是什么？可能的行动是什么？ 有什么奖励？3.折扣率是多少？ 如果修改计数率，最优政策会改变吗？4.您如何衡量强化学习代理的表现？5.什么是信用分配问题？ 什么时候发生？ 你怎么能减轻它？6.使用重放内存有什么意义？7.什么是非策略RL算法？ 练习解答 强化学习是机器学习的一个领域，旨在创建能够以最大化奖励的方式在环境中采取行动的代理。 RL与常规监督和无监督学习之间存在许多差异。以下是一些： 在有监督和无监督学习中，目标通常是在数据中找到模式。在强化学习中，目标是找到一个好的策略。 与监督学习不同，代理人没有明确给出“正确”的答案。它必须通过反复试验来学习。 与无监督学习不同，通过奖励有一种监督形式。我们不会告诉代理如何执行任务，但我们会告诉它何时进行任务或何时失败。 强化学习代理需要在探索环境，寻找获得奖励的新方法以及利用已经知道的奖励来源之间找到适当的平衡点。相比之下，有监督和无监督的学习系统通常不需要担心探索;他们只是根据他们给出的训练数据。 在有监督和无监督的学习中，训练实例通常是独立的（事实上，它们通常是洗牌的）。在强化学习中，连续观察通常不是独立的。在移动之前，代理可能会在环境的同一区域停留一段时间，因此连续的观察将非常相关。在某些情况下，使用重放存储器来确保训练算法获得相当独立的观察。 除了第16章中提到的那些之外，以下是强化学习的一些可能应用： 音乐个性化 环境是用户的个性化网络电台。代理是决定该用户接下来要播放的歌曲的软件。其可能的行动是播放目录中的任何歌曲（它必须尝试选择用户将喜欢的歌曲）或播放广告（它必须尝试选择用户将被介入的广告）。每次用户收听歌曲时获得小奖励，每次用户收听广告时获得更大奖励，当用户跳过歌曲或广告时获得负奖励，如果用户离开则获得非常负面奖励。 营销 环境是贵公司的营销部门。代理商是一个软件，根据他们的个人资料和购买历史记录定义应向哪些客户发送邮件活动（对于每个客户，它有两个可能的操作：发送或不发送）。它会对邮寄广告系列的费用产生负面回报，并对此广告系列产生的估算收入产生积极回报。 产品交付 让代理商控制一批运货卡车，决定他们应该在油库接收什么，他们应该去哪里，他们应该放下什么，等等。对于按时交付的每种产品，他们都会得到积极的回报，对于延迟交付，他们会得到负面的回报。 在估算行动的价值时，强化学习算法通常会将此行为带来的所有奖励加起来，给予即时奖励更多的权重，减少后期奖励的权重（考虑到行动对近期的影响大于在遥远的未来）。为了对此进行建模，通常在每个时间步应用折扣率。例如，在折扣率为0.9的情况下，当您估算行动的价值时，两个时间段后收到的100的奖励仅计为0.92×100 = 81。您可以将计算率视为衡量未来相对于现在的估值程度的指标：如果它非常接近1，那么未来的估值几乎与现在一样多。如果它接近0，那么只有直接奖励很重要。当然，这极大地影响了最优政策：如果你重视未来，你可能愿意为最终奖励的前景忍受很多直接的痛苦，而如果你不重视未来，你就会抓住您可以找到的任何直接奖励，永远不会投资于未来。 要衡量强化学习代理的表现，您可以简单地总结其获得的奖励。 在模拟环境中，您可以运行许多epi-sodes并查看平均得到的总奖励（并且可能会查看最小值，最大值，标准差等）。 信用分配问题是，当强化学习代理收到奖励时，它无法直接了解其先前的哪些行为对此奖励有贡献。 它通常发生在一个动作与所产生的奖励之间存在很大的延迟时（例如，在Atari的乒乓球比赛期间，在球员击球之前和赢得该球的那一刻之间可能会有几十个时间步长）。 减轻它的一种方法是在可能的情况下为代理人提供短期奖励。 这通常需要有关任务的先验知识。 例如，如果我们想要建立一个学会下棋的代理人，而不是仅在它赢得比赛时给予奖励，我们可以在每次捕获对手的棋子时给予奖励。 代理人通常可以在一段时间内保持在其环境的同一区域，因此在这段时间内，它的所有经验都非常相似。 这可以在学习算法中引入一些偏差。 它可能会调整这个环境区域的政策，但一旦离开这个区域就不会表现良好。 要解决此问题，您可以使用重放内存; 代理人不会仅使用最直接的学习经验，而是根据过去经验的缓冲来学习，最近也不是最近的经历（也许这就是为什么我们在晚上做梦：重播我们当天的经历并更好地学习 他们？）。 非策略RL算法学习最优策略的值（即，如果代理最佳地行为，则可以为每个状态预期的折扣奖励的总和），而与代理实际行为的方式无关。 Q-Learning是这种算法的一个很好的例子。 相反，on-policy算法学习代理实际执行的策略的值。 Exercises How would you define Reinforcement Learning? How is it different from regular supervised or unsupervised learning? Can you think of three possible applications of RL that were not mentioned in this chapter? For each of them, what is the environment? What is the agent?What are possible actions? What are the rewards? What is the discount rate? Can the optimal policy change if you modify the dis‐count rate? How do you measure the performance of a Reinforcement Learning agent? What is the credit assignment problem? When does it occur? How can you allevi‐ate it? What is the point of using a replay memory? What is an off-policy RL algorithm? Exercise Solutions Reinforcement Learning is an area of Machine Learning aimed at creating agents capable of taking actions in an environment in a way that maximizes rewards over time. There are many differences between RL and regular supervised and unsupervised learning. Here are a few: In supervised and unsupervised learning, the goal is generally to find patterns in the data. In Reinforcement Learning, the goal is to find a good policy. Unlike in supervised learning, the agent is not explicitly given the “right” answer. It must learn by trial and error. Unlike in unsupervised learning, there is a form of supervision, through rewards. We do not tell the agent how to perform the task, but we do tell it when it is making progress or when it is failing. A Reinforcement Learning agent needs to find the right balance between exploring the environment, looking for new ways of getting rewards, and exploiting sources of rewards that it already knows. In contrast, supervised and unsupervised learning systems generally don’t need to worry about explora‐tion; they just feed on the training data they are given. In supervised and unsupervised learning, training instances are typically inde‐pendent (in fact, they are generally shuffled). In Reinforcement Learning, con‐secutive observations are generally not independent. An agent may remain in the same region of the environment for a while before it moves on, so consecu‐tive observations will be very correlated. In some cases a replay memory is used to ensure that the training algorithm gets fairly independent observa‐tions. Here are a few possible applications of Reinforcement Learning, other than those mentioned in Chapter 16: Music personalization The environment is a user’s personalized web radio. The agent is the software deciding what song to play next for that user. Its possible actions are to play any song in the catalog (it must try to choose a song the user will enjoy) or to play an advertisement (it must try to choose an ad that the user will be inter‐ested in). It gets a small reward every time the user listens to a song, a larger reward every time the user listens to an ad, a negative reward when the user skips a song or an ad, and a very negative reward if the user leaves. Marketing The environment is your company’s marketing department. The agent is the software that defines which customers a mailing campaign should be sent to, given their profile and purchase history (for each customer it has two possi‐ble actions: send or don’t send). It gets a negative reward for the cost of the mailing campaign, and a positive reward for estimated revenue generated from this campaign. Product delivery Let the agent control a fleet of delivery trucks, deciding what they should pick up at the depots, where they should go, what they should drop off, and so on. They would get positive rewards for each product delivered on time, and negative rewards for late deliveries. When estimating the value of an action, Reinforcement Learning algorithms typ‐ically sum all the rewards that this action led to, giving more weight to immediate rewards, and less weight to later rewards (considering that an action has more influence on the near future than on the distant future). To model this, a discount rate is typically applied at each time step. For example, with a discount rate of 0.9, a reward of 100 that is received two time steps later is counted as only 0.92 × 100 = 81 when you are estimating the value of the action. You can think of the dis‐count rate as a measure of how much the future is valued relative to the present: if it is very close to 1, then the future is valued almost as much as the present. If it is close to 0, then only immediate rewards matter. Of course, this impacts the optimal policy tremendously: if you value the future, you may be willing to put up with a lot of immediate pain for the prospect of eventual rewards, while if you don’t value the future, you will just grab any immediate reward you can find, never investing in the future. To measure the performance of a Reinforcement Learning agent, you can simply sum up the rewards it gets. In a simulated environment, you can run many epi‐sodes and look at the total rewards it gets on average (and possibly look at the min, max, standard deviation, and so on). The credit assignment problem is the fact that when a Reinforcement Learning agent receives a reward, it has no direct way of knowing which of its previous actions contributed to this reward. It typically occurs when there is a large delay between an action and the resulting rewards (e.g., during a game of Atari’s Pong, there may be a few dozen time steps between the moment the agent hits the ball and the moment it wins the point). One way to alleviate it is to provide the agent with shorter-term rewards, when possible. This usually requires prior knowledge about the task. For example, if we want to build an agent that will learn to play chess, instead of giving it a reward only when it wins the game, we could give it a reward every time it captures one of the opponent’s pieces. An agent can often remain in the same region of its environment for a while, so all of its experiences will be very similar for that period of time. This can intro‐duce some bias in the learning algorithm. It may tune its policy for this region of the environment, but it will not perform well as soon as it moves out of this region. To solve this problem, you can use a replay memory; instead of using only the most immediate experiences for learning, the agent will learn based on a buffer of its past experiences, recent and not so recent (perhaps this is why we dream at night: to replay our experiences of the day and better learn from them?). An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of discounted rewards that can be expected for each state if the agent acts opti‐mally), independently of how the agent actually acts. Q-Learning is a good exam‐ple of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploi‐tation.]]></content>
      <categories>
        <category>机器学习</category>
        <category>hands-on-ml-with-sklearn-and-tf(Aurelien Geron)</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ALL_Tensorflow]]></title>
    <url>%2F2018%2F07%2F27%2FALL_Tensorflow%2F</url>
    <content type="text"><![CDATA[教程·短文GitHub | EffectiveTensorflow | 20180727]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四川大学研究生软件项目管理期末考试分析]]></title>
    <url>%2F2018%2F07%2F09%2F%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%94%9F%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[目标 Understanding the role of software project management in software development Understanding why software project management practices are important Knowing good basic software project management practices Possessing basic skills using software management tools and practices Having applied those skills in software developing with realistic challenges 理解软件项目管理在软件开发中的作用理解为什么软件项目管理实践很重要了解良好的基本软件项目管理实践拥有使用软件管理工具和实践的基本技能将这些技能应用到软件开发中，并具有现实的挑战 Course outline (24 hours) week 1: Introduction of Software Project Management week 2: Project evaluation and programme management week 3: Overview of project planning &amp; Selection of project approach week 4: Software effort estimation week 5: Activity planning week 6: Risk management week 7: Resource allocation week 8: Monitoring and control: contracts, people, team, and quality week 9-17: Team work week 18: Finial Exam 第1周：软件项目管理的引入第2周：项目评估和方案管理第3周：项目规划和项目方法选择的概述第4周：软件工作评估星期5:活动计划第6周:风险管理第七周:资源分配第8周：监控和控制：合同、人员、团队和质量上行线周:团队合作精神18周:顶尖考试 week 2:Project evaluation and programme management项目评估和方案管理We plan to invest 5000 for a project and wish to get 12500 profit after five years. Suppose that there is 2500 profit for every year and the interest rate is 12%. Please compute the NPV and DPBP for the project.我们计划为一个项目投资5000美元，并希望在五年后获得12500美元的利润。 假设每年有2500美元的利润，利率为12％。 请计算项目的NPV和DPBP。 Year Cash-flow Discount Factor Discount cash flow 0 -5000 1.0000 -5000 1 2500 0.8929 2232.14 2 2500 0.7972 1992.98 3 2500 0.7118 1779.45 4 2500 0.6355 1588.84 5 2500 0.5674 1418.53 NP 7500 NPV 4011.94 净利润 $\text{NP(Net profit)}=sum(\text{cash-flow})=7500$ 年平均利润 $\text{average annual profit}=\dfrac{NP}{years}=\dfrac{7500}{5}=1500$ 投资回报率 $\text{ROI(Return on investment )}=\dfrac{\text{Average annual profit}}{\text{Total investment}}=\dfrac{1500}{10000}=0.15$ 折扣因子 Discount factor$\text{Discount factor}=1 / {(1+r)}^t$r is the interest rate (e.g. 10% is 0.10)t is the number of years r is the interest rate (e.g. 10% is 0.10)t is the number of years $\text{DPBP(Dynamic Pay Back Period)} = \text{year.when(sum(Discount cash flow)&gt;0)}=3 years$ 知识点 Year 0’ represents all the costs before system is operation ‘Cash-flow’ is value of income less outgoing Net profit value of all the cash-flows for the lifetime of the application Internal rate of return (IRR) is the discount rate that would produce an NPV of 0 for the project 内部收益率 Internal rate of return (IRR) 某项目期初投资200万，以后的10年每年都有30万的现金流，求该项目的内部收益率（IRR）。（注：利率插值区间宽度小于1%即可）解答：内部收益率（IRR），是指项目投资实际可望达到的收益率，实质上，它是能使项目的净现值等于零时的折现率。-200+[30/(1+IRR)+30/(1+IRR)^2+….+30/(1+IRR)^10]=0 , IRR=8.14%注意：将IRR=8.14%代入上式后结果为0.0375。另外，用Excel函数IRR求得结果为8.14%。 week 3: Overview of project planning &amp; Selection of project approach第3周：项目规划和项目方法选择的概述An invoicing system is to have the following components: amend invoice, produce invoice, produce monthly statements, record cash payment, clear paid invoices from database, create customer records, delete customer. 发票系统具有以下组成部分：修改发票，生成发票，生成月结单，记录现金支付，清除数据库中的付款发票，创建客户记录，删除客户。 (a) What physical dependencies govern the order in which these transactions are implemented?哪些物理依赖关系决定了这些交易的实施顺序？ Create customer 2. Delete customer 3. Produce invoice Amend invoice 5. Payment 6.Clear paid invoices Produce monthly statements (b) How could the system be broken down into increments which would be of some value to the users (hint – think about the problems of taking existing details onto a database when a system is first implemented).如何将系统分解为对用户有一定价值的增量（提示 - 考虑在首次实施系统时将现有细节带入数据库的问题）。 increment 1: Create customer, Delete customer increment 2: Produce invoice, Amend invoice increment 3: Payment, Clear paid invoices increment 4: Produce monthly statements 知识点 产品分解结构 Products CAN BE deliverable or intermediate产品可以是可交付的或中间的 Could be Products(among other things)physical thing (‘installed pc’),a document (‘logical data structure’)a person (‘trained user’)a new version of an old product (‘updated software’) The following are NOT normally products:activities (e.g. ‘training’)events (e.g. ‘interviews completed’)resources and actors (e.g. ‘software developer’) - may be exceptions to this 产品流程图 瀑布模型imposes structure on the projectevery stage needs to be checked and signed offBUTlimited scope for iteration“经典”模型将结构强加于项目每个阶段都需要检查和签名但有限的空间迭代 V-process model Reasons for prototyping原型设计的理由 learning by doing improved communication improved user involvement a feedback loop is established reduces the need for documentation reduces maintenance costs i.e. changes after the application goes live prototype can be used for producing expected results在实践中学习改进的通信改进的用户参与建立了一个反馈回路减少对文档的需求减少维护成本，即应用程序运行后的更改原型可以用于产生预期的结果 Prototyping: some dangers原型:一些危险 users may misunderstand the role of the prototype lack of project control and standards possible additional expense of building prototype focus on user-friendly interface could be at expense of machine efficiency用户可能误解了原型的作用缺乏项目控制和标准建筑原型的额外费用专注于用户友好的界面可能会牺牲机器的效率 渐进的过程 Incremental approach:benefits增量的方法:好处 feedback from early stages used in developing latter stages shorter development thresholds user gets some benefits earlier project may be put aside temporarily reduces ‘gold-plating’开发后阶段使用的早期阶段的反馈缩短开发阈值用户可以更早获得一些好处项目可以暂时搁置减少“画蛇添足” BUT there are some possible disadvantages loss of economy of scale ‘software breakage’ steps ideally 1% to 5% of the total project non-computer steps should be included ideal if a step takes one month or less:not more than three months each step should deliver some benefit to the user some steps will be physically dependent on others规模经济损失“软件破损each step should deliver some benefit to the usersome steps will be physically dependent on others每一步都应该给用户带来一些好处有些步骤将在物理上依赖于他人 V/C ratios: V is a score 1-10 representing value to customerC is a score 0-10 representing value to developers ‘Agile’ methods“敏捷”的方法 structured development methods have some perceived disadvantages： produce large amounts of documentation which can be largely unread documentation has to be kept up to date division into specialist groups and need to follow procedures stifles communication users can be excluded from decision process long lead times to deliver anything etc. etc结构化开发方法有一些明显的缺点产生大量的文档，这些文档大部分都是未读的文档必须保持最新划分为专家小组并需要遵循程序抑制沟通用户可以被排除在决策过程之外交付任何东西的时间很长，等等 The Manifesto for Agile Software Development敏捷软件开发的宣言“We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a planThat is, while there is value in the items on the right, we value the items on the left more.” by Kent Beck et al. 个体交互优于流程和工具 软件产品优于详尽的文档 客户合作优于合同谈判 对变更作出响应优于计划 Eight core DSDM principles八个核心共享原则 Focus on business need Deliver on time Collaborate Never compromise quality Develop iteratively Build incrementally from firm foundations Communication continuously Demonstrate control1。专注于业务需求2。按时交货3。合作4。永不妥协的质量5。迭代开发6。从坚实的基础逐步构建7。不断的沟通8。显示控制 time-box fixed deadline by which something has to be delivered时间盒固定期限，必须交付一些东西 Chapter5 Software effort estimationChapter5软件工作量评估 Given the project data below: What items are size drivers?哪些项目是规模驱动的 inputs, outputs, entity accesses (system users for certain aspects) What items are productivity drivers?哪些项目是生产率驱动的 Programming language What are the productivity rates for programming languages x, y and z?编程语言 x, y, z的生产率分别是多少 x: 10 FPs/day y: 7 FPs/day z: 12 FPs/day x:[(210x0.58+420x0.26+40x1.66)/30+(469x0.58+1406x0.26+125x1.66)/85]/2d)y:[(5130.58+12830.26+761.66)/108+(6600.58+23100.26+881.66)/161+(16000.58+32000.26+237*1.66)/308]/3=[7.012+7.015+6.991]=7.006 比例系数需要记住inputs, outputs, entity accesses：0.58,0.26,1.66 What would be the estimated effort for projects X and Y using a Mark II function point count?使用Mark II功能点计数的项目X和Y的估计工作量是多少？ FP of Project X is 261.8 and FP of Project Y is 704.66. Using the productivity rate for programming language y, the estimate for Project X would be 262/7 i.e. 37 days, and for Project Y 705/7 i.e. 101 days.项目X的FP为261.8，项目Y的FP为704.66。 使用编程语言y的生产率，项目X的估计值为262/7，即37天，而项目Y 705/7，即101天。 What would be the estimated effort for X and Y using an approximate analogy approach? 使用近似类比方法对X和Y的估计工作量是多少？ Project X seems closest to Project 5 which provides an estimate of 22 days, and Project Y seems to be closest to Project 3 which gives an estimate of 108 days.项目X似乎最接近项目5，其提供22天的估计，而项目Y似乎最接近项目3，其估计为108天。 知识点estimated effort = (system size) / productivity e.g.system size = lines of codeproductivity = lines of code per dayproductivity = (system size) / effortbased on past projects估计工作量=（系统大小）/生产率如。系统大小=代码行生产力=每天的代码行数生产力=（系统大小）/努力 Some models focus on task or system size e.g. Function Points一些模型关注任务或系统大小，例如功能点FPs originally used to estimate Lines of Code, rather than effortFPs最初用于估计代码行，而不是工作 Function points Mark II功能点马克二世 For each transaction, countdata items input (Ni)data items output (No)entity types accessed (Ne) Chapter6 Activity PlanningChapter6活动计划 Create a PERT activity network using above data. Calculate the earliest and latest start and end dates and the float associated with each activity. From this identify the critical path.使用上述数据创建PERT活动网络。 计算最早和最晚的开始和结束日期以及与每项活动相关的浮动。 由此确定关键路径。 Answer: 知识点 Activity ‘write report software’Earliest start (ES)Earliest finish (EF) = ES + durationLatest finish (LF) = latest task can be completed without affecting project endLatest start (LS) = LF - duration 活动区间（最左下角的值） = 持续时间 + 最右下角的值 Chapter7 Risk Management第7章风险管理 Using the activity times above: Calcaulate the expected duration and standard deviation for each activity Identify the critical path Draw up an activity diagram applying critical chain principles for this project:Local the places where buffers will need to be located.Assess the size of the buffersStart all activities as later as possible.使用上面的活动时间：1。对每个活动的预期持续时间和标准偏差进行计算2。确定关键路径3。为这个项目绘制一个应用关键链原理的活动图：本地的缓冲区需要被定位的地方。评估缓冲区的大小尽可能晚地开始所有活动。 Activity $t_e$ $s$ $b$ A 10 0.67 2 B 15 1.67 5 C 7 0.67 2 D 10 0.67 2 E 6 1.00 3 A: $t_e=\dfrac{(a+4m+b)}{6}=\dfrac{(8+4*10+12)}{6}=10$,$s=\dfrac{(b-a)}{6}=\dfrac{(12-8)}{6}=0.67$,$b=\dfrac{(b-a)}{2}=2.$ Project buffer = sum(b:where b is key point)/2 = (2+5+2+3)/2=6 Subsidiary chain’s feeding buffer=(b:not key point)/2 = 2/2=1 知识点Using PERT to evaluate the effects of uncertainty使用PERT图来评估不确定性的影响 Three estimates are produced for each activityMost likely time (m)Optimistic time (a)Pessimistic (b)‘expected time’ t_e=\dfrac{(a+4m+b)}{6}‘activity standard deviation’ s=\dfrac{(b-a)}{6} What would be the expected duration of the chain A + B + C? Answer: 12.66 + 10.33 + 25.66 = 48.65 What would be the standard deviation for A + B+ C? Answer: square root of (12 + 12 + 32)= 3.32 Say the target for completing A+B+C was 52 days (T)Calculate the z value thus z = (T – te)/s In this example z = (52-48.33)/3.32 = 1.01 Look up in table of z values – see next overhead Risk exposure (RE)= (potential damage) x (probability of occurrence) Risk reduction leverage =(REbefore- REafter)/ (cost of risk reduction) REbeforeis risk exposure before risk reduction e.g. 1% chance of a fire causing £200k damageREafter is risk exposure after risk reduction e.g. fire alarm costing £500 reduces probability of fire damage to 0.5%RRL = (1% of £200k-0.5% of £200k)/£500 = 2RRL &gt; 1.00 therefore worth doing在降低风险之前，rebefore是风险暴露，例如，1%的可能性发生火灾，造成200 k的损失REafter是在风险降低后的风险敞口，例如，火灾报警成本为500英镑，将火灾损失的可能性降低到0.5%。RRL = (1% of £200k-0.5% of £200k)/£500 = 2RRL &gt; 1.00 因此值得做 Chapter9 Monitoring &amp; ControlChapter9监测与控制 一个项目涉及到四个软件模块的设计和构建，分别称为A、B、C和D。每个模块的估计工作量为60小时，B为30小时，C为40小时，d为45。正在进行这项工作的组织假设是为了获得价值分析（EVA），设计占了30%的工作量，编码40%，测试30%。在这个EVA进行的当天，这个项目应该已经完成了。事实上，情况如下： 实际工作时间显示任务已经完成。1。计算进度和成本差异。2。计算成本性能和进度性能指标。3。从这些数据中可以得出一般的结论这个项目吗? 详细分析已知|model|Estimated effort|Estimated design|Design(actual hours)|Estimated code|Code (actual hours)|Estimated test|Test (actual hours)||-|-|-|-|-|-|-|-||||30%||40%||30%|||A|60||25||40||Not completed||B|30||15||15||15||C|40||15||Not completed||Not completed||D|45||10||Not completed||Not completed| 首先按题目给的比例 30% of the effort, coding 40% and testing 30%,把 Estimated design、Estimated code 和 Estimated test 填写了：|model|Estimated effort|Estimated design|Design(actual hours)|Estimated code|Code (actual hours)|Estimated test|Test (actual hours)||-|-|-|-|-|-|-|-||||30%||40%||30%|||A|60|18|25|24|40|18|Not completed||B|30|9|15|12|15|9|15||C|40|12|15|16|Not completed|12|Not completed||D|45|13.5|10|18|Not completed|13.5|Not completed| Planned value (PV) 计划价值 $PV = sum(\text{Estimated effort)} =60+30+40+45=175$ Earned value (EV) 预算成本（不包括没完成的项目） $EV=sum(\text{Estimated effort)}-\text{Not completed Estimated item}$eg. EV = 175 - (18+16+12+18+13.5)=175-77.5=97.5 或者 竖着加刚才计算出的那三列，同样不包括没完成的项目eg. 18+9+12+13.5=52.5 24+12=36 1552.5+36+15=97.5 Actual cost (AC) 实际成本 $AC=sum\text{(actual hours)}=25+15+15+10+40+15+15=135$ Schedule variance (SV) 进度偏差 $SV=EV-PV=97.5-175=-77.5$ Schedule performance indicator (SPI) 进度绩效指标 $SPI=EV/PV=97.5/175=0.56$ Cost variance (CV) 成本偏差 $CV=EV-AC=97.5-135=-37.5$ Cost performance indicator (CPI) 成本绩效指数 $CPI = EV/AC=97.5/135=72.2$ 注意：都是用预算成本 EV 做分子 综上可知 中英词汇对照表 英文词汇 中文解释 Net profit(NP) 净利润 Cost benefit analysis (CBA)| 成本效益分析Return on investment (ROI) |投资回报率(ROI)Net present value (NPV) |净现值(NPV)Internal rate of return (IRR) |内部收益率（IRR）product breakdown structure(PBS)|产品分解结构Product description (PD)|产品描述(PD)PFD(Product Flow Diagram)|PFD(产品流程图)Gantt charts|甘特图Function Points（FPs）|功能点Risk exposure (RE)|风险承担Risk Reduction Leverage(RRL)|减少风险杠杆(RRL)Earned Value Analysis (EVA)|挣值分析(EVA)Planned value (PV) |计划价值Earned value (EV) |预算成本Actual cost (AC) | 实际成本Schedule variance (SV)| 进度偏差Schedule performance indicator (SPI)| 进度绩效指标Cost variance (CV)| 成本偏差Cost performance indicator (CPI)| 成本绩效指数customized off-the-shelf (COTS) |定制的现成的Invitation to tender (ITT)|招标(ITT)Memoranda of agreement (MoA)|备忘录 补充知识名词解释 名词 解释 项目 项目是一系列具有特定目标,有明确开始和终止日期，资金有限，消耗资源的活动和任务。 检查点 指在规定的时间间隔内对项目进行检查，比较实际与计划之间的差异，并根据差异进行调整。可将检查点看作是一个 固定 “ 采样 ” 时点，而时间间隔根据项目周期长短不同而不同，频度过小会失去意义，频度过大会增加管理成本。常见的间隔是每周一次，项目经理需要召开例会并上交周报。 里程碑 重要的检查点是里程碑，重要的需要客户确认的里程碑，就是基线。在我们实际的项目中，周例会是检查点的表现形式，高层的阶段汇报会是基线的表现形式。 基线 基线是经过评审和批准的配置项的集合，其作用是明确划分项目各阶段，确定各阶段的结束点。在项目的开发过程中，最基本的基线有需求基线、概要设计基线、详细设计基线、代码基线、测试基线、交付基线 CPM|即关键路径法(Critical Path Method），又称关键线路法。|PERT|Program Evaluation Review Technique计划评审技术，是一种任务工期估算的图示法。| 简答题请简述项目管理的9个知识领域和5个过程组。Please briefly describe 9 knowledge areas and 5 process groups of project management. 项目管理的九大知识领域包括项目整体管理、项目范围管理、项目时间管理、项目费用管理、项目质量管理、项目人力资源管理、项目沟通管理、项目风险管理和项目采购管理。项目管理5个过程组包括启动过程组、规划过程组、执行过程组、监督和控制过程组、收尾过程组。 项目管理有哪些不同类型的组织形式？What are the different types of organizational forms of project management? 项目管理组织分为：职能型矩阵型（包括弱矩阵型、平衡矩阵型、强矩阵型、复合矩阵型）项目新 请简述项目经理应该具备的素质。Please give a brief account of the qualities that the project manager should have. 项目经理应具备的素质包括以下几点：a) 广博的知识：包括项目管理知识、IT行业知识、客户行业知识。b) 丰富的经历c) 良好的协调能力d) 良好的职业道德e) 良好的沟通和表达能力f) 良好的领导能力 需求开发过程包括哪些过程？What processes do the requirements development process include? 需求开发包括：需求获取，需求分析，需求规格说明和需求验证等几个过程。 项目经理在需求变更管理中的职责和目标是什么？What are the responsibilities and objectives of project managers in requirements change management? 项目经理在需求彼岸管理中的职责是：a) 负责协调变更的需求并对变更的需求有拒绝的权利b) 负责对变更的需求部分设计的修改c) 保证项目的开发与需求的一致性d) 确定开发进度是否需要进行变更e) 分配新需求给相关开发人员项目经理在需求变更管理的目标：1、相关的干系人必须清楚地了解发生的变更。2、变更处于有效的管理中。3、尽量降低变更带来的风险。 什么是项目时间管理？以及项目管理包括哪些主要过程。What is project time management? And what are the main processes that project management includes. 项目时间管理是：“按时、保质地完成项目”大概是每一位项目经理最希望做到的。但工期托延的情况却时常发生。因而合理地安排项目时间是项目管理中一项关键内容，它的目的是保证按时完成项目、合理分配资源、发挥最佳工作效率。它的主要工作包括定义项目活动、任务、活动排序、每项活动的合理工期估算、制定项目完整的进度计划、资源共享分配、监控项目进度等内容。 综合题假如您是一个项目经理，负责一个基于web的图书管理系统软件的开发，问：应该如何分解此项目所应该包括的工作？请按WBS为该项目制定一份工作的分解计划。If you are a project manager, you are responsible for the development of a web based library management system software, and ask: how should you break down the work that the project should include? Please work out a breakdown plan for the project according to WBS. 答：我作为一个项目经理我按如下方式进行项目分解工作，1、首先做好用户调研计划、调研方式，通过多次迭代方式进行调研，形成最终的用户需求说明书；通过UML工具按模块进行用例图，通过用例图来确定系统范围及边界。2、同项目组开发人员进行沟通，确定项目所拥有的人力资源，根据建设的内容确定人力资源同项目建设内容搭配，根据模块的特性配备不同技术能力、沟通能力的技术人员，实现技术人员使用最优化。3、根据项目里程碑时间，以终为始划分项目开发计划里程碑，在做项目开发计划的根据实际情况酌情调整部分时间。4、做好项目开发计划，公布给项目开发组成员，并且要求严格按照进行执行；在中间发现进度出现问题，及时进行调整。]]></content>
      <categories>
        <category>考试</category>
      </categories>
      <tags>
        <tag>软件项目管理</tag>
        <tag>考试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分类、多分类、多标签和多输出问题解析]]></title>
    <url>%2F2018%2F07%2F01%2F%E4%BA%8C%E5%88%86%E7%B1%BB%E3%80%81%E5%A4%9A%E5%88%86%E7%B1%BB%E4%B8%8E%E5%A4%9A%E6%A0%87%E7%AD%BE%E9%97%AE%E9%A2%98%E7%9A%84%E5%8C%BA%E5%88%AB%E2%80%94%E2%80%94%E5%AF%B9%E5%BA%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[本文分为理论和实验两部分： 理论部分先讲解二分类、多分类与多标签分类问题的基本概念，然后分析它们为什么使用不同的激活函数和损失函数，结论见文末总结。 实验部分重点讲解了多标签和多输出问题。 二分类、多分类、多标签和多输出问题的基本概念二分类：表示分类任务中有两个类别，比如我们想识别一幅图片是不是猫。也就是说，训练一个分类器，输入一幅图片，用特征向量x表示，输出是不是猫，用y=0或1表示。二类分类是假设每个样本都被设置了一个且仅有一个标签 0 或者 1。 多类分类(Multiclass classification): 表示分类任务中有多个类别, 比如对一堆水果图片分类, 它们可能是橘子、苹果、梨等. 多类分类是假设每个样本都被设置了一个且仅有一个标签: 一个水果可以是苹果或者梨, 但是同时不可能是两者。 多标签分类(Multilabel classification): 给每个样本一系列的目标标签. 可以想象成一个数据点的各属性不是相互排斥的(一个水果既是苹果又是梨就是相互排斥的), 比如一个文档相关的话题. 一个文本可能被同时认为是宗教、政治、金融或者教育相关话题。 多输出分类：多个多分类或多标签分类组合输出的分类。网络至少会分支两次（有时候会更多），从而在网络末端创建出多组全连接头——然后你的网络的每个头都会预测一组类别标签，使其有可能学习到不相交的标签组合。比如一个网络同时预测服饰的款式类型和颜色类型。 多分类问题与二分类问题关系 首先，两类问题是分类问题中最简单的一种。其次，很多多类问题可以被分解为多个两类问题进行求解（请看下文分解）。所以，历史上有很多算法都是针对两类问题提出的。下面我们来分析如何处理多分类问题： 直接分成多类比如使用 Softmax 回归。 一对一的策略给定数据集D这里有N个类别，这种情况下就是将这些类别两两配对，从而产生N(N−1)2个二分类任务，在测试的时候把样本交给这些分类器，然后进行投票。 一对其余策略将每一次的一个类作为正例，其余作为反例，总共训练N个分类器。测试的时候若仅有一个分类器预测为正的类别则对应的类别标记作为最终分类结果，若有多个分类器预测为正类，则选择置信度最大的类别作为最终分类结果。 多标签问题与二分类问题关系面临的问题：图片的标签数目不是固定的，有的有一个标签，有的有两个标签，但标签的种类总数是固定的，比如为5类。 解决该问题：采用了标签补齐的方法，即缺失的标签全部使用0标记，这意味着，不再使用one-hot编码。例如：标签为：-1,1,1,-1,1 ;-1表示该类标签没有，1表示该类标签存在，则这张图片的标签编码为： 0 0 0 0 00 1 0 0 00 0 1 0 00 0 0 0 00 0 0 0 1 2.如何衡量损失？ 计算出一张图片各个标签的损失，然后取平均值。 3.如何计算精度 计算出一张图片各个标签的精度，然后取平均值。 该处理方法的本质：把一个多标签问题，转化为了在每个标签上的二分类问题。 损失函数的选择问题基于逻辑回归的二分类问题对于logistic回归，有： h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}逻辑回归有以下优点： 它的输入范围是 $-\infty \to+\infty$，而之于刚好为（0，1），正好满足概率分布为（0，1）的要求。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； 它是一个单调上升的函数，具有良好的连续性，不存在不连续点。 对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function) 。 L(Y,P(Y|X)) = -logP(Y|X)逻辑回归中，采用的则是对数损失函数。根据上面的内容，我们可以得到逻辑回归的对数似然损失函数cost function： cost(h_{\theta}(x),y) = \begin{cases} -log(h_{\theta}(x)) & \text {if y=1} \\ -log(1-h_{\theta}(x)) & \text{if y=0} \end{cases}将以上两个表达式合并为一个，则单个样本的损失函数可以描述为： cost(h_{\theta}(x),y) = -y_ilog(h_{\theta}(x)) - (1-y_i)log(1-h_{\theta}(x))这就是逻辑回归最终的损失函数表达式。 基于 Softmax 的多分类问题softmax层中的softmax 函数是logistic函数在多分类问题上的推广，它将一个N维的实数向量压缩成一个满足特定条件的N维实数向。压缩后的向量满足两个条件： 向量中的每个元素的大小都在[0,1] 向量所有元素的和为 1 因此，softmax适用于多分类问题中对每一个类别的概率判断，softmax的函数公式如下： a^L_j = \frac {e^{z^L_j}}{\sum_k e^{z^L_k}}基于 Softmax 的多分类问题采用的是 log似然代价函数（log-likelihood cost function）来解决。 单个样本的 log似然代价函数的公式为： C = - \sum_i y_i log a_i其中， $y_i$ 表示标签向量的第 $i$ 个分量。因为往往只有一个分量为 1 其余的分量都为 0，所以可以去掉损失函数中的求和符号，化简为， C \equiv -\ln a_j其中， $a_j$ 是向量 $y$ 中取值为 1 对应的第 $j$ 个分量的值。 交叉熵损失函数与 log 似然代价函数关系 本质一样有的文献中也称 log 似然代价函数为交叉熵损失函数，这两个都是交叉熵损失函数，但是看起来长的却有天壤之别。为什么同是交叉熵损失函数，长的却不一样呢？ cost(h_{\theta}(x),y) = -y_ilog(h_{\theta}(x)) - (1-y_i)log(1-h_{\theta}(x))C = - \sum_i y_i log a_i因为这两个交叉熵损失函数对应不同的最后一层的输出。第一个对应的最后一层是 sigmoid，用于二分类问题，第二个对应的最后一层是 softmax，用于多分类问题。但是它们的本质是一样的，请看下面的分析。 首先来看信息论中交叉熵的定义： -\int p(x)\text{log}g(x)dx交叉熵是用来描述两个分布的距离的，神经网络训练的目的就是使 g(x) 逼近 p(x)。 sigmoid + 对数损失函数先看看 sigmoid 作为神经网络最后一层的情况。sigmoid 作为最后一层输出的话，那就不能吧最后一层的输出看作成一个分布了，因为加起来不为 1。现在应该将最后一层的每个神经元看作一个分布，对应的 target 属于二项分布(target的值代表是这个类的概率)，那么第 i 个神经元交叉熵为 y_i\text{log}(h_{\theta})+(1-y_i)\text{log}(1-h_{\theta})其实这个式子可以用求和符号改写， C = - \sum_i log a_i其中， cost(h_{\theta}(x),y) = \begin{cases} -log(a_i) & a_i=h_{\theta} & \text {if } y_i=1 \\ -log(a_i) & a_i=1-h_{\theta} & \text{if } y_i=0 \end{cases}Softmax + 交叉熵现在来看 softmax 作为神经网络最后一层的情况。g(x)是什么呢？就是最后一层的输出 y 。p(x)是什么呢？就是我们的one-hot标签。我们带入交叉熵的定义中算一下，就会得到： C = - \sum_i y_i log a_i交叉熵损失函数与 log 似然损失函数的总结注意到不管是交叉熵损失函数与 log 似然损失函数，交叉熵损失函数用于二分类问题， log 似然损失函数用于多分类，但是对于某一个样本只属于一个类别，只有一个标签。如果用 one-hot 编码样本的标签那么，对于标签向量只有一个分量的值为 1 其余的值都为 0。 所以不管是交叉熵损失函数与 log 似然损失函数，都可以化简为， C \equiv -\ln a_j其中， $a_j$ 是向量 $y$ 中取值为 1 对应的第 $j$ 个分量的值。这两个长的不一样的损失函数实际上是对应的不同的输出层。本质上是一样的。 我的建议是，采用 Kears 中的命名方法，对于二分类的交叉熵损失函数称之为 “二分类交叉熵损失函数（binary_crossentropy）” ，对于多分类的交叉熵损失函数称之为 “多类别交叉熵损失函数（categorical_crossentropy）”。 在 Kears 中也有提示（注意: 当使用categorical_crossentropy损失时，你的目标值应该是分类格式 (即，如果你有10个类，每个样本的目标值应该是一个10维的向量，这个向量除了表示类别的那个索引为1，其他均为0)。 为了将 整数目标值 转换为 分类目标值，你可以使用Keras实用函数to_categorical：） 一种更直接的证明方法，数学公式， 内容来源于 Hands-on Machine Learning with Scikit-Learn and TensorFlow 141 页。 多标签分类 + 二分类交叉熵损失函数多标签问题与二分类问题关系在上文已经讨论过了，方法是计算一个样本各个标签的损失（输出层采用sigmoid函数），然后取平均值。把一个多标签问题，转化为了在每个标签上的二分类问题。 总结 分类问题名称 输出层使用激活函数 对应的损失函数 二分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy） 多分类 Softmax函数 多类别交叉熵损失函数（categorical_crossentropy） 多标签分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy） 为什么要引入多输出分类问题使用Keras执行多标签分类非常简单，包括两个主要步骤： 使用 sigmoid 激活替换网络末端的 softmax 激活换出明确的交叉熵的二进制交叉熵对你的损失函数从那里你可以像往常一样训练你的网络。应用上述过程的最终结果是多类分类器。可以使用Keras多类分类来预测多个标签只是一个单一的向前传递。 但是，您需要考虑以下问题： 您需要针对要预测的每个类别组合的培训数据。 就像神经网络无法预测从未接受过训练的类一样，您的神经网络无法预测从未见过的组合的多个类别标签。这种行为的原因是由于网络内神经元的激活。 如果您的网络接受过（1）黑色裤子和（2）红色衬衫的示例训练，现在您想要预测“红裤子”（数据集中没有“红裤子”图像），负责检测的神经元“红色”和“裤子”会触发，但由于网络一旦到达完全连接的层之前从未见过这种数据/激活组合，您的输出预测很可能是不正确的（即，您可能会遇到“红色”或“裤子”，但两者都不太可能）。 同样，您的网络无法正确预测从未接受过培训的数据（您也不应该期望它）。在培训您自己的Keras网络进行多标签分类时，请牢记这一点。 使用多输出分类就可以解决此问题。通过创建两个全连接头和相关的子网络（如有必要），我们可以训练一个头分类服装种类，另一个头负责识别颜色——最终得到的网络可以分类「黑色裙子」，即使它之前从未在这样的数据上训练过！ 多输出分类:针对服装类型和颜色的实验实验详细指导 创建模型的关键代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class FashionNet: @staticmethod def build_category_branch(inputs, numCategories, finalAct="softmax", chanDim=-1): # utilize a lambda layer to convert the 3 channel input to a # grayscale representation x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs) # CONV =&gt; RELU =&gt; POOL x = Conv2D(32, (3, 3), padding="same")(x) x = Activation("relu")(x) x = BatchNormalization(axis=chanDim)(x) x = MaxPooling2D(pool_size=(3, 3))(x) x = Dropout(0.25)(x) # Omit some similar code # define a branch of output layers for the number of different # clothing categories (i.e., shirts, jeans, dresses, etc.) x = Flatten()(x) x = Dense(256)(x) x = Activation("relu")(x) x = BatchNormalization()(x) x = Dropout(0.5)(x) x = Dense(numCategories)(x) x = Activation(finalAct, name="category_output")(x) # return the category prediction sub-network return x @staticmethod def build_color_branch(inputs, numColors, finalAct="softmax", chanDim=-1): # CONV =&gt; RELU =&gt; POOL x = Conv2D(16, (3, 3), padding="same")(inputs) x = Activation("relu")(x) x = BatchNormalization(axis=chanDim)(x) x = MaxPooling2D(pool_size=(3, 3))(x) x = Dropout(0.25)(x) # Omit some similar code # define a branch of output layers for the number of different # colors (i.e., red, black, blue, etc.) x = Flatten()(x) x = Dense(128)(x) x = Activation("relu")(x) x = BatchNormalization()(x) x = Dropout(0.5)(x) x = Dense(numColors)(x) x = Activation(finalAct, name="color_output")(x) # return the color prediction sub-network return x @staticmethod def build(width, height, numCategories, numColors, finalAct="softmax"): # initialize the input shape and channel dimension (this code # assumes you are using TensorFlow which utilizes channels # last ordering) inputShape = (height, width, 3) chanDim = -1 # construct both the "category" and "color" sub-networks inputs = Input(shape=inputShape) categoryBranch = FashionNet.build_category_branch(inputs, numCategories, finalAct=finalAct, chanDim=chanDim) colorBranch = FashionNet.build_color_branch(inputs, numColors, finalAct=finalAct, chanDim=chanDim) # create the model using our input (the batch of images) and # two separate outputs -- one for the clothing category # branch and another for the color branch, respectively model = Model( inputs=inputs, outputs=[categoryBranch, colorBranch], name="fashionnet") # return the constructed network architecture return model 实验输出 模型预测效果 参考文献[1] François Chollet‏. Keras Document[DB/OL]. https://keras.io/, 2018-07-01. [2] 目力过人. 多标签分类（multilabel classification ）[DB/OL]. https://blog.csdn.net/bemachine/article/details/10471383, 2018-07-01. [3] Inside_Zhang.【联系】二项分布的对数似然函数与交叉熵（cross entropy）损失函数[DB/OL]. https://blog.csdn.net/lanchunhui/article/details/75433608, 2018-07-01. [4] ke1th. 两种交叉熵损失函数的异同[DB/OL]. https://blog.csdn.net/u012436149/article/details/69660214, 2018-07-01. [5] bitcarmanlee. logistic回归详解一：为什么要使用logistic函数[DB/OL]. https://blog.csdn.net/bitcarmanlee/article/details/51154481, 2018-07-01. [6] Aurélien Géron. Hands-On Machine Learning with Scikit-Learn and TensorFlow[M]. America: O’Reilly Media, 2017-03-10, 140-141. [7] Adrian Rosebrock. 使用Keras实现多输出分类：用单个模型同时执行两个独立分类任务[DB/OL]. https://www.jiqizhixin.com/articles/2018-08-14-2, 2018-08-18.]]></content>
      <categories>
        <category>实验</category>
      </categories>
      <tags>
        <tag>二分类</tag>
        <tag>多分类</tag>
        <tag>多标签</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络可以计算任何函数的可视化证明]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 4 A visual proof that neural nets can compute any function 神经网络的一个最显著的事实就是它可以计算任何的函数。不管这个函数是什么样的，总会确保有一个神经网络能够对任何可能的输入 $x$，其值 $f(x)$ （或者某个足够准确的近似）是网络的输出。 表明神经网络拥有一种普遍性。不论我们想要计算什么样的函数，我们都确信存在一个神经网络可以计算它。 而且，这个普遍性定理甚至在我们限制了神经网络只在输入层和输出层之间存在一个中间层的情况下成立。所以即使是很简单的网络架构都极其强大。普遍性定理在使用神经网络的人群中是众所周知的。但是它为何正确却不被广泛地理解。现有的大多数的解释都具有很强的技术性。 如果你是数学家，这个证明应该不大难理解，但对于大多数人还是很困难的。这不得不算是一种遗憾，因为这个普遍性背后的原理其实是简单而美妙的。在这一章，我给出这个普遍性定理的简单且大部分为可视化的解释。我们会一步步深入背后的思想。你会理解为何神经网络可以计算任何的函数。你会理解到这个结论的一些局限。并且你还会理解这些结论如何和深度神经网络关联的。 神经网络拥有强大的算法来学习函数。学习算法和普遍性的结合是一种有趣的混合。直到现在，本书一直是着重谈学习算法。到了本章，我们来看看普遍性，看看它究竟意味着什么。 两个预先声明在解释为何普遍性定理成立前，我想要提下关于非正式的表述“神经网络可以计算任何函数”的两个预先声明。 第一点，这句话不是说一个网络可以被用来准确地}计算任何函数。而是说，我们可以获得尽可能好的一个近似。通过增加隐藏元的数量，我们可以提升近似的精度。为了让这个表述更加准确，假设我们给定一个需要按照目标精度 $\epsilon &gt; 0$ 的函数 $f(x)$。通过使用足够多的隐藏神经元使得神经网络的输出 $g(x)$ 对所有的 $x$，满足 $|g(x) - f(x)| &lt; \epsilon$ 从而实现近似计算。换言之，近似对每个可能的输入都是限制在目标准确度范围内的。 第二点，就是可以按照上面的方式近似的函数类其实是连续}函数。如果函数不是连续的，也就是会有突然、极陡的跳跃，那么一般来说无法使用一个神经网络进行近似。这并不意外，因为神经网络计算的就是输入的连续函数。然而，即使那些我们真的想要计算的函数是不连续的，一般来说连续的近似其实也足够的好了。如果这样的话，我们就可以用神经网络来近似了。实践中，这通常不是一个严重的限制。 总结一下，更加准确的关于普遍性定理的表述是包含一个隐藏层的神经网络可以被用来按照任意给定的精度来近似任何连续函数。 本章可视化证明的说明 我们会使用了两个隐藏层的网络来证明这个结果的弱化版本。然后我将简要介绍如何通过一些微调把这个解释适应于只使用一个隐藏层的网络的证明。 我们使用 S 型神经元作为神经网络的激活函数，在后面我们可以推广到其它激活函数。 证明的理论准备 S 型神经元作为神经网络的激活函数非常普遍，但是为了方便我们的证明，我们对 S 型神经元做一点点处理，把它变成阶跃函数（感知器）。实际上处理阶跃函数比一般的 S 型函数更加容易。原因是在输出层我们把所有隐藏神经元的贡献值加在一起。分析一串阶跃函数的和是容易的，相反，思考把一串 S 形状的曲线加起来是什么则要更困难些。所以假设我们的隐藏神经元输出阶跃函数会使事情更容易。更具体些，我们把权重 $w$ 固定在一个大的值，然后通过修改偏置设置阶跃函数的位置。当然，把输出作为一个阶跃函数处理只是一个近似，但是它是一个非常好的近似，现在我们把它看作是精确的。稍后我会再讨论偏离这种近似的影响。关于 S 型神经元和感知器的关系分析见 使用神经网络识别手写数字——感知器。 S 型函数变阶跃函数过程 S 型神经元的代数形式是， \sigma(z) \equiv \frac{1}{1+e^{-z}}其中， $z = w \cdot x+b$ S 型神经元的函数图形是， $x$ 取何值时阶跃会发生呢？换种方式，阶跃的位置如何取决于权重和偏置？ 因为 $z = w \cdot x+b$ 是一个直线方程， $w$ 是直线的斜率， $b$ 是直线的截距，直线与 $x$ 轴的交点是 $(-b/w, 0)$，令 $s=-b/w$。 假设 $z \equiv w \cdot x + b$ 是一个很大的正数。那么 $e^{-z} \approx 0$ 而 $\sigma(z) \approx 1$。即，当 $z = w \cdot x+b$ 很大并且为正， S 型神经元的输出近似为 $1$，正好和感知器一样。相反地，假设 $z = w \cdot x+b$ 是一个很大的负数。那么$e^{-z} \rightarrow \infty$，$\sigma(z) \approx 0$。所以当 $z = w \cdot x +b$ 是一个很大的负数。 根据上面两点可以知道，当 $|w|$ 的值很大时，$x$ 就能直接决定 S 型神经元的函数结果为零还是为一，根据第一点还可以知道 $s=-b/w$ 就是阶跃的分界点。当 $x&gt;s$ 时 S 型函数取 1，当 $x&lt;s$时 S 型函数取 0。注意，$s=-b/w$ 时我们将在后文修补阶跃函数中讨论。 S 型神经元的函数变成阶跃函数图形是： 下面的证明中，我们总是让 $|w|$ 的值很大，也就是说我们将一直使用变成阶跃函数的 S 型神经元。并且用 $s$ 表示阶跃函数阶跃的位置。这将用仅仅一个参数 $s$ 来极大简化我们描述隐藏神经元的方式，这就是阶跃位置，$s =-b/w$。 一个输入和一个输出的普遍性目前为止我们专注于仅仅从顶部隐藏神经元输出。让我们看看整个网络的行为。尤其，我们假设隐藏神经元在计算以阶跃点 $s_1$ （顶部神经元）和 $s_2$ （底部神经元）参数化的节约函数。它们各自有输出权重 $w_1$ 和 $w_2$。是这样的网络： 右边的绘图是隐藏层的加权输出 $w_1 a_1 + w_2 a_2$。这里 $a_1$ 和 $a_2$ 各自是顶部和底部神经元的输出。这些输出由 $a$ 表示，是因为它们通常被称为神经元的激活值。注意，注意整个网络的输出是 $\sigma(w_1 a_1+w_2 a_2 + b)$，其中 $b$ 是隐藏层的偏置。很明显，这不同于隐藏层加权后的输出，也就是我们这里的绘图。我们现在打算专注于隐藏层的加权输出，不一会就会考虑如何把它关联到整个网络的输出。 思考增加和减少每一个输出权重。注意，这如何调整从各自的隐藏神经元的贡献值。当一个权重是 0 时会发生什么？ 最后，试着设置 $w_1$ 为 $0.8$，$w_2$ 为 $-0.8$。你得到一个“凸起”的函数，它从点 $s_1$ 开始，到点 $s_2$ 结束，高为 $0.8$。例如，加权后的输出可能看起来像这样： 当然，我们可以调整为任意的凸起高度。让我们用一个参数，$h$，来表示高度。为了减少混乱我也会移除“$s_1 = \ldots$”和“$w_1 = \ldots$”的标记。 试着将 $h$ 值改大或改小，看看凸起的高度如何改变。试着把高度值改为负数，观察发生了什么。并且试着改变阶跃点来看看如何改变凸起的形状。我们可以用凸起制作的技巧来得到两个凸起，通过把两对隐藏神经元一起填充进同一个网络： 这里我抑制了权重，只是简单地在每对隐藏神经元上写了 $h$ 的值。试着增加和减少两个 $h$ 值，观察它如何改变图形。通过修改节点来移动凸起。更普遍地，我们可以利用这个思想来取得我们想要的任何高度的峰值。 其实到这里我们可以说已经证明了神经网络在一个输入和一个输出上的普遍性，因为从微积分的观点来看，只需要增加“凸起”的个数，众多“凸起”合在一起的图形就可更加接近需要近似的函数图形。无限多个“凸起”就能无限逼近于目标函数。一个示意的例子如下， 让我快速总结一下那是如何工作的。 第一层的权重都有一些大的，恒定的值，比如：$w = 1000$。 隐藏神经元上的偏置只是 $b = -w s$。例如，对于第二个隐藏神经元 $s = 0.2$ 变成了 $b = -1000 \times 0.2 = -200$。 最后一层的权重由 $h$ 值决定。例如，我们上面已经选择的第一个 $h$，$h = -0.5$，意味着顶部两个隐藏神经元的相应的输出权重是 $-0.5$ 和 $0.5$。如此等等，确定整个层的输出权重。 最后，输出神经元的偏置为 $0$。 这是所有要做的事情：现在我们有了一个可以很好计算我们原始目标函数的神经网络的完整的描述。而且我们理解如何通过提高隐层神经元的数目来提高近似的质量。 在本质上，我们使用我们的单层神经网络来建立一个函数的查找表。我们将能够建立这个思想，以提供普遍性的一般性证明。 多个输入变量的普遍性让我们把结果扩展到有很多个输入变量的情况下。这听上去挺复杂，但是所有我们需要的概念都可以在两个输入的情况下被理解。所以让我们处理两个输入的情况。我们从考虑当一个神经元有两个输入会发生什么开始： 这里，我们有输入 $x$ 和 $y$，分别对应于权重 $w_1$ 和 $w_2$，以及一个神经元上的偏置 $b$。让我们把权重 $w_2$ 设置为 $0$，然后反复琢磨第一个权重 $w_1$ 和偏置 $b$，看看他们如何影响神经元的输出： 正如我们前面讨论的那样，随着输入权重变大，输出接近一个阶跃函数。不同的是，现在的阶跃函数是在三个维度。也如以前一样，我们可以通过改变偏置的位置来移动阶跃点的位置。阶跃点的实际位置是 $s_x \equiv -b / w_1$。 我们可以用我们刚刚构造的阶跃函数来计算一个三维的凹凸函数。为此，我们使用两个神经元，每个计算一个$x$ 方向的阶跃函数。然后我们用相应的权重 $h$ 和 $-h$ 将这两个阶跃函数混合，这里 $h$ 是凸起的期望高度。所有这些在下面图示中说明： 试着改变高度 $h$ 的值。观察它如何和网络中的权重关联。并看看它如何改变右边凹凸函数的高度。 我们已经解决了如何制造一个 $x$ 方向的凹凸函数。当然，我们可以很容易地制造一个$y$ 方向的凹凸函数，通过使用 $y$ 方向的两个阶跃函数。回想一下，我们通过使 $y$输入的权重变大，$x$ 输入的权重为 $0$ 来这样做。这是结果： 这看上去和前面的网络一模一样！唯一的明显改变的是在我们的隐藏神经元上现在标记有一个小的 $y$。那提醒我们它们在产生 $y$ 方向的阶跃函数，不是 $x$ 方向的，并且 $y$上输入的权重变得非常大，$x$ 上的输入为 $0$，而不是相反。正如前面，我决定不去明确显示它，以避免图形杂乱。 让我们考虑当我们叠加两个凹凸函数时会发生什么，一个沿 $x$ 方向，另一个沿 $y$ 方向，两者都有高度 $h$： 为了简化图形，我丢掉了权重为 $0$ 的连接。现在，我在隐藏神经元上留下了 $x$ 和 $y$的标记，来提醒你凹凸函数在哪个方向上被计算。后面我们甚至为丢掉这些标记，因为它们已经由输入变量说明了。试着改变参数 $h$。正如你能看到，这引起输出权重的变化，以及 $x$ 和 $y$ 上凹凸函数的高度。 我们构建的有点像是一个塔型函数，如果我们能构建这样的塔型函数，那么我们能使用它们来近似任意的函数。 如果我们选择适当的阈值，比如，$3h/2$，这是高原的高度和中央塔的高度中间的值 ——我们可以把高原下降到零，并且依旧矗立着塔。 你能明白怎么做吗？试着用下面的网络做实验来解决。请注意， 我们现在正在绘制整个网络的输出，而不是只从隐藏层的加权输出。这意味着我们增加了一个偏置项到隐藏层的加权输出，并应用 S 型函数。 你能找到 $h$ 和 $b$ 的值，能产生一个塔型吗？这有点难，所以如果你想了一会儿还是困住，这是有两个提示：（1）为了让输出神经元显示正确的行为，我们需要输入的权重（所有 $h$ 或 $-h$）变得很大；（2）$b$ 的值决定了阈值的大小。 这是它看起来的样子，我们使用 $h = 10$： 甚至对于这个相对适中的 $h$ 值，我们得到了一个相当好的塔型函数。当然，我们可以通过更进一步增加 $h$ 并保持偏置$b = -3h/2$ 来使它如我们所希望的那样。 让我们尝试将两个这样的网络组合在一起，来计算两个不同的塔型函数。为了使这两个子网络更清楚，我把它们放在如下所示的分开的方形区域：每个方块计算一个塔型函数，使用上面描述的技术。图上显示了第二个隐藏层的加权输出，即，它是一个加权组合的塔型函数。 尤其你能看到通过修改最终层的权重能改变输出塔型的高度。同样的想法可以用在计算我们想要的任意多的塔型。我们也可以让它们变得任意细，任意高。通过使第二个隐藏层的加权输出为 $\sigma^{-1} \circ f$ 的近似，我们可以确保网络的输出可以是任意期望函数 $f$ 的近似。 让我们试试三个变量 $x_1, x_2, x_3$。下面的网络可以用来计算一个四维的塔型函数： 这里，$x_1, x_2, x_3$ 表示网络的输入。$s_1, t_1$ 等等是神经元的阶跃点~——~即，第一层中所有的权重是很大的，而偏置被设置为给出阶跃点 $s_1, t_1, s_2, \ldots$。第二层中的权重交替设置为 $+h, -h$，其中 $h$ 是一个非常大的数。输出偏置为 $-5h/2$。 这个网络计算这样一个函数，当三个条件满足时：$x_1$ 在 $s_1$ 和 $t_1$ 之间；$x_2$在$s_2$ 和 $t_2$ 之间；$x_3$ 在 $s_3$ 和 $t_3$ 之间，输出为 $1$。其它情况网络输出为 $0$。即，这个塔型在输入空间的一个小的区域输出为 $1$，其它情况输出 $0$。 通过组合许多个这样的网络我们能得到任意多的塔型，如此可近似一个任意的三元函数。对于 $m$ 维可用完全相同的思想。唯一需要改变的是将输出偏置设为 $(-m+1/2)h$，为了得到正确的夹在中间的行为来弄平高原。 好了，所以现在我们知道如何用神经网络来近似一个多元的实值函数。对于 $f(x_1,\ldots, x_m) \in R^n$ 的向量函数怎么样？当然，这样一个函数可以被视为 $n$ 个单独的实值函数： $f^1(x_1, \ldots, x_m)$， $f^2(x_1, \ldots, x_m)$ 等等。所以我们创建一个网络来近似 $f^1$，另一个来近似 $f^2$，如此等等。然后简单地把这些网络都组合起来。 所以这也很容易应付。 思考 我们已经看到如何使用具有两个隐藏层的网络来近似一个任意函数。你能否找到一个证明，证明只有一个隐藏层是可行的？作为一个提示，试着在只有两个输入变量的情况下工作，并证明：（a）可以得到一个不仅仅在 $x$ 和 $y$ 方向，而是在一个任意方向上的阶跃函数；（b）可以通过累加许多的源自（a）的结构，近似出一个塔型的函数，其形状是圆的，而不是方的；（c）使用这些圆形塔，可以近似一个任意函数。对于（c）可以使用本章稍后的一些思想。 S 型神经元的延伸我们已经证明了由 S 型神经元构成的网络可以计算任何函数。回想下在一个 S 型神经元中，输入$x_1, x_2, \ldots$ 导致输出 $\sigma(\sum_j w_j x_j + b)$，这里 $w_j$ 是权重，$b$ 是偏置，而 $\sigma$ 是 S 型函数： 如果我们考虑一个不同类型的神经元，它使用其它激活函数，比如如下的 $s(z)$，会怎样？ 更确切地说，我们假定如果神经元有输入 $x_1, x_2, \ldots$，权重 $w_1, w_2, \ldots$和偏置 $b$，那么输出为 $s(\sum_j w_j x_j + b)$。我们可以使用这个激活函数来得到一个阶跃函数，正如用 S 型函数做过的一样。 正如使用 S 型函数的时候，这导致激活函数收缩，并最终变成一个阶跃函数的很好的近似。试着改变偏置，然后你能看到我们可以设置我们想要的阶跃位置。所以我们能使用所有和前面相同的技巧来计算任何期望的函数。 $s(z)$ 需要什么样的性质来满足这样的结果呢？我们确实需要假定 $s(z)$ 在 $z\rightarrow -\infty$ 和 $z \rightarrow \infty$ 时是定义明确的。这两个界限是在我们的阶跃函数上取的两个值。我们也需要假定这两个界限彼此不同。如果它们不是这样，就没有阶跃，只是一个简单的平坦图形！但是如果激活函数 $s(z)$ 满足这些性质，基于这样一个激活函数的神经元可普遍用于计算。 问题 在本书前面我们遇到过其它类型的称为修正线性单元的神经元。解释为什么这样的神经元不满足刚刚给出的普遍性的条件。找到一个普遍性的证明，证明修正线性单元可普遍用于计算。 假设我们考虑线性神经元，即具有激活函数 $s(z) = z$ 的神经元。解释为什么线性神经元不满足刚刚给出的普遍性的条件。证明这样的神经元不能用于通用计算。 修补阶跃函数目前为止，我们假定神经元可以准确生成阶跃函数。这是一个非常好的近似，但也仅仅是近似。实际上，会有一个很窄的故障窗口，如下图说明，在这里函数会表现得和阶跃函数非常不同。 在这些故障窗口中我给出的普遍性的解释会失败。 现在，它不是一个很严重的故障。通过使得输入到神经元的权重为一个足够大的值，我们能把这些故障窗口变得任意小。当然，我们可以把故障窗口窄过我在上面显示的~——~窄得我们的眼睛都看不到。所以也许我们可以不用过于担心这个问题。 尽管如此，有一些方法解决问题是很好的。 实际上，这个问题很容易解决。让我们看看只有一个输入和一个输出的神经网络如何修补其计算函数。同样的想法也可以解决有更多输入和输出的问题。 特别地，假设我们想要我们的网络计算函数 $f$。和以前一样，我们试着设计我们的网络，使得隐藏神经元的加权输出是 $\sigma^{-1} \circ f(x)$： 如果我们要使用前面描述的技术做到这一点，我们会使用隐藏神经元产生一系列的凹凸函数： 再说一下，我夸大了图上的故障窗口大小，好让它们更容易看到。很明显如果我们把所有这些凹凸函数加起来，我们最终会得到一个合理的 $\sigma^{-1} \circ f(x)$ 的近似，除了那些故障窗口。 假设我们使用一系列隐藏神经元来计算我们最初的目标函数的一半，即 $\sigma^{-1} \circ f(x) / 2$，而不是使用刚刚描述的近似。当然，这看上去就像上一个图像的缩小的版本： 并且假设我们使用另一系列隐藏神经元来计算一个 $\sigma^{-1} \circ f(x) / 2$ 的近似，但是用将凹凸图形偏移一半宽度： 现在我们有两个不同的 $\sigma^{-1} \circ f(x) / 2$ 的近似。如果我们把这两个近似图形加起来，我们会得到一个 $\sigma^{-1} \circ f(x)$ 的整体近似。这个整体的近似仍然在一些小窗口的地方有故障。但是问题比以前要小很多。原因是在一个近似中的故障窗口的点，不会在另一个的故障窗口中。所以在这些窗口中，近似会有 $2$ 倍的因素更好。 我们甚至能通过加入大量的，用 $M$ 表示，重叠的近似 $\sigma^{-1} \circ f(x) / M$来做得更好。假设故障窗口已经足够窄了，其中的点只会在一个故障窗口中。并且假设我们使用一个 $M$ 足够大的重叠近似，结果会是一个非常好的整体近似。 结论我们已经讨论的对于普遍性的解释当然不是如何使用神经网络计算的切实可行的用法！其更像是 NAND 门或者其它类似的普遍性证明。因为这个原因，我主要专注于让解释更清晰和易于理解，而不是过于挖掘细节。然而，你可以发现如果你能改进这个解释是个很有趣和有教益的练习。 尽管这个结果并不能直接用于解释网络，它还是是很重要的，因为它解开了是否使用一个神经网络可以计算任意特定函数的问题。对这个问题的答案总是“是”。所以需要问的正确问题，并不是是否任意函数可计算，而是计算函数的好的方法是什么。 我们建立的对于普遍性的解释只使用了两个隐藏层来计算一个任意的函数。而且，正如我们已经讨论过的，只使用单个的隐藏层来取得相同的结果是可能的。鉴于此，你可能想知道为什么我们会对深度网络感兴趣，即具有很多隐藏层的网络。我们不能简单地把这些网络用浅层的、单个隐藏层的网络替换吗？ 尽管在原理上这是可能的，使用深度网络仍然有实际的原因。正如在第一章中表明过，深度网络有一个分级结构，使其尤其适用于学习分级的知识，这看上去可用于解决现实世界的问题。但是更具体地，当攻克诸如图像识别的问题，使用一个不仅能理解单独的像素，还能理解越来越复杂的概念的系统是有帮助的，这里说的复杂的概念，可以从图像的边缘信息到简单的几何形状，以及所有复杂的、多物体场景的方式。在后面的章节中，我们将看到在学习这样的分级知识时，深度网络要比浅层网络做得更好。总结一下： 普遍性告诉我们神经网络能计算任何函数；而实际经验依据提示深度网络最能适用于学习能够解决许多现实世界问题的函数。 参考文献[1] Michael Nielsen. CHAPTER 4 A visual proof that neural nets can compute any function[DB/OL]. http://neuralnetworksanddeeplearning.com/chap4.html, 2018-06-29. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap4.tex, 2018-06-29.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 4 A visual proof that neural nets can compute any function</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——如何选择神经网络的超参数]]></title>
    <url>%2F2018%2F06%2F28%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%B6%85%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 为什么选择神经网络的超参数是个难题直到现在，我们还没有解释对诸如学习率 $\eta$，正则化参数 $\lambda$ 等等超参数选择的方法。我只是给出那些效果很好的值而已。实践中，当你使用神经网络解决问题时，寻找好的超参数其实是很困难的一件事。例如，我们要解决 MNIST 问题，开始时对于选择什么样的超参数一无所知。假设，刚开始的实验中选择前面章节的参数都是运气较好。但在使用学习率 $\eta=10.0$ 而\正则化参数 $\lambda=1000.0$。下面是我们的一个尝试： 1234567891011121314151617181920212223242526&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10])&gt;&gt;&gt; net.SGD(training_data, 30, 10, 10.0, lmbda = 1000.0,... evaluation_data=validation_data, monitor_evaluation_accuracy=True)Epoch 0 training completeAccuracy on evaluation data: 1030 / 10000Epoch 1 training completeAccuracy on evaluation data: 990 / 10000Epoch 2 training completeAccuracy on evaluation data: 1009 / 10000...Epoch 27 training completeAccuracy on evaluation data: 1009 / 10000Epoch 28 training completeAccuracy on evaluation data: 983 / 10000Epoch 29 training completeAccuracy on evaluation data: 967 / 10000 我们分类准确率并不比随机选择更好。网络就像随机噪声产生器一样。 你可能会说，“这好办，降低学习率和正则化参数就好了。”不幸的是，你并不先验地知道这些就是需要调整的超参数。可能真正的问题出在 $30$ 个隐藏元中，本身就不能很有效，不管我们如何调整其他的超参数都没有作用的？可能我们真的需要至少 $100$ 个隐藏神经元？或者是 $300$ 个隐藏神经元？或者更多层的网络？或者不同输出编码方式？可能我们的网络一直在学习，只是学习的回合还不够？可能 minibatch 设的太小了？可能我们需要切换成二次代价函数？可能我们需要尝试不同的权重初始化方法？等等。很容易就在超参数的选择中迷失了方向。如果你的网络规模很大，或者使用了很多的训练数据，这种情况就很令人失望了，因为一次训练可能就要几个小时甚至几天乃至几周，最终什么都没有获得。如果这种情况一直发生，就会打击你的自信心。可能你会怀疑神经网络是不是适合你所遇到的问题？可能就应该放弃这种尝试了？ 本节，我会给出一些用于设定超参数的启发式想法。目的是帮你发展出一套工作流来确保很好地设置超参数。当然，我不会覆盖超参数优化的每个方法。那是太繁重的问题，而且也不会是一个能够完全解决的问题，也不存在一种通用的关于正确策略的共同认知。总是会有一些新的技巧可以帮助你提高一点性能。但是本节的启发式想法能帮你开个好头。 宽泛策略（Broad strategy）在使用神经网络来解决新的问题时，一个挑战就是获得任何一种非寻常的学习，也就是说，达到比随机的情况更好的结果。这个实际上会很困难，尤其是遇到一种新类型的问题时。让我们看看有哪些策略可以在面临这类困难时候尝试。 假设，我们第一次遇到 MNIST 分类问题。刚开始，你很有激情，但是当第一个神经网络完全失效时，你会觉得有些沮丧。此时就可以将问题简化。丢开训练和验证集合中的那些除了$0$ 和 $1$ 的那些图像。然后试着训练一个网络来区分 $0$ 和 $1$。不仅仅问题比 $10$个分类的情况简化了，同样也会减少 80\% 的训练数据，这样就给出了 $5$ 倍的加速。这样可以保证更快的实验，也能给予你关于如何构建好的网络更快的洞察。 你通过简化网络来加速实验进行更有意义的学习。如果你相信 $[784, 10]$ 的网络更可能比随机更加好的分类效果，那么就从这个网络开始实验。这会比训练一个 $[784, 30 ,10]$的网络更快，你可以进一步尝试后一个。 你可以通过提高监控的频率来在试验中获得另一个加速了。 我们可以继续，逐个调整每个超参数，慢慢提升性能。一旦我们找到一种提升性能的 $\eta$ 值，我们就可以尝试寻找好的值。然后按照一个更加复杂的网络架构进行实验，假设是一个有 $10$ 个隐藏元的网络。然后继续调整 $\eta$ 和 $\lambda$。接着调整成 $20$ 个隐藏元。然后将其他的超参数调整再调整。如此进行，在每一步使用我们 hold out 验证数据集来评价性能，使用这些度量来找到越来越好的超参数。当我们这么做的时候，一般都需要花费更多时间来发现由于超参数改变带来的影响，这样就可以一步步减少监控的频率。 所有这些作为一种宽泛的策略看起来很有前途。然而，我想要回到寻找超参数的原点。实际上，即使是上面的讨论也传达出过于乐观的观点。实际上，很容易会遇到神经网络学习不到任何知识的情况。你可能要花费若干天在调整参数上，仍然没有进展。 所以我想要再重申一下在前期你应该从实验中尽可能早的获得快速反馈。 直觉上看，这看起来简化问题和架构仅仅会降低你的效率。实际上，这样能够将进度加快，因为你能够更快地找到传达出有意义的信号的网络。一旦你获得这些信号，你可以尝尝通过微调超参数获得快速的性能提升。这和人生中很多情况一样，万事开头难。 好了，上面就是宽泛的策略。现在我们看看一些具体的设置超参数的推荐。我会聚焦在学习率 $\eta$，L2 regularization参数 $\lambda$，和小批量数据大小。然而，很多的观点同样可以应用在其他的超参数的选择上，包括一些关于网络架构的、其他类型的regularization和一些本书后面遇到的如 momentum co-efficient 这样的超参数。 学习速率（Learning rate）假设我们运行了三个不同学习速率（$\eta=0.025$、$\eta=0.25$、$\eta=2.5$）的 MNIST 网络。我们会像前面介绍的实验那样设置其他的超参数，进行 $30$ 回合，minibatch 大小为 $10$，然后 $\lambda = 5.0$。我们同样会使用整个 $50,000$ 幅训练图像。下面是一副展示了训练代价的变化情况的图： 使用 $\eta=0.025$，代价函数平滑下降到最后的回合。使用 $\eta=0.25$，代价刚开始下降，在大约 $20$ 回合后接近饱和状态，后面就是微小的震荡和随机抖动。最终使用 $\eta=2.5$ 代价从始至终都震荡得非常明显。为了理解震荡的原因，回想一下随机梯度下降其实是期望我们能够逐渐地抵达代价函数的谷底的， 然而，如果 $\eta$ 太大的话，步长也会变大可能会使得算法在接近最小值时候又越过了谷底。这在 $\eta=2.5$ 时非常可能发生。当我们选择 $\eta=0.25$ 时，初始几步将我们带到了谷底附近，但一旦到达了谷底，又很容易跨越过去。而在我们选择 $\eta=0.025$ 时，在前 $30$ 回合的训练中不再受到这个情况的影响。当然，选择太小的%学习速率，也会带来另一个题,\gls*{sgd}算法变慢了。一种更加好的策略其实是，在开始时使用 $\eta=0.25$，随着越来越接近谷底，就换成$\eta=0.025$。这种可变学习速率的方法我们后面会介绍。现在，我们就聚焦在找出一个单独的好的学习速率的选择，$\eta$。 所以，有了这样的想法，我们可以如下设置 $\eta$。 首先，我们选择在训练数据上的代价立即开始下降而非震荡或者增加时作为 $\eta$ 的阈值的估计。这个估计并不需要太过精确。你可以估计这个值的量级，比如说从 $\eta=0.01$ 开始。如果代价在训练的前面若干回合开始下降，你就可以逐步地尝试 $\eta=0.1, 1.0,…$，直到你找到一个 $\eta$ 的值使得在开始若干回合代价就开始震荡或者增加。 相反，如果代价在 $\eta=0.01$ 时就开始震荡或者增加，那就尝试 $\eta=0.001, 0.0001,…$ 直到你找到代价在开始回合就下降的设定。按照这样的方法，我们可以掌握学习速率的阈值的量级的估计。你可以选择性地优化估计，选择那些最大的 $\eta$，比方说 $\eta=0.5$ 或者 $\eta=0.2$（这里也不需要过于精确）。 显然，$\eta$ 实际值不应该比阈值大。实际上，如果 $\eta$ 的值重复使用很多回合的话，你更应该使用稍微小一点的值，例如，阈值的一半这样的选择。这样的选择能够允许你训练更多的回合，不会减慢学习的速度。 在 MNIST 数据中，使用这样的策略会给出一个关于学习速率 $\eta$ 的一个量级的估计，大概是 $0.1$。在一些改良后，我们得到了阈值 $\eta=0.5$。所以，我们按照刚刚的取一半的策略就确定了学习速率为 $\eta=0.25$。实际上，我发现使用 $\eta=0.5$ 在 $30$ 回合内表现是很好的，所以选择更低的学习速率，也没有什么问题。 为什么使用代价函数来选择学习速率？而不是验证集这看起来相当直接。然而，使用训练代价函数来选择 $\eta$ 看起来和我们之前提到的通过验证集来确定超参数的观点有点矛盾。实际上，我们会使用验证准确率来选择\正则化超参数，minibatch 大小，和层数及隐藏元个数这些网络参数，等等。为何对学习速率要用不同的方法呢？坦白地说，这些选择其实是我个人美学偏好，个人习惯罢了。原因就是其他的超参数倾向于提升最终的测试集上的分类准确率，所以将他们通过验证准确率来选择更合理一些。然而，学习速率仅仅是偶然地影响最终的分类准确率的。学习率主要的目的是控制梯度下降的步长，监控训练代价是最好的检测步长过大的方法。 所以，这其实就是个人的偏好。在学习的前期，如果验证准确率提升，训练代价通常都在下降。所以在实践中使用那种衡量方式并不会对判断的影响太大。 学习速率调整我们一直都将学习速率设置为常量。但是，通常采用可变的学习速率更加有效。在学习的前期，权重可能非常糟糕。所以最好是使用一个较的学习速率让权重变化得更快。越往后，我们可以降低学习速率，这样可以作出更加精良的调整。 我们要如何设置学习速率呢？其实有很多方法。一种自然的观点是使用提前终止的想法。就是保持学习速率为一个常量直到验证准确率开始变差。然后按照某个量下降学习速率，比如说按照$10$ 或者 $2$。我们重复此过程若干次，直到%学习速率是初始值的 $1/1024$（或者$1/1000$）。那时就终止。 可变学习速率可以提升性能，但是也会产生大量可能的选择。这些选择会让人头疼~——~你可能需要花费很多精力才能优化学习规则。对刚开始实验，我建议使用单一的常量作为学习速率的选择。这会给你一个比较好的近似。后面，如果你想获得更好的性能，值得按照某种规则进行实验，根据我已经给出的资料。 提前停止 来确定训练的周期的数量正如我们在本章前面讨论的那样，提前停止表示在每个回合的最后，我们都要计算验证集上的分类准确率。当准确率不再提升，就终止它。这让选择回合数变得很简单。特别地，也意味着我们不再需要担心显式地掌握回合数和其他超参数的关联。而且，这个过程还是自动的。另外，提前停止也能够帮助我们避免过拟合。尽管在实验前期不采用提前停止，这样可以看到任何过匹配的信号，使用这些来选择正则化方法，但提前停止仍然是一件很棒的事。 我们需要再明确一下什么叫做分类准确率不再提升，这样方可实现提前停止。正如我们已经看到的，分类准确率在整体趋势下降的时候仍旧会抖动或者震荡。如果我们在准确度刚开始下降的时候就停止，那么肯定会错过更好的选择。一种不错的解决方案是如果分类准确率在一段时间内不再提升的时候终止。例如，我们要解决 MNIST 问题。如果分类准确度在近$10$ 个回合都没有提升的时候，我们将其终止。这样不仅可以确保我们不会终止得过快，也能够使我们不要一直干等直到出现提升。 这种 $10$ 回合不提升就终止的规则很适合 MNIST 问题的一开始的探索。然而，网络有时候会在很长时间内于一个特定的分类准确率附近形成平缓的局面，然后才会有提升。如果你尝试获得相当好的性能，这个规则可能就会太过激进了~——~停止得太草率。所以，我建议在你更加深入地理解网络训练的方式时，仅仅在初始阶段使用 $10$ 回合不提升规则，然后逐步地选择更久的回合，比如说：$20$ 回合不提升就终止，$50$ 回合不提升就终止，以此类推。当然，这就引入了一种新的需要优化的超参数！实践中，其实比较容易设置这个超参数来获得相当好的结果。类似地，对不同于 MNIST 的问题，$10$ 回合不提升就终止的规则会太过激进或者太过保守，这都取决于问题本身的特质。然而，进行一些小的实验，发现好的提前终止的策略还是非常简单的。 我们还没有在我们的 MNIST 实验中使用提前终止。原因是我们已经比较了不同的学习观点。这样的比较其实比较适合使用同样的训练回合。但是，在 network2.py 中实现提前终止还是很有价值的： 小批量数据的大小我们应该如何设置小批量数据的大小？为了回答这个问题，让我们先假设正在进行在线学习，也就是说使用大小为 $1$ 的%小批量数据。 一个关于在线学习的担忧是使用只有一个样本的小批量数据会带来关于梯度的错误估计。实际上，误差并不会真的产生这个问题。原因在于单一的梯度估计不需要绝对精确。我们需要的是确保代价函数保持下降的足够精确的估计。就像你现在要去北极点，但是只有一个不大精确的（差个 $10-20$ 度）指南针。如果你不断频繁地检查指南针，指南针会在平均状况下给出正确的方向，所以最后你也能抵达北极点。 基于这个观点，这看起来好像我们需要使用在线学习。实际上，情况会变得更加复杂。在上一章的问题中我指出我们可以使用矩阵技术来对所有在小批量数据中的样本同时计算梯度更新，而不是进行循环。所以，取决于硬件和线性代数库的实现细节，这会比循环方式进行梯度更新快好多。也许是 $50$ 和 $100$ 倍的差别。 现在，看起来这对我们帮助不大。我们使用 $100$ 的小批量数据的学习规则如下; w \rightarrow w' = w-\eta \frac{1}{100} \sum_x \nabla C_x这里是对小批量数据中所有训练样本求和。而在线学习是 w \rightarrow w' = w-\eta \nabla C_x即使它仅仅是 $50$ 倍的时间，结果仍然比直接在线学习更好，因为我们在线学习更新得太过频繁了。假设，在小批量数据下，我们将学习率扩大了 $100$ 倍，更新规则就是 w \rightarrow w' = w-\eta \sum_x \nabla C_x这看起来像做了 $100$ 次独立的在线学习。但是仅仅花费了 $50$ 次在线学习的时间。当然，其实不是同样的 100 次在线学习，因为小批量数据中 $\nabla C_x$ 是都对同样的权重进行衡量的，而在线学习中是累加的学习。使用更大的小批量数据看起来还是显著地能够进行训练加速的。 所以，选择最好的小批量数据大小也是一种折衷。太小了，你不会用上很好的矩阵库的快速计算。太大，你是不能够足够频繁地更新权重的。你所需要的是选择一个折衷的值，可以最大化学习的速度。 幸运的是，小批量数据大小的选择其实是相对独立的一个超参数（网络整体架构外的参数），所以你不需要优化那些参数来寻找好的%小批量数据大小。因此，可以选择的方式就是使用某些可以接受的值（不需要是最优的）作为其他参数的选择，然后进行不同小批量数据大小的尝试，像上面那样调整 $\eta$。画出验证准确率的值随时间（非回合）变化的图，选择哪个得到最快性能的提升的小批量数据大小。得到了小批量数据大小，也就可以对其他的超参数进行优化了。 当然，你也发现了，我这里并没有做到这么多。实际上，我们的实现并没有使用到%小批量数据更新快速方法。就是简单使用了小批量数据大小为 $10$。所以，我们其实可以通过降低小批量数据大小来进行提速。我也没有这样做，因为我希望展示小批量数据大于$1$ 的使用，也因为我实践经验表示提升效果其实不明显。在实践中，我们大多数情况肯定是要实现更快的小批量数据更新策略，然后花费时间精力来优化小批量数据大小，来达到总体的速度提升。 自动技术：我已经给出很多在手动进行超参数优化时的启发式规则。手动选择当然是种理解网络行为的方法。不过，现实是，很多工作已经使用自动化过程进行。通常的技术就是网格搜索，可以系统化地对超参数的参数空间的网格进行搜索。网格搜索的成就和限制（易于实现的变体）在 James Bergstra 和 YoshuaBengio $2012$年的论文作者为 James Bergstra 和 Yoshua Bengio（2012）。中已经给出了综述。很多更加精细的方法也被大家提出来了。 总结跟随上面的经验并不能帮助你的网络给出绝对最优的结果。但是很可能给你一个好的开始和一个改进的基础。特别地，我已经非常独立地讨论了超参数的选择。实践中，超参数之间存在着很多关系。你可能使用 $\eta$ 进行试验，发现效果不错，然后去优化 $\lambda$，发现这里又和 $\eta$ 混在一起了。在实践中，一般是来回往复进行的，最终逐步地选择到好的值。总之，启发式规则其实都是经验，不是金规玉律。你应该注意那些没有效果的尝试的信号，然后乐于尝试更多试验。特别地，这意味着需要更加细致地监控神经网络的行为，特别是验证集上的准确率。 选择超参数的难度在于如何选择超参数的方法太分散, 这些方法分散在许多的研究论文，软件程序甚至仅仅在一些研究人员的大脑中, 因而变得更加困难。很多很多的论文给出了（有时候矛盾的）建议。 设定超参数的挑战让一些人抱怨神经网络相比较其他的机器学习算法需要大量的工作进行参数选择。我也听到很多不同的版本：“的确，参数完美的神经网络可能会在这问题上获得最优的性能。但是，我可以尝试一下随机森林（或者 SVM 或者……这里脑补自己偏爱的技术）也能够工作的。我没有时间搞清楚那个最好的神经网络。” 当然，从一个实践者角度，肯定是应用更加容易的技术。这在你刚开始处理某个问题时尤其如此，因为那时候，你都不确定一个机器学习算法能够解决那个问题。但是，如果获得最优的性能是最重要的目标的话，你就可能需要尝试更加复杂精妙的知识的方法了。如果机器学习总是简单的话那是太好不过了，但也没有什么理由说机器学习非得这么简单。 参考文献[1] Michael Nielsen. CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-28. [2] Zhu Xiaohu. Zhang Freeman. Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-28.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>超参数</tag>
        <tag>Broad strategy</tag>
        <tag>Learning rate</tag>
        <tag>Early stopping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——代码实现]]></title>
    <url>%2F2018%2F06%2F27%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 再看手写识别问题：代码让我们实现本章讨论过的这些想法（交叉熵、正则化、初始化权重等等）。我们将写出一个新的程序，network2.py，这是一个对中开发的 network.py 的改进版本。如果你没有仔细看过 network.py，那你可能会需要重读前面关于这段代码的讨论。仅仅 $74$ 行代码，也很易懂。对 network.py 的详细讲解见第一章的 使用神经网络识别手写数字——代码实现 和 network.py 一样，主要部分就是 Network 类了，我们用这个来表示神经网络。使用一个 sizes 的列表来对每个对应层进行初始化，默认使用交叉熵作为代价 cost 参数： 1234567class Network(object): def __init__(self, sizes, cost=CrossEntropyCost): self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost 最开始几行里的 方法的和 network.py 中一样，可以轻易弄懂。但是下面两行是新的，我们需要知道他们到底做了什么。12345678910### 权重初始化我们先看看 default_weight_initializer 方法，使用了我们新式改进后的初始权重方法。如我们已经看到的，使用了均值为 $0$ 而标准差为 $1/\sqrt&#123;n&#125;$，$n$ 为对应的输入连接个数。我们使用均值为 $0$ 而标准差为 $1$ 的高斯分布来初始化偏置。下面是代码：```Pythondef default_weight_initializer(self): self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] 为了理解这段代码，需要知道 np 就是进行线性代数运算的 Numpy 库。我们在程序的开头会 import Numpy。同样我们没有对第一层的神经元的偏置进行初始化。因为第一层其实是输入层，所以不需要引入任何的偏置。我们在 network.py 中做了完全一样的事情。 作为 default_weight_initializer 的补充，我们同样包含了一个 large_weight_initializer 方法。这个方法使用了第一章中的观点初始化了 权重和偏置。代码也就仅仅是和 default_weight_initializer 差了一点点了： 1234def large_weight_initializer(self): self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] 我将 larger_weight_initializer 方法包含进来的原因也就是使得跟第一章的结果更容易比较。我并没有考虑太多的推荐使用这个方法的实际情景。 交叉熵初始化方法 ``` 中的第二个新的东西就是我们初始化了 cost 属性。为了理解这个工作的原理，让我们看一下用来表示交叉熵代价的类1234567891011```Pythonclass CrossEntropyCost(object): @staticmethod def fn(a, y): return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): return (a-y) 让我们分解一下。第一个看到的是：即使使用的是交叉熵，数学上看，就是一个函数，这里我们用 Python 的类而不是 Python 函数实现了它。为什么这样做呢？答案就是代价函数在我们的网络中扮演了两种不同的角色。明显的角色就是代价是输出激活值 $a$ 和目标输出 $y$ 差距优劣的度量。这个角色通过 CrossEntropyCost.fn 方法来扮演。（注意，np.nan_to_num 调用确保了 Numpy 正确处理接近 $0$ 的对数值）但是代价函数其实还有另一个角色。回想第二章中运行反向传播算法时，我们需要计算网络输出误差，$\delta^L$。这种形式的输出误差依赖于 代价函数的选择：不同的代价函数，输出误差的形式就不同。对于交叉熵函数，输出误差就如公式所示： \delta^L = a^L-y类似地，network2.py 还包含了一个表示二次代价函数的类。这个是用来和第一章的结果进行对比的，因为后面我们几乎都在使用交叉函数。代码如下。QuadraticCost.fn 方法是关于网络输出 $a$ 和目标输出 $y$ 的二次代价函数的直接计算结果。由 QuadraticCost.delta! 返回的值基于二次代价函数的误差表达式，我们在第二章中得到它。 123456789class QuadraticCost(object): @staticmethod def fn(a, y): return 0.5*np.linalg.norm(a-y)**2 @staticmethod def delta(z, a, y): return (a-y) * sigmoid_prime(z) Network2.py 完整代码现在，我们理解了 network2.py 和 network.py 两个实现之间的主要差别。都是很简单的东西。还有一些更小的变动，下面我们会进行介绍，包含 L2 正则化的实现。在讲述正则化之前，我们看看 network2.py 完整的实现代码。你不需要太仔细地读遍这些代码，但是对整个结构尤其是文档中的内容的理解是非常重要的，这样，你就可以理解每段程序所做的工作。当然，你也可以随自己意愿去深入研究！如果你迷失了理解，那么请读读下面的讲解，然后再回到代码中。不多说了，给代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315"""network2.py~~~~~~~~~~~~~~An improved version of network.py, implementing the stochasticgradient descent learning algorithm for a feedforward neural network.Improvements include the addition of the cross-entropy cost function,regularization, and better initialization of network weights. Notethat I have focused on making the code simple, easily readable, andeasily modifiable. It is not optimized, and omits many desirablefeatures."""#### Libraries# Standard libraryimport jsonimport randomimport sys# Third-party librariesimport numpy as np#### Define the quadratic and cross-entropy cost functionsclass QuadraticCost(object): @staticmethod def fn(a, y): """Return the cost associated with an output ``a`` and desired output ``y``. """ return 0.5*np.linalg.norm(a-y)**2 @staticmethod def delta(z, a, y): """Return the error delta from the output layer.""" return (a-y) * sigmoid_prime(z)class CrossEntropyCost(object): @staticmethod def fn(a, y): """Return the cost associated with an output ``a`` and desired output ``y``. Note that np.nan_to_num is used to ensure numerical stability. In particular, if both ``a`` and ``y`` have a 1.0 in the same slot, then the expression (1-y)*np.log(1-a) returns nan. The np.nan_to_num ensures that that is converted to the correct value (0.0). """ return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): """Return the error delta from the output layer. Note that the parameter ``z`` is not used by the method. It is included in the method's parameters in order to make the interface consistent with the delta method for other cost classes. """ return (a-y)#### Main Network classclass Network(object): def __init__(self, sizes, cost=CrossEntropyCost): """The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using ``self.default_weight_initializer`` (see docstring for that method). """ self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost def default_weight_initializer(self): """Initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1 over the square root of the number of weights connecting to the same neuron. Initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers. """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def large_weight_initializer(self): """Initialize the weights using a Gaussian distribution with mean 0 and standard deviation 1. Initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers. This weight and bias initializer uses the same approach as in Chapter 1, and is included for purposes of comparison. It will usually be better to use the default weight initializer instead. """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def feedforward(self, a): """Return the output of the network if ``a`` is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0, evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): """Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory, as is the regularization parameter ``lmbda``. The method also accepts ``evaluation_data``, usually either the validation or test data. We can monitor the cost and accuracy on either the evaluation data or the training data, by setting the appropriate flags. The method returns a tuple containing four lists: the (per-epoch) costs on the evaluation data, the accuracies on the evaluation data, the costs on the training data, and the accuracies on the training data. All values are evaluated at the end of each training epoch. So, for example, if we train for 30 epochs, then the first element of the tuple will be a 30-element list containing the cost on the evaluation data at the end of each epoch. Note that the lists are empty if the corresponding flag is not set. """ if evaluation_data: n_data = len(evaluation_data) n = len(training_data) evaluation_cost, evaluation_accuracy = [], [] training_cost, training_accuracy = [], [] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch( mini_batch, eta, lmbda, len(training_data)) print "Epoch %s training complete" % j if monitor_training_cost: cost = self.total_cost(training_data, lmbda) training_cost.append(cost) print "Cost on training data: &#123;&#125;".format(cost) if monitor_training_accuracy: accuracy = self.accuracy(training_data, convert=True) training_accuracy.append(accuracy) print "Accuracy on training data: &#123;&#125; / &#123;&#125;".format( accuracy, n) if monitor_evaluation_cost: cost = self.total_cost(evaluation_data, lmbda, convert=True) evaluation_cost.append(cost) print "Cost on evaluation data: &#123;&#125;".format(cost) if monitor_evaluation_accuracy: accuracy = self.accuracy(evaluation_data) evaluation_accuracy.append(accuracy) print "Accuracy on evaluation data: &#123;&#125; / &#123;&#125;".format( self.accuracy(evaluation_data), n_data) print return evaluation_cost, evaluation_accuracy, \ training_cost, training_accuracy def update_mini_batch(self, mini_batch, eta, lmbda, n): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the learning rate, ``lmbda`` is the regularization parameter, and ``n`` is the total size of the training data set. """ nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def accuracy(self, data, convert=False): """Return the number of inputs in ``data`` for which the neural network outputs the correct result. The neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation. The flag ``convert`` should be set to False if the data set is validation or test data (the usual case), and to True if the data set is the training data. The need for this flag arises due to differences in the way the results ``y`` are represented in the different data sets. In particular, it flags whether we need to convert between the different representations. It may seem strange to use different representations for the different data sets. Why not use the same representation for all three data sets? It's done for efficiency reasons -- the program usually evaluates the cost on the training data and the accuracy on other data sets. These are different types of computations, and using different representations speeds things up. More details on the representations can be found in mnist_loader.load_data_wrapper. """ if convert: results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data] else: results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) def total_cost(self, data, lmbda, convert=False): """Return the total cost for the data set ``data``. The flag ``convert`` should be set to False if the data set is the training data (the usual case), and to True if the data set is the validation or test data. See comments on the similar (but reversed) convention for the ``accuracy`` method, above. """ cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) cost += 0.5*(lmbda/len(data))*sum( np.linalg.norm(w)**2 for w in self.weights) return cost def save(self, filename): """Save the neural network to the file ``filename``.""" data = &#123;"sizes": self.sizes, "weights": [w.tolist() for w in self.weights], "biases": [b.tolist() for b in self.biases], "cost": str(self.cost.__name__)&#125; f = open(filename, "w") json.dump(data, f) f.close()#### Loading a Networkdef load(filename): """Load a neural network from the file ``filename``. Returns an instance of Network. """ f = open(filename, "r") data = json.load(f) f.close() cost = getattr(sys.modules[__name__], data["cost"]) net = Network(data["sizes"], cost=cost) net.weights = [np.array(w) for w in data["weights"]] net.biases = [np.array(b) for b in data["biases"]] return net#### Miscellaneous functionsdef vectorized_result(j): """Return a 10-dimensional unit vector with a 1.0 in the j'th position and zeroes elsewhere. This is used to convert a digit (0...9) into a corresponding desired output from the neural network. """ e = np.zeros((10, 1)) e[j] = 1.0 return edef sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) L1 、L2 正则化有个更加有趣的变动就是在代码中增加了 L2 正则化。尽管这是一个主要的概念上的变动，在实现中其实相当简单。对大部分情况，仅仅需要传递参数 lmbda 到不同的方法中，主要是 Network.SGD 方法。实际上的工作就是一行代码的事在 Network.update_mini_batch 的倒数第四行。这就是我们改动梯度下降规则来进行权重下降的地方。尽管改动很小，但其对结果影响却很大！ 12self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] 上面代码仅仅是下面 L2 正则化计算式子的简单代码翻译， w = \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial C_0}{\partial w}同样的道理根据 L1 正则化的计算式子可以直接翻译成代码， w =w-\frac{\eta \lambda}{n} sgn(w) - \eta \frac{\partial C_0}{\partial w}代码就是 12self.weights = [w-eta*(lmbda/n)*np.sign(w)-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] 其实这种情况在神经网络中实现一些新技术的常见现象。我们花费了近千字的篇幅来讨论正则化。概念的理解非常微妙困难。但是添加到程序中的时候却如此简单。精妙复杂的技术可以通过微小的代码改动就可以实现了。 Network2.py 构建神经网络的其它注意事项另一个微小却重要的改动是随机梯度下降方法的几个标志位的增加。这些标志位让我们可以对在代价和准确率的监控变得可能。这些标志位默认是 False! 的，但是在我们例子中，已经被置为 True! 来监控 Network 的性能。另外，network2.py 中的 Network.SGD 方法返回了一个四元组来表示监控的结果。我们可以这样使用： 12345678&gt;&gt;&gt; evaluation_cost, evaluation_accuracy,... training_cost, training_accuracy = net.SGD(training_data, 30, 10, 0.5,... lmbda = 5.0,... evaluation_data=validation_data,... monitor_evaluation_accuracy=True,... monitor_evaluation_cost=True,... monitor_training_accuracy=True,... monitor_training_cost=True) 所以，比如 evaluation_cost 将会是一个 $30$ 个元素的列表其中包含了每个周期在验证集合上的代价函数值。这种类型的信息在理解网络行为的过程中特别有用。比如，它可以用来画出展示网络随时间学习的状态。其实，这也是我在前面的章节中展示性能的方式。然而要注意的是如果任何标志位都没有设置的话，对应的元组中的元素就是空列表。 另一个增加项就是在 Network.save 方法中的代码，用来将 Network 对象保存在磁盘上，还有一个载回内存的函数。这两个方法都是使用 JSON 进行的，而非 Python 的 pickle 或者 cPickle 块。这些通常是 Python 中常见的保存和装载对象的方法。使用 JSON 的原因是，假设在未来某天，我们想改变 Network 类来允许非 sigmoid 的神经元。对这个改变的实现，我们最可能是改变在 Network.__init__ 方法中定义的属性。如果我们简单地 pickle 对象，会导致 load 函数出错。使用 JSON 进行序列化可以显式地让老的 Network 仍然能够 load。 其他也还有一些微小的变动。但是那些只是 network.py 的微调。结果就是把程序从 $74$ 行增长到了 $152$ 行。 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-27. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-27. [4] skylook. neural-networks-and-deep-learning, network2.py[DB/OL]. https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py, 2018-06-27.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>手写数字识别</tag>
        <tag>代码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络的反向传播算法]]></title>
    <url>%2F2018%2F06%2F26%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[阅读本篇前，最好先阅读多层感知机的反向传播算法。反向传播的基础知识和符号定义都在该文章中。 卷积神经网络：前向传播算法CNN输入层前向传播到卷积层输入层的前向传播是CNN前向传播算法的第一步。一般输入层对应的都是卷积层。 我们这里还是以图像识别为例。先考虑最简单的，样本都是二维的黑白图片。这样输入层$X$就是一个矩阵，矩阵的值等于图片的各个像素位置的值。这时和卷积层相连的卷积核$W$就也是矩阵如果样本都是有 RGB 的彩色图片，这样输入 $X$ 就是 3 个矩阵，即分别对应 R，G 和 B 的矩阵，或者说是一个张量。这时和卷积层相连的卷积核$W$就也是张量，对应的最后一维的维度为 3 .即每个卷积核都是 3 个子矩阵组成。同样的方法，对于 3D 的彩色图片之类的样本，我们的输入 $X$ 可以是 4 维，5 维的张量，那么对应的卷积核 $W$ 也是个高维的张量。 不管维度多高，对于我们的输入，前向传播的过程可以表示为：a^2= \sigma(z^2) = \sigma(a^1*W^2 +b^2) 其中，上标代表层数，星号代表卷积，而b代表我们的偏倚, $\sigma$为激活函数，这里一般都是ReLU。和多层感知机的前向传播比较一下，其实形式非常的像，只是我们这儿是张量的卷积，而不是矩阵的乘法。同时由于$W$是张量，那么同样的位置，$W$参数的个数就比多层感知机多很多了。 为了简化我们的描述，本文后面如果没有特殊说明，我们都默认输入是3维的张量，即用RBG可以表示的彩色图片。 这里需要我们自己定义的CNN模型参数是： 一般我们的卷积核不止一个，比如有K个，那么我们输入层的输出，或者说第二层卷积层的对应的输入就K个。 卷积核中每个子矩阵的的大小，一般我们都用子矩阵为方阵的卷积核，比如FxF的子矩阵。 填充padding（以下简称P），我们卷积的时候，为了可以更好的识别边缘，一般都会在输入矩阵在周围加上若干圈的0再进行卷积，加多少圈则P为多少。 步幅stride（以下简称S），即在卷积过程中每次移动的像素距离大小。 隐藏层前向传播到卷积层现在我们再来看普通隐藏层前向传播到卷积层时的前向传播算法。 假设隐藏层的输出是M个矩阵对应的三维张量，则输出到卷积层的卷积核也是M个子矩阵对应的三维张量。这时表达式和输入层的很像，也是 a^l= \sigma(z^l) = \sigma(a^{l-1}*W^l +b^l)其中，上标代表层数，星号代表卷积，而 $b$ 代表我们的偏倚, $\sigma$ 为激活函数，这里一般都是ReLU。 也可以写成M个子矩阵子矩阵卷积后对应位置相加的形式，即： a^l= \sigma(z^l) = \sigma(\sum\limits_{k=1}^{M}z_k^l) = \sigma(\sum\limits_{k=1}^{M}a_k^{l-1}*W_k^l +b^l)和CNN输入层前向传播到卷积层的区别仅仅在于，这里的输入是隐藏层来的，而不是我们输入的原始图片样本形成的矩阵。 隐藏层前向传播到池化层池化层的处理逻辑是比较简单的，我们的目的就是对输入的矩阵进行缩小概括。比如输入的若干矩阵是NxN维的，而我们的池化大小是 $k \times k$的区域，则输出的矩阵都是 $\frac{N}{k} \times \frac{N}{k}$ 维的。 这里需要需要我们定义的CNN模型参数是： !. 池化区域的大小 k 池化的标准，一般是 MAX 或者 Average。 隐藏层前向传播到全连接层由于全连接层就是普通的 多层感知机 模型结构，因此我们可以直接使用 多层感知机 的前向传播算法逻辑，即： a^l = \sigma(z^l) = \sigma(W^la^{l-1} + b^l)这里的激活函数一般是 sigmoid 或者 tanh。 经过了若干全连接层之后，最后的一层为Softmax输出层。此时输出层和普通的全连接层唯一的区别是，激活函数是softmax函数。 这里需要需要我们定义的CNN模型参数是： 全连接层的激活函数 全连接层各层神经元的个数 CNN前向传播算法小结有了上面的基础，我们现在总结下CNN的前向传播算法。 输入：1 个图片样本，CNN 模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小 K，卷积核子矩阵的维度 F，填充大小 P，步幅 S。对于池化层，要定义池化区域大小 k 和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。 输出：CNN模型的输出$a^L$ 根据输入层的填充大小P，填充原始图片的边缘，得到输入张量$a^1$。 初始化所有隐藏层的参数$W,b$ for $l$=2 to $L-1$:3.1 如果第$l$层是卷积层,则输出为 a^l= ReLU(z^l) = ReLU(a^{l-1}*W^l +b^l)3.2 如果第$l$层是池化层,则输出为$ a^l= pool(a^{l-1})$, 这里的pool指按照池化区域大小k和池化标准将输入张量缩小的过程。3.3 如果第$l$层是全连接层,则输出为 a^l= \sigma(z^l) = \sigma(W^la^{l-1} +b^l) 对于输出层第L层: a^L= softmax(z^L) = softmax(W^La^{L-1} +b^L) 以上就是 CNN 前向传播算法的过程总结。有了 CNN 前向传播算法的基础，我们后面再来理解CNN的反向传播算法就简单多了。 卷积神经网络：反向传播算法计算池化层的误差 $\delta^l$ {\delta ^l} = {upsample}\left( \delta ^{l + 1} \right) \odot \sigma '\left( \right)池化层用于削减数据量，在这一层上前向传播的数据会有损失，则在反向传播时，传播来的梯度也会有所损失。一般来说，池化层没有参数，于是仅需要计算梯度反向传播的结果。 池化层的反向传播的方法是upsample，先将矩阵还原成原大小，之后： 对于最大值池化，将梯度放置于每个池化区域取得最大值的位置，其他位置为0 对于平均值池化，则把的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置(在池化的正向计算过程中需记录每个小区域内最大值的位置) 下面是一个upsample的例子： 例如对于矩阵： \left( \begin{array}{} 1& 2 \\ 3& 4 \end{array} \right)假设经过2*2的池化，还原为原来大小： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&1& 2&0 \\ 0&3&4&0 \\ 0&0&0&0 \end{array} \right)若是最大值池化，假设每个窗口的最大值位置都是左上，则传播结果为： \left( \begin{array}{ccc} 1&0&2&0 \\ 0&0& 0&0 \\ 3&0&4&0 \\ 0&0&0&0 \end{array} \right)若是经过平均值池化，则传播结果为： \left( \begin{array}{ccc} 0.25&0.25&0.5&0.5 \\ 0.25&0.25& 0.5&0.5 \\ 0.75&0.75&1&1 \\ 0.75&0.75&1&1 \end{array} \right)卷积层的误差 $\delta^l$\delta^{l-1} = \delta^{l}*rot180(w^{l}) \odot \sigma'(z^{l-1})其中 $rot180(w^{l})$ 为卷积核旋转 180 度的函数，* 为卷积符号。 举一个例子来说，对于以下卷积等式： \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&w_{12}\\ w_{21}&w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)对于 $a_11$，有 $z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}$ ，仅 $z_11$ 与其有关，则有 $\nabla a_{11} = \delta_{11}w_{11}$。对于 $a_22$，所有 $z$ 项都和该数有关，有 $\nabla a_{22} = \delta_{11}w_{22} + \delta_{12}w_{21} + \delta_{21}w_{12} + \delta_{22}w_{11}$。 依次类推，可得： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&\delta_{11}& \delta_{12}&0 \\ 0&\delta_{21}&\delta_{22}&0 \\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&w_{21}\\ w_{12}&w_{11} \end{array} \right) = \left( \begin{array}{ccc} \nabla a_{11}&\nabla a_{12}&\nabla a_{13} \\ \nabla a_{21}&\nabla a_{22}&\nabla a_{23}\\ \nabla a_{31}&\nabla a_{32}&\nabla a_{33} \end{array} \right)卷积层的权重梯度 $\partial C / \partial w$对权重的梯度为： \frac{\partial C}{\partial w^{l}} = rot180(\delta^l*a^{l-1})同样对于前向传播： \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23}\ a_{31}&a_{32}&a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&w_{12}\\ w_{21}&w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)对于z，有以下： $z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}$ $z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22}$ $z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22}$ $z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22}$ 有梯度： $\nabla w_{11} = \delta_{11}a_{11} + \delta_{12}a_{12} + \delta_{21}a_{21} + \delta_{22}a_{22}$ $\nabla w_{12} = \delta_{11}a_{12} + \delta_{12}a_{13} + \delta_{21}a_{22} + \delta_{22}a_{12}$ $\nabla w_{21} = \delta_{11}a_{21} + \delta_{12}a_{22} + \delta_{21}a_{31} + \delta_{22}a_{32}$ $\nabla w_{22} = \delta_{11}a_{22} + \delta_{12}a_{23} + \delta_{32}a_{21} + \delta_{33}a_{22}$ 反向传播为： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&\delta_{11}& \delta_{12}&0 \\ 0&\delta_{21}&\delta_{22}&0 \\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13}\\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{array} \right) = \left( \begin{array}{ccc} \nabla w_{22}&\nabla w_{21}\\ \nabla w_{12}&\nabla w_{11} \end{array} \right)卷积层的权重梯度 $\partial C / \partial b$偏置梯度较为简单，为上一层梯度和： \frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v}卷积神经网络：反向传播算法思想卷积神经网络相比于多层感知机，增加了两种新的层次——卷积层与池化层。由现在我们想把同样的思想用到CNN中，很明显，CNN有些不同的地方，不能直接去套用多层感知机的反向传播算法的公式。 多层感知机反向传播的四个基本方程： 要套用多层感知机的反向传播算法到CNN，有几个问题需要解决： 池化层没有激活函数，这个问题倒比较好解决，我们可以令池化层的激活函数为 $\sigma(z)=z$，即激活后就是自己本身。这样池化层激活函数的导数为 1. 池化层在前向传播的时候，对输入进行了压缩，那么我们现在需要向前反向推导 $\delta^{l-1}$，这个推导方法和多层感知机完全不同。 卷积层是通过张量卷积，或者说若干个矩阵卷积求和而得的当前层的输出，这和多层感知机很不相同，多层感知机的全连接层是直接进行矩阵乘法得到当前层的输出。这样在卷积层反向传播的时候，上一层的 $\delta^{l-1}$ 递推计算方法肯定有所不同。 对于卷积层，由于 $w$ 使用的运算是卷积，那么从 $\delta$ 推导出该层的所有卷积核的 $w, b$ 的方式也不同。 从上面可以看出，问题1比较好解决，但是问题 2, 3, 4 就需要好好的动一番脑筋了，而问题 2, 3, 4也是解决 CNN 反向传播算法的关键所在。另外大家要注意到的是，多层感知机中的 $a^l, z^l$ 都只是一个向量，而我们 CNN 中的 $a^l, z^l$ 都是一个张量，这个张量是三维的，即由若干个输入的子矩阵组成。 下面我们就针对问题 2, 3, 4 来一步步研究CNN的反向传播算法。 在研究过程中，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。 卷积神经网络：反向传播计算公式的证明 {\delta ^l} = {upsample}\left( \delta ^{l + 1} \right) \odot \sigma '\left( {z^l} \right)\delta^{l-1} = \delta^{l}*rot180(w^{l}) \odot \sigma'(z^{l-1})\frac{\partial C}{\partial w^{l}} = rot180(\delta^l*a^{l-1})\frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v}证明 ${\delta ^l} = {upsample}\left( \delta ^{l + 1} \right) \odot \sigma ‘ \left( {z^l} \right)$我们首先解决上面的问题2，如果已知池化层的$\delta^l$，推导出上一隐藏层的$\delta^{l-1}$。 在前向传播算法时，池化层一般我们会用 MAX 或者 Average 对输入进行池化，池化的区域大小已知。现在我们反过来，要从缩小后的误差 $\delta^l$ ，还原前一次较大区域对应的误差。 在反向传播时，我们首先会把 $\delta^l$ 的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是 MAX，则把 $\delta^l$ 的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是 Average，则把 $\delta^l$ 的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做 upsample。 这样我们就得到了上一层 $\frac{\partial C}{\partial a_k^{l-1}}$的值，要得到 $\delta_k^{l-1}$： \delta_k^{l-1} = \frac{\partial C}{\partial a_k^{l-1}} \frac{\partial a_k^{l-1}}{\partial z_k^{l-1}} = upsample(\delta_k^l) \odot \sigma'(z_k^{l-1})其中，upsample函数完成了池化误差矩阵放大与误差重新分配的逻辑。 我们概括下，对于张量$\delta^{l-1}$，我们有： \delta^{l-1} = upsample(\delta^l) \odot \sigma'(z^{l-1})证明 $\delta^{l-1} = \delta^{l}*rot180(w^{l}) \odot \sigma’(z^{l-1})$对于卷积网络，前向传播公式为： a^l= \sigma(z^l) = \sigma(a^{l-1}*w^l +b^l)注意到 $ \delta^{l-1}$ 和 $\delta^l$ 的递推关系为： \delta^{l} = \frac{\partial C}{\partial z^l} = \frac{\partial C}{\partial z^{l+1}}\frac{\partial z^{l+1}}{\partial z^{l}} = \delta^{l+1}\frac{\partial z^{l+1}}{\partial z^{l}}因此要推导出 $\delta^{l-1}$ 和 $\delta^{l}$ 的递推关系，必须计算 $\frac{\partial z^{l}}{\partial z^{l-1}}$的梯度表达式。 注意到 $z^{l}$和$z^{l-1}$ 的关系为：z^l = a^{l-1}*w^l +b^l =\sigma(z^{l-1})*w^l +b^l 因此我们有： \delta^{l-1} = \delta^{l}\frac{\partial z^{l}}{\partial z^{l-1}} = \delta^{l}*rot180(W^{l}) \odot \sigma'(z^{l-1})这里的式子其实和 DNN 的类似，区别在于对于含有卷积的式子求导时，卷积核被旋转了 180 度。即式子中的 $rot180()$，翻转 180 度的意思是上下翻转一次，接着左右翻转一次。在 DNN 中这里只是矩阵的转置。那么为什么呢？由于这里都是张量，直接推演参数太多了。我们以一个简单的例子说明为啥这里求导后卷积核要翻转。 假设我们 $l-1$ 层的输出 $a^{l-1}$ 是一个 3x3 矩阵，第 $l$ 层的卷积核 $W^l$ 是一个2x2矩阵，采用 1 像素的步幅，则输出 $z^{l}$ 是一个 2x2 的矩阵。我们简化 $b^l$ 都是 $0$,则有 a^{l-1} * w^l = z^{l}我们列出 $a,W,z$ 的矩阵表达式如下： \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&w_{12}\\ w_{21}&w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)利用卷积的定义，很容易得出： $z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}$ $z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22}$ $z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22}$ $z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22}$ 接着我们模拟反向求导： \nabla a^{l-1} = \frac{\partial C}{\partial a^{l-1}} = \frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial a^{l-1}} = \delta^{l} \frac{\partial z^{l}}{\partial a^{l-1}}从上式可以看出，对于 $a^{l-1}$ 的梯度误差 $\nabla a^{l-1}$，等于第 $l$ 层的梯度误差乘以 $\frac{\partial z^{l}}{\partial a^{l-1}}$，而 $\frac{\partial z^{l}}{\partial a^{l-1}}$ 对应上面的例子中相关联的 $w$ 的值。假设我们的 $z$ 矩阵对应的反向传播误差是 $\delta_{11}, \delta_{12}, \delta_{21}, \delta_{22}$ 组成的 2x2 矩阵，则利用上面梯度的式子和 4 个等式，我们可以分别写出 $\nabla a^{l-1}$ 的 9 个标量的梯度。 比如对于 $a_{11}$ 的梯度，由于在 4 个等式中 $a_{11}$ 只和 $z_{11}$ 有乘积关系，从而我们有： \nabla a_{11} = \delta_{11}w_{11}对于 $a_{12}$ 的梯度，由于在 4 个等式中 $a_{12}$ 和 $z_{12}, z_{11}$ 有乘积关系，从而我们有： \nabla a_{12} = \delta_{11}w_{12} + \delta_{12}w_{11}同样的道理我们得到： $ \nabla a_{13} = \delta_{12}w_{12} $ $\nabla a_{21} = \delta_{11}w_{21} + \delta_{21}w_{11}$ $\nabla a_{22} = \delta_{11}w_{22} + \delta_{12}w_{21} + \delta_{21}w_{12} + \delta_{22}w_{11}$ $\nabla a_{23} = \delta_{12}w_{22} + \delta_{22}w_{12}$ $\nabla a_{31} = \delta_{21}w_{21}$ $\nabla a_{32} = \delta_{21}w_{22} + \delta_{22}w_{21}$ $\nabla a_{33} = \delta_{22}w_{22}$ 这上面9个式子其实可以用一个矩阵卷积的形式表示，即： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&\delta_{11}& \delta_{12}&0 \\ 0&\delta_{21}&\delta_{22}&0 \\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&w_{21}\\ w_{12}&w_{11} \end{array} \right) = \left( \begin{array}{ccc} \nabla a_{11}&\nabla a_{12}&\nabla a_{13} \\ \nabla a_{21}&\nabla a_{22}&\nabla a_{23}\\ \nabla a_{31}&\nabla a_{32}&\nabla a_{33} \end{array} \right)为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子求导时，卷积核要翻转180度的原因。 以上就是卷积层的误差反向传播过程。 证明 $\frac{\partial C}{\partial w^{l}} = rot180(\delta^l*a^{l-1})$ 和 $\frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v}$我们现在已经可以递推出每一层的梯度误差 $\delta^l$ 了，对于全连接层，可以按 DNN 的反向传播算法求该层 $W,b$ 的梯度，而池化层并没有 $W,b$,也不用求 $W,b$ 的梯度。只有卷积层的 $W,b$ 需要求出。 对于卷积网络，前向传播公式为： a^l= \sigma(z^l) = \sigma(a^{l-1}*w^l +b^l)又因为 \delta^l \equiv \frac{\partial C}{\partial z^l}.z^l = a^{l-1}*w^l +b^l那么 \frac{\partial C}{\partial w^l} = \frac{\partial C}{\partial z^l}\frac{\partial z^l}{\partial w^l}=\delta^l \frac{\partial z^l}{\partial w^l}= \delta^l * rot180(a^{l-1})即 \frac{\partial C}{\partial w^{l}} = \delta^l*rot180(a^{l-1})而对于b,则稍微有些特殊，因为 $\delta^l$ 是张量，而 $b$ 只是一个向量，不能像 DNN 那样直接和 $\delta^l$ 相等。通常的做法是将 $\delta^l$ 的各个子矩阵的项分别求和，得到一个误差向量，即为 $b$ 的梯度： \frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v} 参考文献[1] 月见樽. CNN的反向传播[DB/OL]. https://qiankun214.github.io/2018/02/21/CNN的反向传播/, 2018-06-26. [2] oio328Loio. 神经网络学习（三）反向（BP）传播算法（1）[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79537246, 2018-06-26. [3] oio328Loio. 神经网络学习（十二）卷积神经网络与BP算法[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79705332, 2018-06-26. [4] 刘建平Pinard. 卷积神经网络(CNN)反向传播算法[DB/OL]. http://www.cnblogs.com/pinard/p/6494810.html, 2018-06-26. [5] 刘建平Pinard. 卷积神经网络(CNN)前向传播算法[DB/OL]. http://www.cnblogs.com/pinard/p/6489633.html, 2018-06-26.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>反向传播算法</tag>
        <tag>卷积神经网络</tag>
        <tag>前向传播算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——弃权]]></title>
    <url>%2F2018%2F06%2F24%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%BC%83%E6%9D%83%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 2 Improving the way neural networks learn 弃权（Dropout）的过程弃权是一种相当激进的技术。和规范化不同，弃权技术并不依赖于代价函数的修改。而是，在弃权中，我们改变了网络本身。在介绍它为什么能工作，以及所得到的结果前，让我描述一下弃权（Dropout）基本的工作机制。 特别地，假设我们有一个训练数据 $x$ 和 对应的目标输出 $y$。通常我们会通过在网络中前向传播 $x$ ，然后进行反向传播来确定对梯度的贡献。使用弃权（Dropout）技术，这个过程就改了。我们会从随机（临时）地删除网络中的一半的隐藏神经元开始，同时让输入层和输出层的神经元保持不变。在此之后，我们会得到最终如下线条所示的网络。注意那些被弃权（Dropout）的神经元，即那些临时被删除的神经元，用虚圈表示在图中： 我们前向传播输入 $x$，通过修改后的网络，然后反向传播结果，同样通过这个修改后的网络。在一个小批量数据的若干样本上进行这些步骤后，我们对有关的权重和偏置进行更新。然后重复这个过程，首先重置弃权（Dropout）的神经元，然后选择一个新的随机的隐藏神经元的子集进行删除，估计对一个不同的小批量数据的梯度，然后更新权重和偏置。通过不断地重复，我们的网络会学到一个权重和偏置的集合。当然，这些权重和偏置也是在一半的隐藏神经元被弃权（Dropout）的情形下学到的。当我们实际运行整个网络时，是指两倍的隐藏神经元将会被激活。为了补偿这个，我们将从隐藏神经元出去的权重减半。 为什么弃权（Dropout）有效？这个弃权（Dropout）过程可能看起来奇怪，像是临时安排的。为什么我们会指望这样的方法能够行正则化呢？为了解释所发生的事，我希望你停下来想一下标准没有弃权（Dropout））的训练方式。特别地，想象一下我们训练几个不同的神经网络，都使用同一个训练数据。当然，网络可能不是从同一初始状态开始的，最终的结果也会有一些差异。出现这种情况时，我们可以使用一些平均或者投票的方式来确定接受哪个输出。例如，如果我们训练了五个网络，其中三个把一个数字分类成 “3”，那很可能它就是“3”。另外两个可能就犯了错误。这种平均的方式通常是一种强大（尽管代价昂贵）的方式来减轻过拟合。原因在于不同的网络可能会以不同的方式过拟合，平均法可能会帮助我们消除那样的过拟合。 那么这和弃权（Dropout）有什么关系呢？启发式地看，当我们弃权（Dropout）掉不同的神经元集合时，有点像我们在训练不同的神经网络。所以，弃权（Dropout）过程就如同大量不同网络的效果的平均那样。不同的网络会以不同的方式过拟合了，所以，弃权（Dropout）过的网络的效果会减轻过拟合。 一个相关的启发式解释在早期使用这项技术的论文中曾经给出：“因为神经元不能依赖其他神经元特定的存在，这个技术其实减少了复杂的互适应的神经元。所以，强制要学习那些在神经元的不同随机子集中更加健壮的特征。” 换言之，如果我们将我们的神经网络看做一个进行预测的模型的话，我们就可以将弃权（Dropout）看做是一种确保模型对于一部分证据丢失健壮的方式。这样看来，弃权（Dropout）和 L1、L2 regularization也是有相似之处的，这也倾向于更小的权重，最后让网络对丢失个体连接的场景更加健壮。 对于弃权的理解CSDN: 神经网络之dropout层 参考文献[1] Michael Nielsen.CHAPTER 2 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html. 2018-06-26. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-26. [3] 鹿往森处走. 神经网络之dropout层[DB/OL]. https://www.cnblogs.com/zyber/p/6824980.html, 2018-06-26.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>弃权（Dropout）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——正则化]]></title>
    <url>%2F2018%2F06%2F24%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 产生过度拟合和正则化的原因拥有大量的自由参数的模型能够描述特别神奇的现象。即使这样的模型能够很好的拟合已有的数据，但并不表示是一个好模型。因为这可能只是因为模型中足够的自由度使得它可以描述几乎所有给定大小的数据集，而不需要真正洞察现象的本质。所以发生这种情形时，模型对已有的数据会表现的很好，但是对新的数据很难泛化。对一个模型真正的测验就是它对没有见过的场景的预测能力。 我们用来对 MNIST 数字分类的 30 个隐藏神经元神经网络拥有将近 24,000 个参数！当然很多。我们有 100 个隐藏元的网络拥有将近 80,000 个参数，而目前最先进的深度神经网络包含百万级或者十亿级的参数。我们应当信赖这些结果么？ 让我们通过构造一个网络泛化能力很差的例子使这个问题更清晰。我们的网络有 30 个隐藏神经元，共 23,860 个参数。但是我们不会使用所有 50,000 幅 MNIST 训练图像。相反，我们只使用前 1,000 幅图像。使用这个受限的集合，会让泛化的问题突显。我们按照之前同样的方式，使用交叉熵损失函数，学习率设置为 $\eta = 0.5$ 而小批量大小设置为 $10$。不过这里我们要训练 400 个周期。我们现在使用 network2 来研究损失函数改变的情况： 12345678&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,... monitor_evaluation_accuracy=True, monitor_training_cost=True) 使用上面的结果，我们可以画出当网络学习时代价变化的情况： 这看起来令人振奋，因为损失函数有一个光滑的下降，跟我们预期一致。注意，我只是展示了 $200$ 到 $399$ 损失函数的情况。这给出了很好的近距离理解训练后期的情况，也是出现有趣现象的地方。 让我们看看分类准确率在测试集上的表现： 这里我还是聚焦到了后面的过程。 在前 200 迭代期（图中没有显示）中准确率提升到了 82%。然后学习逐渐变缓。最终，在 280 迭代期左右分类准确率就停止了增长。后面的的迭代期，仅仅看到了在 280 迭代期准确率周围随机的小波动。将这幅图和前面的图进行对比，前面的图中和训练数据相关的代价持续平滑下降。如果我们只看那个代价，会发现我们模型的表现变得“更好”。但是测试准确率展示了提升只是一种假象。就像费米不大喜欢的那个模型一样，我们的网络在 280 迭代期后就不再能够推广到测试数据上。所以这不是有用的学习。我们说网络在 280 迭代期后就过度拟合或者过度训练了。 这里引出过度拟合的概念，过度拟合即过拟合。为了得到一致假设而使假设变得过度严格称为过拟合。过拟合的一种定义：给定一个假设空间 $H$，一个假设 $h$ 属于 $H$，如果存在其他的假设 $h’$ 属于 $H$,使得在训练样例上h的错误率比 $h’$ 小，但在整个实例分布上 $h’$ 比 $h$ 的错误率小，那么就说假设 $h$ 过度拟合训练数据。 你可能想知道这里的问题是不是由于我们看的是训练数据的代价，而对比的却是测试数据上的分类准确率导致的。 换言之，可能我们这里在进行苹果和橙子的对比。如果我们比较训练数据上的代价和测试数据上的代价，会发生什么，我们是在比较类似的度量吗？或者可能我们可以比较在两个数据集上的分类准确率啊？实际上，不管我们使用什么度量的方式，尽管细节会变化，但本质上都是一样的。让我们来看看测试数据集上的代价变化情况： 我们可以看到测试集上的代价在 15 迭代期前一直在提升，随后越来越差，尽管训练数据集上的代价表现是越来越好的。这其实是另一种模型过度拟合的迹象。尽管，这里带来了关于我们应当将 15 还是 280 迭代期当作是过度拟合开始影响学习的时间点的困扰。从一个实践角度，我们真的关心的是提升测试数据集上的分类准确率，而测试集合上的代价不过是分类准确率的一个反应。所以更加合理的选择就是将 280 迭代期看成是过度拟合开始影响学习的时间点。 另一个过度拟合的迹象在训练数据上的分类准确率上也能看出来： 准确率一直在提升接近 100%。也就是说，我们的网络能够正确地对所有 $1000$ 幅图像进行分类！而在同时，我们的测试准确率仅仅能够达到 82.27%。所以我们的网络实际上在学习训练数据集的特例，而不是能够一般地进行识别。我们的网络几乎是在单纯记忆训练集合，而没有对数字本质进行理解能够泛化到测试数据集上。 过拟合是神经网络的一个主要问题。这在现代网络中特别正常，因为网络权重 $W$ 和偏置 $b$ 数量巨大。为了高效地训练，我们需要一种检测过拟合是不是发生的技术，这样我们不会过度训练。并且我们也想要找到一些技术来降低过拟合的影响。 检测过拟合的明显方法是使用上面的方法跟踪测试数据集合上的准确率随训练变化情况。如果我们看到测试数据上的准确率不再提升，那么我们就停止训练。当然，严格地说，这其实并非是过拟合的一个必要现象，因为测试集和训练集上的准确率可能会同时停止提升。当然，采用这样的策略是可以阻止过拟合的。 实际上，我们会使用这种策略的变化形式来试验。记得之前我们载入 MNIST 数据时用了三个数据集： 123&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper() 到现在我们一直在使用 training_data 和 test_data，没有用过 validation_data。validation_data 中包含了 $10,000$ 幅数字图像，这些图像和 MNIST 训练数据集中的 $50,000$ 幅图像以及测试数据集中的$10,000$ 幅都不相同。我们会使用 validation_data 而不是 test_data 来防止过拟合。我们会为 test_data 使用和上面提到的相同的策略。我们在每个迭代期·的最后都计算在 validation_data 上的分类准确率。一旦分类准确率已经饱和，就停止训练。这个策略被称为提前停止。当然，实际应用中，我们不会立即知道什么时候准确率会饱和。相反，我们会一直训练直到我们确信准确率已经饱和（这里需要一些判定标准来确定什么时候停止）。 为何要使用 validation_data 来替代 test_data 防止过拟合问题？实际上，这是一个更为一般的策略的一部分，这个一般的策略就是使用 validation_data 来衡量不同的超参数（如迭代期}，学习率，最好的网络架构等等）的选择的效果。我们使用这样方法来找到超参数的合适值。因此，尽管到现在我并没有提及这点，但其实本书前面已经稍微介绍了一些超参数选择的方法。 当然，这仍然没有回答为什么我们用 validation_data 而不是 test_data 来防止过拟合的问题。实际上，有一个更加一般的问题，就是 为何用 validation_data 取代 test_data 来设置更好的超参数？ 为了理解这点，想想当设置超参数时，我们想要尝试许多不同的超参数选择。如果我们设置超参数是基于 test_data 的话，可能最终我们就会得到 过拟合 于 test_data 的超参数。也就是说，我们可能会找到那些符合 test_data 特点的超参数，但是网络的性能并不能够泛化到其他数据集合上。我们借助 validation_data 来克服这个问题。然后一旦获得了想要的超参数，最终我们就使用 test_data 进行准确率测量。这给了我们在 test_data 上的结果是一个网络泛化能力真正的度量方式的信心。换言之，你可以将验证集看成是一种特殊的训练数据集能够帮助我们学习好的超参数。这种寻找好的超参数的方法有时候被称为 hold out 方法，因为 validation_data 是从 traning_data 训练集中留出或者“拿出”的一部分。 在实际应用中，甚至在衡量了 test_data 的性能后，我们可能也会改变想法并去尝试另外的方法，也许是一种不同的网络架构，这将会引入寻找新的超参数的过程。如果我们这样做，难道不会产生 过拟合 于 test_data 的困境么？我们是不是需要一种数据集的潜在无限回归，这样才能够确信模型能够泛化？去除这样的疑惑其实是一个深刻而困难的问题。但是对我们实际应用的目标，我们不会担心太多。相反，我们会继续采用基于 training_data，validation_data，和 test_data 的基本 Hold-Out 方法。 我们已经研究了只使用 $1,000$ 幅训练图像时的 过拟合 问题。那么如果我们使用所有的 50,000 幅图像的训练数据会发生什么？我们会保留所有其它的参数都一样（$30$ 个隐藏元，learning-rate $0.5$ mini-batch 规模为 $10$），但是 epoch为 30 次。下图展示了分类准确率在训练和测试集上的变化情况。注意我们使用的测试数据，而不是验证集合，为了让结果看起来和前面的图更方便比较。 如你所见，测试集和训练集上的准确率相比我们使用 $1,000$ 个训练数据时相差更小。特别地，在训练数据上的最佳的分类准确率 97.86% 只比测试集上的 95.33% 准确率高了1.53%。而之前的例子中，这个差距是 17.73%！过拟合仍然发生了，但是已经减轻了不少。我们的网络从训练数据上更好地泛化到了测试数据上。一般来说，最好的降低 过拟合 的方式之一就是增加训练样本的量。有了足够的训练数据，就算是一个规模非常大的网络也不大容易 过拟合。不幸的是，训练数据其实是很难或者很昂贵的资源，所以这不是一种太切实际的选择。 正则化增加训练样本的数量是一种减轻过拟合的方法。还有其他的方法能够减轻过拟合的程度吗？一种可行的方式就是降低网络的规模。然而，大的网络拥有一种比小网络更强的潜力，所以这里存在一种应用冗余性的选项。 幸运的是，还有其他的技术能够缓解过拟合，即使我们只有一个固定的网络和固定的训练集合。这种技术就是正则化。本节，我会给出一种最为常用的正则化手段，有时候被称为权重-decay 或者 L2正则化。L2正则化的想法是增加一个额外的项到损失函数上，这个项叫做正则化-term。下面是正则化的交叉熵： C = -\frac{1}{n} \sum_{xj} \left[ y_j \ln a^L_j+(1-y_j) \ln(1-a^L_j)\right] + \frac{\lambda}{2n} \sum_w w^2其中第一个项就是常规的交叉熵的表达式。第二个现在加入的就是所有权重的平方的和。然后使用一个因子 $\lambda / 2n$ 进行量化调整，其中 $\lambda &gt; 0$ 可以称为正则化参数，而 $n$ 就是训练集合的大小。我们会在后面讨论 $\lambda$ 的选择策略。需要注意的是，正则化项里面并不包偏置。这点我们后面也会再讲述。 当然，对其他的损失函数也可以进行正则化，例如二次 损失函数。类似的正则化的形式如下： C = \frac{1}{2n} \sum_x \|y-a^L\|^2 + \frac{\lambda}{2n} \sum_w w^2两者都可以写成这样： C = C_0 + \frac{\lambda}{2n} \sum_w w^2其中 $C_0$ 是原始的 损失函数。 直觉地看，正则化的效果是让网络倾向于学习小一点的权重，其他的东西都一样的。大的权重只有能够给出损失函数第一项足够的提升时才被允许。换言之，正则化可以当做一种寻找小的权重和最小化原始的损失函数之间的折中。这两部分之间相对的重要性就由 $\lambda$ 的值来控制了：$\lambda$ 越小，就偏向于最小化原始 损失函数，反之，倾向于小的权重。 现在，对于这样的折中为何能够减轻过拟合还不是很清楚！但是，实际表现表明了这点。我们会在下一节来回答这个问题。但现在，我们来看看一个正则化的确减轻过拟合的例子。 为了构造这个例子，我们首先需要弄清楚如何将随机梯度下降算法应用在一个正则化的神经网络上。特别地，我们需要知道如何计算对网络中所有权重和偏置的偏导数 $\partial C/\partial w$ 和 $\partial C/\partial b$。对方程$C = C_0 + \frac{\lambda}{2n} \sum_w w^2$进行求偏导数得： \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} w \frac{\partial C}{\partial b} = \frac{\partial C_0}{\partial b}$\partial C_0/\partial w$ 和 $\partial C_0/\partial b$ 可以通过反向传播算法进行计算，正如《反向传播算法》中描述的那样。所以我们看到其实计算正则化的损失函数的梯度是很简单的：仅仅需要反向传播，然后加上$\frac{\lambda}{n} w$ 得到所有权重的偏导数。而偏置的偏导数就不要变化，所以偏置的梯度下降学习规则不会发生变化： b \rightarrow b -\eta \frac{\partial C_0}{\partial b}weight 的学习规则就变成： w \rightarrow w-\eta \frac{\partial C_0}{\partial w}-\frac{\eta \lambda}{n} w w = \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial C_0}{\partial w}这正和通常的梯度下降学习规则相同，除了通过一个因子 $1-\frac{\eta\lambda}{n}$ 重新调整了权重$w$。这种调整有时被称为权重衰减，因为它使得权重变小。粗看，这样会导致权重会不断下降到 $0$。但是实际不是这样的，因为如果在原始损失函数中造成下降的话其他的项（比如 $\eta \frac{\partial C_0}{\partial w}$）可能会让权重增加。 好的，这就是梯度下降工作的原理。那么随机梯度下降呢？正如在没有正则化的随机梯度下降中，我们可以通过平均 $m$ 个训练样本的 mini-batch 来估计 $\partial C_0/\partialw$。因此，为了随机梯度下降的正则化学习规则就变成 w \rightarrow \left(1-\frac{\eta \lambda}{n}\right) w -\frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial w}其中后面一项是在训练样本的 mini-batch $x$ 上进行的，而 $C_x$ 是对每个训练样本的（无正则化的）代价。这其实和之前通常的随机梯度下降的规则是一样的，除了有一个权重下降的因子 $1-\frac{\eta \lambda}{n}$。最后，为了完整，我给出偏置的正则化的学习规则。这当然是和我们之前的非正则化的情形一致了， b \rightarrow b - \frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial b}这里求和也是在训练样本的 mini-batch $x$ 上进行的。 让我们看看正则化给网络带来的性能提升吧。这里还会使用有 $30$ 个隐藏神经元、 mini-batch大小为 $10$， learning-rate为 $0.5$，使用交叉熵的神经网络。然而，这次我们会使用正则化参数为 $\lambda = 0.1$。注意在代码中，我们使用的变量名字为 lmbda!，这是因为在 Python 中 lambda是关键字，有着不相关的含义。我也会再次使用 test_data，而不是 validation_data。不过严格地讲，我们应当使用 validation_data 的，因为前面已经讲过了。这里我这样做，是因为这会让结果和非正则化的结果对比起来效果更加直接。你可以轻松地调整为 validation_data，你会发现有相似的结果。 12345678910&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data[:1000], 400, 10, 0.5,... evaluation_data=test_data, lmbda = 0.1,... monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,... monitor_training_cost=True, monitor_training_accuracy=True) 训练集上的损失函数持续下降，和前面无正则化的情况一样的规律： 但是这次测试集上的准确率在整个 400 迭代期内持续增加： 显然，正则化的使用能够解决过拟合的问题。而且，准确率相当高了，最高处达到了87.1%，相较于之前的 82.27%。因此，我们几乎可以确信在 400 迭代期之后持续训练会有更加好的结果。看起来，经实践检验，正则化让网络具有更好的泛化能力，显著地减轻了过拟合的影响。 如果我们摆脱人为的仅用 1,000 个训练图像的环境，转而用所有 50,000 图像的训练集，会发生什么？当然，我们之前已经看到过拟合在大规模的数据上其实不是那么明显了。那正则化能不能起到相应的作用呢？保持超参数和之前一样，30 epoch, learning-rate 为 0.5, mini-batch 大小为 10。不过我们这里需要改变正则化参数。原因在于训练数据的大小已经从 $n=1,000$ 改成了 $n=50,000$，这个会改变权重衰减因子 $1-\frac{\eta\lambda}{n}$。如果我们持续使用 $\lambda = 0.1$ 就会产生很小的权重衰减，因此就将正则化的效果降低很多。我们通过修改为 $\lambda = 5.0$ 来补偿这种下降。 好了，来训练网络，重新初始化权重： 1234&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data, 30, 10, 0.5,... evaluation_data=test_data, lmbda = 5.0,... monitor_evaluation_accuracy=True, monitor_training_accuracy=True) 我们得到： 这个结果很不错。第一，我们在测试集上的分类准确率在使用正则化后有了提升，从95.49% 到 96.49%。这是个很大的进步。第二，我们可以看到在训练数据和测试数据上的结果之间的差距也更小了。这仍然是一个大的差距，不过我们已经显著得到了本质上的降低过拟合的进步。 最后，我们看看在使用 100 个隐藏神经元和正则化参数为 $\lambda = 5.0$ 相应的测试分类准确率。我不会给出详细分析，纯粹为了好玩，来看看我们使用一些技巧（交叉熵函数和 L2正则化）能够达到多高的准确率。 12345&gt;&gt;&gt; net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data, 30, 10, 0.5, lmbda=5.0,... evaluation_data=validation_data,... monitor_evaluation_accuracy=True) 最终在验证集上的准确率达到了 97.92%。这是比 30 个隐藏元的较大飞跃。实际上，稍微改变一点，60 epoch $\eta=0.1$ 和$\lambda = 5.0$。我们就突破了 98%，达到了98.04% 的分类准确率 98%。对于 152 行代码这个效果还真不错！ 我已经把正则化描述为一种减轻过拟合和提高分类准确率的方法。实际上，这不是仅有的好处。实践表明，在使用不同的（随机）权重初始化进行多次 MNIST 网络训练的时候，我发现无正则化的网络会偶然被限制住，明显困在了损失函数的局部最优值处。结果就是不同的运行会给出相差很大的结果。对比看来，正则化的网络能够提供更容易复制的结果。 为何会这样子？从经验上看，如果损失函数是无正则化的，那么权重向量的范数可能会增长，而其他的东西都保持一样。随着时间的推移，这会导致权重向量变得非常大。所以会使得权重向量卡在朝着更多还是更少的方向上变化，因为当范数很大的时候梯度下降带来的变化仅仅会引起在那个方向发生微小的变化。我相信这个现象让我们的学习算法更难有效地探索权重空间，最终导致很难找到损失函数的最优值。 为什么正则化可以帮助减轻过度拟合我们已经看到了正则化在实践中能够减少过拟合了。这是令人振奋的，不过，这背后的原因还不得而知！通常的说法是：小的权重在某种程度上，意味着更低的复杂性，也就对数据给出了一种更简单却更强大解释，因此应该优先选择。这虽然很简短，不过暗藏了一些可能看起来会令人困惑的因素。让我们将这个解释细化，认真地研究一下。现在给一个简单的数据集，我们为其建立模型： 这里我们其实在研究某种真实的现象，$x$ 和 $y$ 表示真实的数据。我们的目标是训练一个模型来预测 $y$ 关于 $x$ 的函数。我们可以使用神经网络来构建这个模型，但是我们先来个简单的：用一个多项式来拟合数据。这样做的原因其实是多项式相比神经网络能够让事情变得更加清楚。一旦我们理解了多项式的场景，对于神经网络可以如法炮制。现在，图中有十个点，我们就可以找到唯一的 $9$ 阶多项式 $y=a_0x^9 + a_1x^8 + … + a_9$ 来完全拟合数据。下面是多项式的图像，这里我不明确列出这些系数。 这给出了一个准确的拟合。但是我们同样也能够使用线性模型 $y=2x$ 得到一个好的拟合效果： 哪个是更好的模型？哪个更可能是真的？还有哪个模型更可能泛化到其他的拥有同样现象的样本上？ 这些都是很难回答的问题。实际上，我们如果没有更多关于真实现象背后的信息的话，并不能确定给出上面任何一个问题的答案。但是让我们考虑两种可能的情况：（1）9 阶多项式实际上是完全描述了真实情况的模型，最终它能够很好地泛化；（2）正确的模型是 $y=2x$，但是存在着由于测量误差导致的额外的噪声，使得模型不能够准确拟合。 先验假设无法说出哪个是正确的（或者，如果还有其他的情况出现）。逻辑上讲，这些都可能出现。并且这不是微不足道的差异。在给出的数据上，两个模型的表现其实是差不多的。但是假设我们想要预测对应于某个超过了图中所有的 $x$ 的 $y$ 的值，在两个模型给出的结果之间肯定有一个极大的差距，因为 9 阶多项式模型肯定会被 $x^9$ 主导，而线性模型只是线性的增长。 在科学中，一种观点是我们除非不得已应该追随更简单的解释。当我们找到一个简单模型似乎能够解释很多数据样本的时候，我们都会激动地认为发现了规律！我们怀疑模型必须表达出某些关于现象的内在的真理。如上面的例子，线性模型加噪声肯定比多项式更加可能。我们会认为线性模型加噪声表达出了一些潜在的真理。从这个角度看，多项式模型仅仅是学习到了局部噪声的影响效果。所以尽管多项式对于这些特定的数据点表现得很好。模型最终会在未知数据上的泛化上出现问题，所以噪声线性模型具有更强大的预测能力。 让我们从这个观点来看神经网络。假设神经网络大多数有很小的权重，这最可能出现在正则化的网络中。更小的权重意味着网络的行为不会因为我们随便改变了一个输入而改变太大。这会让正则化网络学习局部噪声的影响更加困难。将它看做是一种让单个的证据不会影响网络输出太多的方式。相对的， 正则化网络学习去对整个训练集中经常出现的证据进行反应。对比看，大权重的网络可能会因为输入的微小改变而产生比较大的行为改变。所以一个无正则化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型。简言之，正则化网络受限于根据训练数据中常见的模式来构造相对简单的模型，而能够抵抗训练数据中的噪声的特性影响。我们的想法就是这可以让我们的网络对看到的现象进行真实的学习，并能够根据已经学到的知识更好地进行泛化。 所以，倾向于更简单的解释的想法其实会让我们觉得紧张。人们有时候将这个想法称为“奥卡姆剃刀原则”，然后就会热情地将其当成某种科学原理来应用这个法则。但是，这就不是一个一般的科学原理。也没有任何先验的逻辑原因来说明简单的解释就比更为复杂的解释要好。实际上，有时候更加复杂的解释其实是正确的。 下面有三点提示：第一，确定两种解释中哪个“更加简单”其实是一件相当微妙的工作。第二，即使我们可以做出这样一个判断，简单性也是一个使用时需要相当小心的指导！第三，对模型真正的测试不是简单性，而是它在新场景中对新的活动中的预测能力。 所以，我们应当时时记住这一点，正则化的神经网络常常能够比非正则化的泛化能力更强，这只是一种实验事实（empirical fact）。 我已经在上面讲过了为何现在还没有一个人能够发展出一整套具有说服力的关于正则化可以帮助网络泛化的理论解释。实际上，研究者们不断地在写自己尝试不同的正则化方法，然后看看哪种表现更好，尝试理解为何不同的观点表现的更好。所以你可以将正则化看做某种任意整合的技术。尽管其效果不错，但我们并没有一套完整的关于所发生情况的理解，仅仅是一些不完备的启发式规则或者经验。 这里也有更深的问题，这个问题也是有关科学的关键问题：我们如何泛化。正则化能够给我们一种计算上的魔力帮助神经网络更好地泛化，但是并不会带来原理上理解的指导，甚至不会告诉我们什么样的观点才是最好的。 这实在是令人困扰，因为在日常生活中，我们人类在泛化上表现很好。给一个儿童几幅大象的图片，他就能快速地学会认识其他的大象。当然，他们偶尔也会搞错，很可能将一只犀牛误认为大象，但是一般说来，这个过程会相当准确。所以我们有个系统人的大脑拥有超大量的自由变量。在受到仅仅少量的训练图像后，系统学会了在其他图像上的推广。某种程度上，我们的大脑的正则化做得特别好！怎么做的？现在还不得而知。我期望若干年后，我们能够发展出更加强大的技术来正则化神经网络，最终这些技术会让神经网络甚至在小的训练集上也能够学到强大的泛化能力。 实际上，我们的网络已经比我们预先期望的要好一些了。拥有 100 个隐藏元的网络会有接近 80,000 个参数。我们的训练集仅仅有 50,000 幅图像。这好像是用一个 80,000 阶的多项式来拟合 50,000 个数据点。我们的网络肯定会过拟合得很严重。但是，这样的网络实际上却泛化得很好。为什么？这一点并没有很好地理解。这里有个猜想：梯度下降学习的动态有一种自正则化的效应。这真是一个意料之外的巧合，但也带来了对于这种现象本质无知的不安。不过，我们还是会在后面依照这种实践的观点来应用正则化技术的。神经网络也是由于这点表现才更好一些。 现在我们回到前面留下来的一个细节：L2 正则化没有限制偏置，以此作为本节的结论。当然了，对正则化的过程稍作调整就可以对偏置进行规范了。实践看来，做出这样的调整并不会对结果改变太多，所以，在某种程度上，对不对偏置进行正则化其实就是一种习惯了。然而，需要注意的是，有一个大的偏置并不会像大的权重那样会让神经元对输入太过敏感。所以我们不需要对大的偏置所带来的学习训练数据的噪声太过担心。同时，允许大的偏置能够让网络更加灵活。因为，大的偏置让神经元更加容易饱和，这有时候是我们所要达到的效果。所以，我们通常不会对偏置进行正则化。 L1 正则化L1正则化：这个方法是在未正则化 的损失函数上加上一个权重绝对值的和： C = C_0 + \frac{\lambda}{n} \sum_w |w|凭直觉地看，这和 L2正则化相似，惩罚大的权重，倾向于让网络优先选择小的权重。当然，L1正则化和 L2正则化并不相同，所以我们不应该期望从 L1正则化得到完全同样的行为。让我们来试着理解使用 L1正则化训练的网络和 L2正则化训练的网络所不同的行为。 首先，我们会研究一下 cost-func 的偏导数。对$C = C_0 + \frac{\lambda}{n} \sum_w |w|$求导我们有 \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w}+ \frac{\lambda}{n} {\rm sgn}(w)其中 ${\rm sgn}(w)$ 就是 $w$ 的正负号，即 $w$ 是正数时为 $+1$，而 $w$ 为负数时为 $-1$。使用这个表达式，我们可以轻易地对反向传播算法进行修改从而使用基于 L1正则化的随机梯度下降进行学习。对 L1正则化的网络进行更新的规则就是 w \rightarrow w' =w-\frac{\eta \lambda}{n} sgn(w) - \eta \frac{\partial C_0}{\partial w}其中和往常一样，我们可以用一个小批量数据的均值来估计 $\partial C_0/\partial w$。对比 L2 正则化的更新规则， w \rightarrow w' = w\left(1 - \frac{\eta \lambda}{n} \right)- \eta \frac{\partial C_0}{\partial w}在两种情形下正则化的效果就是缩小权重。这符合我们的直觉，两种正则化都惩罚大的权重。但权重 缩小的方式不同。在 L1正则化中，权重 通过一个常量向 $0$ 进行缩小。在 L2正则化中，权重 通过一个和 $w$ 成比例的量进行缩小的。所以，当一个特定的权重 绝对值 $|w|$ 很大时，L1正则化的权重 缩小得远比 L2正则化要小得多。相反，当一个特定的权重绝对值 $|w|$ 很小时，L1正则化的权重 缩小得要比 L2正则化大得多。最终的结果就是：L1正则化倾向于聚集网络的权重 在相对少量的高重要度连接上，而其他权重 就会被驱使向 $0$ 接近。 我在上面的讨论中其实忽略了一个问题~——~在 $w=0$ 的时候，偏导数 $\partial C/\partial w$ 未定义。原因在于函数 $|w|$ 在 $w=0$ 时有个“直角”，事实上，导数是不存在的。不过也没有关系。我们下面要做的就是应用通常的（无正则化的）随机梯度下降的规则在 $w=0$ 处。这应该不会有什么问题，凭直觉地看，正则化的效果就是缩小权重，显然，不能对一个已经是 $0$ 的权重进行缩小了。更准确地说，我们将会使用上述方程并约定 $ sgn(0) = 0$。这样就给出了一种细致又紧凑的规则来进行采用 L1 正则化的随机梯度下降学习。 L1、L2 正则化与参数估计的关系 通过上面的推导我们可以发现，最大后验估计与最大似然估计最大的不同在于p(参数)项，所以可以说最大后验估计是正好可以解决机器学习缺乏先验知识的缺点，将先验知识加入后，优化损失函数。 其实p(参数)项正好起到了正则化的作用。如：如果假设p(参数)服从高斯分布，则相当于加了一个L2 正则化；如果假设p(参数)服从拉普拉斯分布，则相当于加了一个L1 正则化。(内容引自《贝叶斯估计、最大似然估计、最大后验估计三者的区别》) 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-24. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-24. [3] oio328Loio. 神经网络学习（九）优化方法：正则化[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79597995. 2018-06-24.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>正则化</tag>
        <tag>奥卡姆剃刀原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——Softmax]]></title>
    <url>%2F2018%2F06%2F22%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94Softmax%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn Softmax 前言我们大多数情况会使用交叉熵来解决学习缓慢的问题。但是，我希望简要介绍一下另一种解决这个问题的方法，基于 softmax 神经元层。在人工神经网络（ANN）中，softmax 通常被用作输出层的激活函数。这不仅是因为它的效果好，而且因为它使得 ANN 的输出值更易于理解。同时，softmax 配合 log 似然代价函数，其训练效果也要比采用二次代价函数的方式好。 Softmax 函数性质 softmax的函数公式如下： a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},在公式中的指数确保了所有的输出激活值是正数。然后方程中分母的求和又保证了 softmax 的输出和为 $1$。这个特定的形式确保输出激活值形成一个概率分布的自然的方式。你可以将其想象成一种重新调节 $z^L_j$ 的方法，然后将这个结果整合起来构成一个概率分布。 softmax函数最明显的特点在于：它把每个神经元的输入占当前层所有神经元输入之和的比值，当作该神经元的输出。这使得输出更容易被解释：神经元的输出值越大，则该神经元对应的类别是真实类别的可能性更高。 softmax 的单调性 证明如果 $j=k$ 则 $\partial a^L_j / \partial z^L_k$ 为正，$j \neq k$ 时为负。结果是，增加 $z^L_j$ 会提高对应的输出激活值 $a^L_j$ 并降低其他所有输出激活值。单调性证明见后文。 softmax的非局部性 softmax 层的一个好处是输出 $a^L_j$ 是对应带权输入 $a^L_j = \sigma(z^L_j)$ 的函数。由于分母求和所有的 $e^{z^L_k}$ 所以计算式子中计算每一个 $a_j^L$ 都与其他 $a_j^L$ 紧密相关。深入理解就是对于 softmax 层来说：任何特定的输出激活值 $a^L_j$ 依赖所有的带权输入。 逆转softmax层 假设我们有一个使用 softmax 输出层的神经网络，然后激活值 $a^L_j$ 已知。容易证明对应带权输入的形式为 $z^L_j = \ln a^L_j + C$，其中常量 $C$ 是独立于 $j$ 的。 Softmax 解决学习缓慢问题我们现在已经对柔性最大值神经元层有了一定的认识。但是我们还没有看到一个柔性最大值层会怎么样解决学习缓慢问题。为了理解这点，让我们先定义一个对数似然函数。我们使用 $x$ 表示网络的训练输入，$y$ 表示对应的目标输出。然后关联这个训练输入的代价函数就是 C \equiv -\ln a^L_y所以，如果我们训练的是 MNIST 图像，输入为 $7$ 的图像，那么对应的对数似然代价就是 $-\ln a_7^L$。看看这个直觉上的含义，想想当网络表现很好的时候，也就是确认输入为 $7$ 的时候。这时，他会估计一个对应的概率 $a_7^L$ 跟$1$ 非常接近，所以代价 $-\ln a_7^L$ 就会很小。反之，如果网络的表现糟糕时，概率$a_7^L$ 就变得很小，代价 $-\ln a_7^L$ 随之增大。所以对数似然代价函数也是满足我们期待的代价函数的条件的。 那关于学习缓慢问题呢？为了分析它，回想一下学习缓慢的关键就是量 $\partial C /\partial w^L_{jk}$ 和 $\partial C / \partial b^L_j$ 的变化情况。这里我不会显式地给出详细的推导，但是通过一点代数运算你会得 \frac{\partial C}{\partial b^L_j} = a^L_j-y_j \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k (a^L_j-y_j)这些方程其实和我们前面对交叉熵得到的类似。而且，正如前面的分析，这些表达式确保我们不会遇到学习缓慢的问题。事实上，把一个具有对数似然代价的 softmax 输出层，看作与一个具有交叉熵代价的 S 型输出层非常相似，这是很有用的。 有了这样的相似性，你应该使用一个具有交叉熵代价的 S 型输出层，还是一个具有对数似然代价的柔性最大值输出层呢？实际上，在很多应用场景中，这两种方式的效果都不错。作为一种通用的视角，柔性最大值加上对数似然的组合更加适用于那些需要将输出激活值解释为概率的场景。那并不总是一个需要关注的问题，但是在诸如 MNIST 这种有着不重叠的分类问题上确实很有用。 数学形式证明 Softmax 有效性softmax的函数公式如下： a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},softmax在的求导结果比较特别，分为两种情况。 上文讲到，二次代价函数在训练ANN时可能会导致训练速度变慢的问题。那就是，初始的输出值离真实值越远，训练速度就越慢。这个问题可以通过采用交叉熵代价函数来解决。其实，这个问题也可以采用另外一种方法解决，那就是采用 softmax 激活函数，并采用log似然代价函数（log-likelihood cost function）来解决。 log似然代价函数的公式为： C = - \sum_i y_i log a_i注意这种情况：其中，表示第 $a_k$ 个神经元的输出值，$y_k$ 表示第 k 个神经元对应的真实值，取值为 0 或 1 。由于 $y_k$ 取值为 0 或 1 ，对于每一个样本， $y_1,y_2,..,y_k$ 只会有一个取 1 其余的都取值为0， 所以对数似然函数求和符号可以去掉，化简为 C \equiv -\ln a^L_j 为了检验 softmax 和这个代价函数也可以解决上述所说的训练速度变慢问题，接下来的重点就是推导ANN的权重 w 和偏置 b 的梯度公式。 先求损失函数对偏置b的偏导数： 当 $i=j$ 时，带入 上面的结果$\frac{\partial a^L_j}{\partial z^L_i}=a_j^L(1-a_j^L)$ \begin{aligned} \frac{\partial C}{\partial b_{j}^L} &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_i} \\ &= - \frac{1}{a^L_j} [a_j^L(1-a_j^L)] \\ &= a_j^L -1 \end{aligned} 当 $i\not= j$ 时，带入 上面的结果$\frac{\partial a^L_j}{\partial z^L_i}=-a_j^La_i^L$ \begin{aligned} \frac{\partial C}{\partial b_{j}^L} &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_i} \\ &= - \frac{1}{a^L_j} (-a_j^La_i^L) \\ &= a_i^L \end{aligned}根据反向传播的四个方程，具体分析见《反向传播算法》 可以知道，$\frac{\partial C}{\partial b^l_j} =\delta^l_j$ 和 $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ 所以,当 $i=j$ 时， \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k (a^L_j-1)当 $i\not= j$ 时， \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k a_i举个例子通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 2, 3, 4 ],那么经过softmax函数作用后概率分别就是=[e^2/(e^2+e^3+e^4),e^3/(e^2+e^3+e^4),e^4/(e^2+e^3+e^4)] = [0.0903,0.2447,0.665],如果这个样本正确的分类是第二个的话，那么计算出来的偏导（实际上这个偏导就是 $\delta^L$ 或者说是 $\partial C/\partial b^L$ ）就是[0.0903,0.2447-1,0.665]=[0.0903,-0.7553,0.665]，是不是非常简单！然后再根据这个进行back propagation就可以了 注意！当 $y_j$ 取值不为 0 或 1，而是区间 [0,1] 的一个实数值时，上面的式子只需稍稍做点修改，只需把下面式子中 $\frac{\partial C}{\partial a^L_j}$ 的结果从 $\frac{1}{a^L_j}$ 改为 $\frac{y_i}{a^L_j}$ 即可， \begin{aligned} \frac{\partial C}{\partial b_{j}^L} &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_i} \\ &= - \frac{1}{a^L_j} [a_j^L(1-a_j^L)] \\ &= a_j^L -1 \end{aligned}其它的求导过程也要做相应调整。所以在有的地方会看到这样的公式， \frac{\partial C}{\partial b^L_j} = a^L_j-y_j \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k (a^L_j-y_j)两者都是正确的，只是因为前提不一样，所以结论也有差异。 交叉熵与对数似然的关系结论：交叉熵和最大似然的loss函数是一致的，在样本所属分类是唯一的情况下。 两者能够和谐统一的关键点是： 样本所属类别是唯一的，样本一定是某一类的，似然的思想是抽样样本的概率最大化，所以每一个样本只能处于一个固定的状态。这就使得每个样本的概率形式可以写成一个综合的形式，而综合的形式呢刚好可以在log下拆分成交叉熵的样子。在多类下，若样本所属类别是唯一的，最大似然的loss与交叉熵的loss仍然是一致的。 论证： 二项分布 二项分布也叫 0-1 分布，如随机变量 x 服从二项分布，关于参数 μ（0≤μ≤1），其值取 1 和取 0 的概率如下： p(x=1|\mu)=\mu p(x=0|\mu)=1-\mu则在 x 上的概率分布为： \text{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x}服从二项分布的样本集的对数似然函数 给定样本集 D={x1,x2,…,xB} 是对随机变量 x 的观测值，假定样本集从二项分布 p(x|μ) 中独立（p(x1,x2,…,xN)=∏ip(xi)）采样得来，则当前样本集关于 μ 的似然函数为： p(\mathcal D|\mu)=\prod_{n=1}^Np(x_n|\mu)=\prod_{n=1}^N\mu^{x_n}\left(1-\mu\right)^{1-x_n}从频率学派的观点来说，通过最大似然函数的取值，可以估计参数 μ，最大化似然函数，等价于最大化其对数形式： 求其关于 μ 的导数，解得 μ 的最大似然解为： \mu_{ML}=\frac1N\sum_{n=1}^Nx_n这里我们仅关注： \ln P(\mathcal D|\mu)=\sum_{n=1}^Nx_n\ln \mu+(1-x_n)\ln(1-\mu)交叉熵损失函数 L_H(\mathbf x,\mathbf z)=-\sum_{n=1}^Nx_n\log z_n+(1-x_n)\log(1-z_n)x 表示原始信号，z 表示重构信号。（损失函数的目标是最小化，似然函数则是最大化，二者仅相差一个符号）。 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-22. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-22. [3] __鸿. softmax的log似然代价函数（公式求导）[DB/OL]. https://blog.csdn.net/u014313009/article/details/51045303. 2018-06-22. [4] 忆臻HIT_NLP. 手打例子一步一步带你看懂softmax函数以及相关求导过程[DB/OL]. https://www.jianshu.com/p/ffa51250ba2e. 2018-06-22.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——交叉熵]]></title>
    <url>%2F2018%2F06%2F21%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%86%B5%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 为什么需要交叉熵代价函数人类却能够根据明显的犯错快速地学习到正确的东西。相反，在我们的错误不是很好地定义的时候，学习的过程会变得更加缓慢。但神经网络却不一定如此，这种行为看起来和人类学习行为差异很大。人工神经元在其犯错较大的情况下其实学习很有难度。 为了理解这个问题的源头，想想我们的神经元是通过改变权重和偏置，并以一个代价函数的偏导数 $\partial C/\partial w$ 和 $\partial C/\partial b$ 决定的速度学习。所以，我们在说“学习缓慢”时，实际上就是说这些偏导数很小。 用上一章《反向传播算法》的符号定义，对于二次代价函数,输出层的权重的偏导数为 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k (a^L_j-y_j) \sigma'(z^L_j)项 $\sigma’(z^L_j)$ 会在一个输出神经元困在错误值时导致学习速度的下降。 我们可以从这幅图看出，当神经元的输出接近 $1$ 的时候，曲线变得相当平，所以$\sigma’(z)$ 就很小了。上面式子中的 $\frac{\partial C}{\partial w^L_{jk}}$ 也会非常小。这其实就是学习缓慢的原因所在。而且，我们后面也会提到，这种学习速度下降的原因实际上也是更加一般的神经网络学习缓慢的原因，并不仅仅是在这个特例中特有的。 注意，在输出层使用线性神经元时使用二次代价函数。假设我们有一个多层多神经元网络，最终输出层的神经元都是线性神经元，输出不再是$S$型函数作用的结果，而是 $a^L_j = z^L_j$。如果我们使用二次代价函数，那么对单个训练样本 $x$ 的输出误差就是 \delta^L = a^L-y这表明如果输出神经元是线性的那么二次代价不再会导致学习速度下降的问题。在此情形下，二次代价函数就是一种合适的选择。 但是如果输出神经元是$S$型函数作用的结果，我们就最好考虑其他的代价函数。 交叉熵代价函数的定义那么我们如何解决这个问题呢？研究表明，我们可以通过使用交叉熵函数来替换二次代价函数。为了理解什么是交叉熵，我们稍微改变一下之前的简单例子。假设，我们现在要训练一个包含若干输入变量的的神经元，$x_1, x_2, \ldots$ 对应的权重为 $w_1,w_2, \ldots$ 和偏置 $b$： 神经元的输出就是 $a = \sigma(z)$，其中 $z = \sum_j w_j x_j+b$ 是输入的带权和。我们如下定义这个神经元的交叉熵代价函数： C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right]其中 $n$ 是训练数据的总数，求和是在所有的训练输入 $x$ 上进行的，$y$ 是对应的目标输出。 对于交叉熵代价函数，针对一个训练样本 $x$ 的输出误差 $\delta^L$为 \delta^L = a^L-y关于输出层的权重的偏导数为 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k (a^L_j-y_j)这里 $\sigma’(z^L_j)$ 就消失了，所以交叉熵避免了学习缓慢的问题。 那么我们应该在什么时候用交叉熵来替换二次代价函数？实际上，如果在输出神经元是$S$时，交叉熵一般都是更好的选择。为什么？考虑一下我们初始化网络的权重和偏置的时候通常使用某种随机方法。可能会发生这样的情况，这些初始选择会对某些训练输入误差相当明显，比如说，目标输出是 $1$，而实际值是$0$，或者完全反过来。如果我们使用二次代价函数，那么这就会导致学习速度的下降。它并不会完全终止学习的过程，因为这些权重会持续从其他的样本中进行学习，但是显然这不是我们想要的效果。 交叉熵在分类问题中的应用交叉熵损失函数应用在分类问题中时，不管是单分类还是多分类，类别的标签都只能是 0 或者 1。 交叉熵在单分类问题中的应用这里的单类别是指，每一张图像样本只能有一个类别，比如只能是狗或只能是猫。交叉熵在单分类问题上基本是标配的方法 loss = -\sum_{i=1}^ny_i log(\hat{y}_i)上式为一张样本的 $loss$ 计算方法。式中 $n$ 代表着 $n$ 种类别。举例如下， 交叉熵在多标签问题中的应用这里的多类别是指，每一张图像样本可以有多个类别，比如同时包含一只猫和一只狗和单分类问题的标签不同，多分类的标签是n-hot。 值得注意的是，这里的Pred采用的是sigmoid函数计算。将每一个节点的输出归一化到[0,1]之间。所有Pred值的和也不再为1。换句话说，就是每一个Label都是独立分布的，相互之间没有影响。所以交叉熵在这里是单独对每一个节点进行计算，每一个节点只有两种可能值，所以是一个二项分布。前面说过对于二项分布这种特殊的分布，熵的计算可以进行简化。 同样的，交叉熵的计算也可以简化，即 loss =-ylog(\hat{y})-(1-y)log(1-\hat{y})注意，上式只是针对一个节点的计算公式。这一点一定要和单分类loss区分开来。 交叉熵代价函数对权重求导的证明交叉熵代价函数的定义： C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right]代价函数 $C$ 对 $w^L_{jk}$ 求偏导 \begin{aligned} \frac{\partial C}{\partial w_{jk}^L} &= -\frac{1}{n} \sum_x \left( \frac{y^L_j }{\sigma(z^L_j)} -\frac{(1-y^L_j)}{1-\sigma(z^L_j)} \right) \frac{\partial \sigma(z^L_j)}{\partial w_{jk}^L} \\ &= -\frac{1}{n} \sum_x \left( \frac{y^L_j }{\sigma(z^L_j)} -\frac{(1-y^L_j)}{1-\sigma(z^L_j)} \right)\sigma'(z^L_j) a_k^{L-1} \\ &=\frac{1}{n} \sum_x \frac{\sigma'(z^L_j) a_k^{L-1}}{\sigma(z^L_j) (1-\sigma(z^L_j))} (\sigma(z^L_j)-y_j) \end{aligned}其中$\frac{\partial z^l_j}{\partial w^l_{jk}} = a_k^{l-1}$ 来自，根据 $z_j^l$ 定义 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j所以 \frac{\partial z^l_j}{\partial w^l_{jk}} = a_k^{l-1}根据 $\sigma(z) = 1/(1+e^{-z})$ 的定义， \begin{aligned} \sigma'(z) &= (\frac{1}{1+e^{-z}})' \\ &= \frac{e^{-z}}{(1+e^{-z})^{2}} \\ &= \frac{1+e^{-z}-1}{(1+e^{-z})^{2}} \\ &= \frac{1}{(1+e^{-z})}(1-\frac{1}{(1+e^{-z})}) \\ &= \sigma(z)(1-\sigma(z)) \\ \end{aligned}把 $\sigma’(z)$ 带入 $\frac{\partial C}{\partial w_j}$ 可得 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_xa^{L-1}_k (a^L_j-y_j)其向量形式是 \frac{\partial C}{\partial w^L} = \frac{1}{n} \sum_x a_j^{L-1}(\sigma(z^L)-y)对偏置用同样的方法可得 \frac{\partial C}{\partial b^L_{j}} = \frac{1}{n} \sum_x (a^L_j-y_j)交叉熵的含义和来源我们对于交叉熵的讨论聚焦在代数分析和代码实现。这虽然很有用，但是也留下了一个未能回答的更加宽泛的概念上的问题，如：交叉熵究竟表示什么？存在一些直觉上的思考交叉熵的方法吗？我们如何想到这个概念？ 让我们从最后一个问题开始回答：什么能够激发我们想到交叉熵？假设我们发现学习速度下降了，并理解其原因是因为对于二次代价函数,输出层的权重的偏导数为 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k (a^L_j-y_j) \sigma'(z^L_j)项 $\sigma’(z^L_j)$ 会在一个输出神经元困在错误值时导致学习速度的下降。在研究了这些公式后，我们可能就会想到选择一个不包含 $\sigma’(z)$ 的代价函数。所以，这时候对一个训练样本 $x$，其代价 $C = C_x$ 满足： \frac{\partial C}{\partial w_j} = a_j^{L-1}(a^L_j-y) \frac{\partial C}{\partial b } = (a-y)如果我们选择的损失函数满足这些条件，那么它们就能以简单的方式呈现这样的特性：初始误差越大，神经元学习得越快。这也能够解决学习速度下降的问题。实际上，从这些公式开始，现在我们就看看凭着我们数学的直觉推导出交叉熵的形式是可行的。我们来推一下，由链式法则，我们有 \frac{\partial C}{\partial b} = \frac{\partial C}{\partial a} \sigma'(z)使用 $\sigma’(z) = \sigma(z)(1-\sigma(z)) = a(1-a)$，上个等式就变成 \frac{\partial C}{\partial b} = \frac{\partial C}{\partial a}a(1-a)对比等式，我们有 \frac{\partial C}{\partial a} = \frac{a-y}{a(1-a)}对此方程关于 $a$ 进行积分，得到 C = -[y \ln a + (1-y) \ln (1-a)]+ {\rm constant}其中 constant 是积分常量。这是一个单独的训练样本 $x$ 对损失函数的贡献。为了得到整个的损失函数，我们需要对所有的训练样本进行平均，得到了 C = -\frac{1}{n} \sum_x [y \ln a +(1-y) \ln(1-a)] + {\rm constant}而这里的常量就是所有单独的常量的平均。所以我们看到方程 \frac{\partial C}{\partial w_j} = a_j^{L-1}(a^L_j-y) \frac{\partial C}{\partial b } = (a-y)唯一确定了交叉熵的形式，并加上了一个常量的项。这个交叉熵并不是凭空产生的。而是一种我们以自然和简单的方法获得的结果。 那么交叉熵直觉含义又是什么？我们如何看待它？深入解释这一点会将我们带到一个不大愿意讨论的领域。然而，还是值得提一下，有一种源自信息论的解释交叉熵的标准方式。粗略地说，交叉熵是“不确定性”的一种度量。特别地，我们的神经元想要计算函数 $x \rightarrow y = y(x)$。但是，它用函数$x\rightarrow a = a(x)$ 进行了替换。假设我们将 $a$ 想象成我们神经元估计为 $y = 1$ 的概率，而 $1-a$ 则是 $y=0$ 的概率。那么交叉熵衡量我们学习到 $y$ 的正确值的平均起来的不确定性。 如果输出我们期望的结果，不确定性就会小一点；反之，不确定性就大一些。当然，我这里没有严格地给出“不确定性”到底意味着什么，所以看起来像在夸夸其谈。但是实际上，在信息论中有一种准确的方式来定义不确定性究竟是什么。详细内容请看交叉熵（cross-entropy）的数学历史。 交叉熵（cross-entropy）的数学历史通用的说，熵(Entropy)被用于描述一个系统中的不确定性(the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。 先给出一个”不严谨”的概念表述： 熵：可以表示一个事件A的自信息量，也就是A包含多少信息。 KL散度：可以用来表示从事件A的角度来看，事件B有多大不同。 交叉熵：可以用来表示从事件A的角度来看，如何描述事件B。 一句话总结的话：KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价。 信息量 首先是信息量。假设我们听到了两件事，分别如下：事件A：巴西队进入了2018世界杯决赛圈。事件B：中国队进入了2018世界杯决赛圈。仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。 假设 $X$ 是一个离散型随机变量，其取值集合为 $\chi$ ,概率分布函数 $p(x)=Pr(X=x),x\in\chi$ ,则定义事件 $X=x_0$ 的信息量为： I(x_0)=-log(p(x_0))由于是概率所以$p(x_0)$的取值范围是[0,1], 绘制为图形如下： 什么是熵(Entropy)？ 放在信息论的语境里面来说，就是一个事件所包含的信息量。我们现在有了信息量的定义，而熵用来表示所有信息量的期望，即： 因此熵被定义为 $S(x)=-\sum_{i}P(x_{i})log_{b}P(x_{i})$ 如何衡量两个事件/分布之间的不同：KL散度 我们上面说的是对于一个随机变量x的事件A的自信息量，如果我们有另一个独立的随机变量x相关的事件B，该怎么计算它们之间的区别？ 此处我们介绍默认的计算方法：KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度不具备有对称性。在距离上的对称性指的是A到B的距离等于B到A的距离。 KL散度的数学定义： 相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异 维基百科对相对熵的定义 In the context of machine learning, DKL(P‖Q) is often called the information gain achieved if P is used instead of Q. 对于离散事件我们可以定义事件A和B的差别为： D_{KL}(A||B) = \sum_{i}P_{A}(x_i) log\bigg(\frac{P_{A}(x_i)}{P_{B}(x_i)} \bigg) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- P_{A}(x_i)log(P_{B}(x_i))对于连续事件，那么我们只是把求和改为求积分而已。 D_{KL}(A||B) = \int a(x) log\bigg(\frac{a(x)}{b(x)} \bigg)从公式中可以看出： 如果 $P_A=P_B$，即两个事件分布完全相同，那么KL散度等于0。 观察公式，可以发现减号左边的就是事件A的熵，请记住这个发现。 如果颠倒一下顺序求 $D_{KL}(B||A)$，那么就需要使用B的熵，答案就不一样了。所以KL散度来计算两个分布A与B的时候是不是对称的，有“坐标系”的问题**，$D_{KL}(A||B)\ne D_{KL}(B||A)$ 换句话说，KL散度由A自己的熵与B在A上的期望共同决定。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是求 A与B之间的对数差 在 A上的期望值。 KL散度 = 交叉熵 - 熵？ 如果我们默认了用KL散度来计算两个分布间的不同，那还要交叉熵做什么？ 事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分(公式2.1)：A和B的交叉熵 = A与B的KL散度 - A的熵。 $D_{KL}(A||B) = -S(A)+H(A,B) $ 对比一下这是KL散度的公式： $D_{KL}(A||B) = \sum_{i}P_{A}(x_i) log\bigg(\frac{P_{A}(x_i)}{P_{B}(x_i)} \bigg) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- P_{A}(x_i)log(P_{B}(x_i)) $ 这是熵的公式： $S(A) = -\sum_{i}P_A(x_{i})logP_A(x_{i})$ 这是交叉熵公式： $H(A,B)= -\sum_{i}P_{A}(x_i)log(P_{B}(x_i)) $ 此处最重要的观察是，如果 $S(A)$是一个常量，那么$D_{KL}(A||B) = H(A,B) $ ，也就是说KL散度和交叉熵在特定条件下等价。 为什么交叉熵可以用作代价？ 接着上一点说，最小化模型分布 $P(model)$ 与 训练数据上的分布 $P(training)$ 的差异 等价于 最小化这两个分布间的KL散度，也就是最小化 $KL(P(training)||P(model))$。 比照第四部分的公式： 此处的A就是数据的真实分布： $P(training)$ 此处的B就是模型从训练数据上学到的分布： $P(model)$ 巧的是，训练数据的分布A是给定的。那么根据我们在第四部分说的，因为A固定不变，那么求 $D_{KL}(A||B)$等价于求 $H(A,B)$ ，也就是A与B的交叉熵。得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。 但是，完美的学到了训练数据分布往往意味着过拟合，因为训练数据不等于真实数据，我们只是假设它们是相似的，而一般还要假设存在一个高斯分布的误差，是模型的泛化误差下线。 因此在评价机器学习模型时，我们往往不能只看训练数据上的误分率和交叉熵，还是要关注测试数据上的表现。如果在测试集上的表现也不错，才能保证这不是一个过拟合或者欠拟合的模型。交叉熵比照误分率还有更多的优势，因为它可以和很多概率模型完美的结合。 所以逻辑思路是，为了让学到的模型分布更贴近真实数据分布，我们最小化 模型数据分布 与 训练数据之间的KL散度，而因为训练数据的分布是固定的，因此最小化KL散度等价于最小化交叉熵。 因为等价，而且交叉熵更简单更好计算，当然用它。 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-21. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-21. [3] 微调. 为什么交叉熵（cross-entropy）可以用于计算代价？[DB/OL]. https://www.zhihu.com/question/65288314/answer/244557337. 2018-06-21. [4] 史丹利复合田. 一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉[DB/OL]. https://blog.csdn.net/tsyccnh/article/details/79163834. 2018-06-22.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>交叉熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播算法]]></title>
    <url>%2F2018%2F06%2F21%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 2 How the backpropagation algorithm works 反向传播概述反向传播算法最初在 1970 年代被提及，但是人们直到 David Rumelhart、Geoffrey Hinton 和 Ronald Williams 的著名的 1986 年的论文中才认识到这个算法的重要性。 反向传播的核心是一个对代价函数 $C$ 关于任何权重 $w$ 和 偏置 $b$ 的偏导数 $\partial{C}/\partial{w}$ 的表达式。 这个表达式告诉我们在改变权重和偏置时，代价函数变化的快慢。 关于代价函数的两个假设 代价函数可以被写成在每一个训练样本 $x$ 上的代价函数 $C_x$ 的均值 $C=\frac{1}{n}\sum_{x}C_x$。 代价函数可以写成神经网络输出的函数。 需要假设1的原因是，反向传播实际上是对一个独立的训练样本计算了 $\partial{C_x}/\partial{w}$ 和 $\partial{C_x}/\partial{b}$。然后通过在所有训练样本上进行平均化获得 $\partial{C}/\partial{w}$ 和 $\partial{C}/\partial{w}$ 。 需要假设2的原因是，要把代价函数与神经网络输出联系起来，进而与神经网络的参数联系起来。 符号定义 $W_{jk}^{l}$ 是从 $l-1$ 层的第 $k$ 个神经元到 $l$ 层的第 $j$ 个神经元的权重。 $b_j^l$ 是第 $l$ 层的第 $j$ 个神经元的偏置。 $a_j^l$ 是第 $l$ 层的第 $j$ 个神经元的激活值。 $\sigma$ 是激活函数。 把上面的符号向量化 $W^{l}$ 是权重矩阵，第 $j$ 行 $k$ 列的元素是 $W_{jk}^{l}$。 例如第二层与第三层之间的权重矩阵是 w^3 = { \left[ \begin{matrix} w_{11}^3 & w_{12}^3 & w_{13}^3 & w_{14}^3\\ w_{21}^3 & w_{22}^3 & w_{23}^3 & w_{24}^3 \end{matrix} \right] } $b^l$ 是偏置向量。第 $j$ 行的元素是 $b_j^l$。 例如第二层的偏置向量是 b^2 = { \left[ \begin{matrix} b_{1}^2 \\ \\ b_{2}^2\\ \\ b_{3}^2\\ \\ b_{4}^2 \end{matrix} \right] }有了这些表示 $l$ 层的第 $j$ 个神经元的激活值 $a_j^l$ 就和 $l-1$ 层的激活值通过方程关联起来了 a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right)把上面式子向量化 a^{l} = \sigma(w^l a^{l-1}+b^l)例如第三层的激活向量是 { \left[ \begin{matrix} a_{1}^3 \\ \\ a_{2}^3 \\ \end{matrix} \right] } =\sigma\left( { \left[ \begin{matrix} w_{11}^3 & w_{12}^3 & w_{13}^3 & w_{14}^3\\ w_{21}^3 & w_{22}^3 & w_{23}^3 & w_{24}^3 \end{matrix} \right] } { \left[ \begin{matrix} a_{1}^2 \\ \\ a_{2}^2 \\ \\ a_{3}^2 \\ \\ a_{4}^2 \\ \end{matrix} \right] }+ { \left[ \begin{matrix} b_{1}^3 \\ \\ b_{2}^3 \\ \end{matrix} \right] }\right) $a^{l}$ 是激活向量。第 $j$ 行的元素是 $a_j^l$。 定义 z^l \equiv w^l a^{l-1}+b^l则 $a^l =\sigma(z^l)$ $z^l$ 表示第第 $l$ 层的带权输入。第 $j$ 个元素是 $z_j^l$。 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j $z_j^l$ 是第 $l$ 层的第 $j$ 个神经元的带权输入。 反向传播的核心是一个对代价函数 $C$ 关于任何权重 $w$ 和 偏置 $b$ 的偏导数 $\partial{C}/\partial{w}$ 的表达式。为了计算这些值，引入一个中间量 $\delta_j^l$ ,表示在 $l$ 层的第 $j$ 个神经元的误差。 定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.$\delta^l$ 是误差向量，$\delta^l$ 的第 $j$ 个元素是 $\delta_j^l$。 反向传播的四个基本方程 $\nabla_a$ 是求梯度运算符，$\nabla_a C$ 结果是一个向量，其元素是偏导数 $\partial C / \partial a^L_j$。 $\odot$ 是按元素乘积的运算符，$ {(s \odot t)}_j = s_j t_j $ ，例如 \left[\begin{array}{c} 1 \\ 2 \end{array}\right] \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right] = \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right] = \left[ \begin{array}{c} 3 \\ 8 \end{array} \right]. 计算公式 维度变化 $\delta^L =\nabla_a C\odot \dfrac{\partial f^{(L)}}{\partial z^L} $ $(d^L, 1) = (d^L, 1) * (d^L, 1)$ $\delta^{(l)} =({(w^{(l+1)})}^T\delta^{(l+1)})\odot \sigma’(z^l) $ $(d^l, 1) = {({d}^{l+1}, {d}^{l})}^T (d^{l+1}, 1) * (d^l, 1)$ $\dfrac{\partial C}{\partial b^{(l)}} =\delta^{(l)}$ $(d^l, 1) = (d^l, 1)$ $\dfrac{\partial C}{\partial w^l} = \delta^l (a^{l-1})^T $ $(d^l, d^{l-1}) = (d^l,1) {(d^{l-1}, 1)}^T$ 反向传播算法 正如我们上面所讲的，反向传播算法对一个训练样本计算代价函数的梯度，$C=C_x$。在实践中，通常将反向传播算法和诸如随机梯度下降这样的学习算法进行组合使用，我们会对许多训练样本计算对应的梯度。特别地，给定一个大小为 m 的小批量数据，下面的算法在这个小批量数据的基础上应用梯度下降学习算法： 反向传播算法与小批量随机梯度下降算法结合的一个示意代码，完整代码参看 network.py 12345678910111213141516171819202122232425262728293031323334def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in range(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) 12345678def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y)def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) 四个基本方程的证明 我们现在证明这四个基本的方程（BP）-（BP4）。所有的这些都是多元微积分的链式法则的推论。 证明$\delta^L = \nabla_a C \odot \sigma’(z^L)$从方程（BP1）开始，它给出了误差 $\delta^l$ 的表达式。根据定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.根据关于代价函数的两个假设2 “代价函数可以写成神经网络输出的函数”，应用链式法测可知可先对神经网络输出求偏导${\partial C}/{\partial a^L_k}$再对带权输出求偏导${\partial a^L_k}/{\partial z^L_j}$。 \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j},看起来上面式子很复杂，但是由于第 $k$ 个神经元的输出激活值 $a_k^l$ 只依赖于 当下标 $k=j$ 时第 $j$ 个神经元的输入权重 $z_j^l$。所有当 $k\neq {j}$ 时 $\partial a^L_k / \partial z^L_j$ 消失了。结果我们可以简化上一个式子为 \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.又因为 $a^L_j = \sigma(z^L_j)$ 所以 $\frac{\partial a^L_j}{\partial z^L_j}$ 可以写成 $\sigma’(z^L_j)$，方程变为 \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j)这就是分量形式的（BP1），再根据$\nabla_a$ 是求梯度运算符，$\nabla_a C$ 结果是一个向量，其元素是偏导数 $\partial C / \partial a^L_j$。方程可以写成向量形式 \delta^L ={\nabla_a {C}} \odot {\sigma'(z^L_j)}（BP1） 得到证明。 证明 $ \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^l)$证明（BP2），它个给出以下一层误差 $\delta^{l+1}$ 的形式表示误差 $\delta^l$。为此，要以 $\delta^l_j = \partial C / \partial z^l_j$的形式重写 $\delta^{l+1}_k = \partial C / \partial z^{l+1}_k$,$\delta^{l+1}$ 和 $\delta^l$ 通过 $z_k^{l+1}$ 和 $z_j^l$ 联系起来，应用链式法测 根据 $z_k^{l+1}$ 的定义有 z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k$z_k^{l+1}$ 对 $z_j^{l}$ 做偏微分，得到 \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j)注意虽然$z^{l+1}$ 和 $z^{l}$ 所在的两层神经元连接错综复杂，但两层之间任意一对神经元（同一层内不连接）只有一条连接，即 $z_k^{l+1}$ 和 $z_j^{l}$ 之间只通过 $w_{kj}^{l+1}$ 连接。所以$z_k^{l+1}$ 对 $z_j^{l}$ 做偏微分的结果很简单，只是 $ w^{l+1}_{kj} \sigma’(z^l_j)$。把这个结果带入 $\delta_j^l$ 中 \delta^l_j = \sum_k w^{l+1}_{kj} \delta^{l+1}_k \sigma'(z^l_j)这正是以分量形式写的(BP2)。 写成向量形式 \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)举例 { \left[ \begin{matrix} \delta_{1}^l \\ \\ \delta_{2}^l \\ \\ ... \\ \delta_{j}^l \end{matrix} \right] } = { \left[ \begin{matrix} w_{11}^{l+1} & w_{21}^{l+1} & w_{31}^{l+1} & ... &w_{k1}^{l+1} \\ \\ w_{12}^{l+1} & w_{22}^{l+1} & w_{32}^{l+1} & ... & w_{k2}^{l+1} \\ \\ ... \\ w_{j1}^{l+1} & w_{j2}^{l+1} & w_{j1}^{l+1} & ... & w_{kj}^{l+1} \end{matrix} \right] } { \left[ \begin{matrix} \delta_{1}^{l+1} \\ \\ \delta_{2}^{l+1} \\ \\ \delta_{3}^{l+1} \\ \\ ... \\ \delta_{k}^{l+1} \end{matrix} \right] } \odot { \left[ \begin{matrix} \sigma'(z_1^l) \\ \\ \sigma'(z_2^l) \\ \\ \sigma'(z_3^l) \\ \\ ... \\ \sigma'(z_k^l) \end{matrix} \right] }（BP2） 得到证明。 证明 $\frac{\partial C}{\partial b^l_j} =\delta^l_j.$根据 $z_j^l$ 定义 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j和 $\delta_j^l$ 定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.因此 \frac{\partial C}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j}\frac{\partial z^l_j}{\partial b^l_j}又因为 \frac{\partial z^l_j}{\partial b^l_j} = 1所以 \frac{\partial C}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j}\frac{\partial z^l_j}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j}\cdot 1= \frac{\partial C}{\partial z^l_j}=\delta^l_j即 \frac{\partial C}{\partial b^l_j} =\delta^l_j写成向量形式 \frac{\partial C}{\partial b^l} =\delta^l（BP3） 得到证明。 证明 $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$根据 $z_j^l$ 定义 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j和 $\delta_j^l$ 定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.又因为 \frac{\partial z^l_j}{\partial w^l_{jk}} = a_k^{l-1}所以 \frac{\partial C}{\partial w^l_{jk}} = \frac{\partial C}{\partial z^l_j}\frac{\partial z^l_j}{\partial w^l_{jk}} = \delta^l_j a_k^{l-1}把式子向量化 \frac{\partial C}{\partial w^l} = \delta^l (a^{l-1})^T举例 \frac{\partial C}{\partial w^l} = { \left[ \begin{matrix} \delta_{1}^l \\ \\ \delta_{2}^l \\ \\ ... \\ \delta_{j}^l \end{matrix} \right] }{ \left[ \begin{matrix} a_{1}^{l-1} & a_{2}^{l-1} & ... &a_{k}^{l-1} \end{matrix} \right] }（BP4） 得到证明。 一个直观的图： 到此关于反向传播的四个方程已经全部证明完毕。 其他学者反向传播四个方程的证明（他写的更简明扼要些）：CSDN: oio328Loio 矩阵形式反向传播算法正如我们上面所讲的，反向传播算法对一个训练样本计算代价函数的梯度，$C=C_x$。在实践中，通常将反向传播算法和诸如随机梯度下降这样的学习算法进行组合使用，我们会对许多训练样本计算对应的梯度。特别地，给定一个大小为 m 的小批量数据，下面的算法在这个小批量数据的基础上应用梯度下降学习算法： 根据上面的小批量数据的符号定义，为了方便用矩阵表示，下面新增了一些符号。 $z^{v,l}$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的带权输入向量，用矩阵 $Z^l$ 来表示就是 Z^l = { \left[ \begin{matrix} {(z^{1,l})}^T \\ \\ {(z^{2,l})}^T\\ \\ ...\\ \\ {(z^{m,l})}^T \end{matrix} \right] }$a^{v,l}$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的激活向量，用矩阵 $A^l$ 来表示就是 A^l = { \left[ \begin{matrix} {(a^{1,l})}^T \\ \\ {(a^{2,l})}^T\\ \\ ...\\ \\ {(a^{m,l})}^T \end{matrix} \right] }$\delta^{v,l}$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的误差向量，用矩阵 $\Delta^l$ 来表示就是 \delta^l = { \left[ \begin{matrix} {(\delta^{1,l})}^T \\ \\ {(\delta^{2,l})}^T\\ \\ ...\\ \\ {(\delta^{m,l})}^T \end{matrix} \right] } \delta^L =\nabla_a C\odot \frac{\partial f^{(L)}}{\partial z^L}的矩阵形式是， \Delta^L =\nabla_{A^L} C\odot \frac{\partial f^{(L)}}{\partial Z^L} \delta^{l}=({(w^{l+1})}^T\delta^{l+1})\odot \sigma'(z^l)的矩阵形式是， \Delta^{l}=(\Delta^{l+1}w^{l+1})\odot \sigma'(Z^l) \frac{\partial C}{\partial b^{(l)}}=\delta^{(l)}的矩阵形式是， \frac{\partial C}{\partial b^{(l)}}=\frac{1}{m}{(sum(\Delta^l, axis=0))}^T \frac{\partial C}{\partial w^l} = \delta^l (a^{l-1})^T的矩阵形式是， \frac{\partial C}{\partial w^{(l)}}=\frac{1}{m}{(\Delta^l)}^TA^{l-1}归纳一下，可以得到矩阵形式的反向传播算法： 计算公式 维度变化 $\Delta^L =\nabla_{A^L} C\odot \dfrac{\partial f^{(L)}}{\partial Z^L}$ $(m, d^L) = (m, d^L) * (m, d^L)$ $\Delta^{l} =(\Delta^{l+1}W^{l+1})\odot \sigma’(Z^l)$ $(m, {d}^l) = (m, {d}^{l+1}) ({d}^{l+1}, {d}^l)$ $\dfrac{\partial C}{\partial b^{(l)}} =\dfrac{1}{m}{(sum(\Delta^l, axis=0))}^T$ $(d^l, 1) = {(1, d^l)}^T$ $\dfrac{\partial C}{\partial w^{(l)}} =\dfrac{1}{m}{(\Delta^l)}^TA^{l-1} $ $(d^{l}, d^{l-1}) = {(m, d^l)}^T(m, d^{l-1})$ 反向传播：全局观 如上图所示，假设我们对 $w_{jk}^l$ 做一点微小的扰动 $\Delta w_{jk}^l$, 这个扰动会沿着神经网络最终影响到代价函数 $C$, 代价函数的 $\Delta C$ 改变和 $\Delta w_{jk}^l$ 按照下面公式联系起来 \Delta C \approx \frac{\partial C}{\partial w^l_{jk}} \Delta w^l_{jk}可以想象影响代价函数的一条路径是 \Delta C \approx \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}为了计算 $C$ 的全部改变，我们需要对所有可能的路径进行求和，即 \Delta C \approx \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}因为 \frac{\partial C}{\partial w^l_{jk}}=\frac{\Delta C}{\Delta w^l_{jk}}根据上面的三个式子可知 \frac{\partial C}{\partial w^l_{jk}} = \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}}上面的公式看起来复杂，这里有一个相当好的直觉上的解释。我们用这个公式计算 $C$ 关于网络中一个权重的变化率。而这个公式告诉我们的是：两个神经元之间的连接其实是关联于一个变化率因子，这仅仅是一个神经元的激活值相对于其他神经元的激活值的偏导数。路径的变化率因子就是这条路径上众多因子的乘积。整个变化率 $\partial C / \partial w^l_{jk}$ 就是对于所有可能从初始权重到最终输出的代价函数的路径的变化率因子的和。针对某一路径，这个过程解释如下， 如果用矩阵运算对上面式子所有的情况求和，然后尽可能化简，最后你会发现，自己就是在做反向传播！可以将反向传播想象成一种计算所有可能路径变化率求和的方式。或者，换句话说，反向传播就是一种巧妙地追踪权重和偏置微小变化的传播，抵达输出层影响代价函数的技术。 如果你尝试用上面的思路来证明反向传播，会比本文的反向传播四个方程证明复杂许多，因为按上面的思路来证明有许多可以简化的地方。其中可以添加一个巧妙的步骤，上面方程的偏导对象是类似 $a_q^{l+1}$ 的激活值。巧妙之处是改用加权输入，例如 $z_q^{l+1}$ ，作为中间变量。如果没想到这个主意，而是继续使用激活值 $a_q^{l+1}$ ，你得到的证明最后会比前文给出的证明稍稍复杂些。 其实最早的证明的出现也不是太过神秘的事情。因为那只是对简化证明的艰辛工作的积累！ 参考文献[1] Michael Nielsen. CHAPTER 2 How the backpropagation algorithm works[DB/OL]. http://neuralnetworksanddeeplearning.com/chap2.html, 2018-06-21. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap2.tex, 2018-06-21. [3] oio328Loio. 神经网络学习（三）反向（BP）传播算法（1）[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79537246, 2018-06-25.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 2 How the backpropagation algorithm works</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>反向传播算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯估计、最大似然估计、最大后验估计三者的区别]]></title>
    <url>%2F2018%2F06%2F20%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[实例分析即使学过机器学习的人，对机器学习中的 MLE(极大似然估计)、MAP(最大后验估计)以及贝叶斯估计(Bayesian) 仍有可能一知半解。对于一个基础模型，通常都可以从这三个角度去建模，比如对于逻辑回归（Logistics Regression）来说： MLE: Logistics Regression MAP: Regularized Logistics RegressionBayesian: Bayesian Logistic Regression 本文结合实际例子，以通俗易懂的方式去讲解这三者之间的本质区别，希望帮助读者扫清理解中的障碍。 先导知识点： 假设空间（Hypothesis Space） 什么叫假设空间呢？我们可以这样理解。机器学习包含很多种算法，比如线性回归、支持向量机、神经网络、决策树、GDBT等等。我们在建模的时候，第一步就是要选择一个特定的算法比如“支持向量机”。一旦选择了一个算法，就相当于我们选择了一个假设空间。 在一个假设空间里，我们通常会有无数种不同的解（或者可以理解成模型），一个优化算法（比如梯度下降法）做的事情就是从中选择最好的一个解或者多个解/模型，当然优化过程要依赖于样本数据。举个例子，如果我们选择用支持向量机，那相当于我们可选的解/模型集中在上半部分（蓝色点）。 一个具体“toy”问题 “ 张三遇到了一个数学难题，想寻求别人帮助。通过一番思考之后发现自己的朋友在清华计算机系当老师。于是，他决定找清华计算机系学生帮忙。那张三用什么样的策略去寻求帮助呢？ 在这里，“清华计算机系”是一个假设空间。在这个假设空间里，每一位学生可以看做是一个模型（的实例化）。 对于张三来说，他有三种不同的策略可以选择。 第一种策略 : MLE 第一种策略就是从系里选出过往成绩最好的学生，并让他去解答这个难题。比如我们可以选择过去三次考试中成绩最优秀的学生。 一般的学习流程分为“学习过程”和“预测过程”。第一种策略的方案可以用下面的图来表示。 在这里，学习过程相当于从所有系的学生中挑选出成绩最好的学生。所以，这里的“学生过往成绩单”就是我们已知的训练数据 D， 选出成绩最好的学生（计算历史平均分数，并选出最高的），这个过程就是MLE。一旦我们找到了成绩最好的学生，就可以进入预测环节。在预测环节中，我们就可以让他回答张三手里的难题 x’, 之后就可以得到他给出的解答 y’。 第二种策略：MAP 跟第一种策略的不同点在于，第二种策略中我们听取了老师的建议，老师就是张三的朋友。这位老师给出了自己的观点：_“小明和小花的成绩中可能存在一些水分”。_当我们按照成绩的高低给学生排序，假设前两名依次为小明和小花，如果我们不考虑这位老师的评价，则我们肯定把小明作为目标对象。然而，既然老师已经对小明和小花做了一些负面的评价，那这个时候，我们很有可能最后选择的是班级里的第三名，而不是小明或者小花。 我们把第二种策略的过程也用一个图来描述。与上面的图相比，唯一的区别在于这里多出了老师的评价，我们称之为 Prior。 也就是说我们根据学生以往的成绩并结合老师评价，选择了一位我们认为最优秀的学生（可以看成是模型）。之后就可以让他去回答张老师的难题 x’，并得到他的解答 y’。整个过程类似于MAP的估计以及预测。 到这里，有些读者可能会有一些疑惑：“_老师的评价(Prior)跟学生过往的成绩（Observation）是怎么结合在一起的？”_。 为了回答这个问题，我们不得不引出一个非常著名的定理，叫做 贝叶斯定理， 如下图所示。左边的项是MAP需要优化的部分，通过贝叶斯定理这个项可以分解成 MLE（第一种策略）和 Prior，也就是老师的评价。在这里，分母是常数项（Constant），所以不用考虑。 第三种策略 - Bayesian 最后，我们来介绍第三种策略。这种策略应该很多人也可以想象得到，其实就是让所有人都去参与回答张三的难题，但最后我们通过一些加权平均的方式获得最终的答案。 比如有三个学生，而且我们对这三个学生情况没有任何了解。通过提问，第一个学生回答的答案是A，第二个学生回答的答案也是A，但第三个学生回答的是B。在这种情况下，我们基本可以把A作为标准答案。接着再考虑一个稍微复杂的情况，假设我们通过以往他们的表现得知第三个学生曾经多次获得过全国奥赛的金牌，那这个时候该怎么办？ 很显然，在这种情况下，我们给予第三个学生的话语权肯定要高于其他两位学生。 我们把上面的这种思路应用到张三的问题上，其实相当于我们让所有计算机系的学生参与回答这个问题，之后把他们的答案进行汇总并得出最终的答案。如果我们知道每一位学生的话语权（权重），这个汇总的过程是确定性（deterministic)。 但每一位学生的话语权（权重）怎么得到呢？ 这就是贝叶斯估计做的事情！ 我们用下面的一幅图来讲述贝叶斯估计和预测的整个过程。跟MAP类似，我们已知每一位学生过去三次考试考试成绩（D）以及老师的评价（Prior）。 但跟MAP不同的是， 我们这里的目标不再是- “选出最优秀的学生”，而是通过观测数据（D）去获得每一位学生的发言权（权重）， 而且这些权重全部加起来要等于1， 相当于是一个valid分布(distribution)。 总结起来，在第三种策略之下，给定过去考试成绩(D)和老师的评价（Prior）, 我们的目标是估计学生权重的分布，也称之为Posterior Distribution。 那这个分布具体怎么去估计呢？ 这部分就是贝叶斯估计做的事情，有很多种方法可以做这件事情，比如MCMC, Variational Method等等，但这并不是本文章的重点，所以不在这里进一步解释，有兴趣的读者可以关注之后关于贝叶斯的专栏文章。从直观的角度思考，因为我们知道每一位学生过往的成绩，所以我们很容易了解到他们的能力水平进而估计出每一位学生的话语权（权重）。 一旦我们获得了这个分布（也就是每一位学生的权重），接下来就可以通过类似于加权平均的方式做预测了，那些权重高的学生话语权自然就越大。 以上是对MLE, MAP以及贝叶斯估计的基本讲解。下面我们试图去回答两个常见的问题。 Q: 随着我们观测到越来越多的数据，MAP估计逐步逼近MLE，这句话怎么理解？ 我们接着使用之前MAP（第二种策略）的例子。在这里，我们对原来的问题稍作改变。在之前的例子里我们假设能够得到每一位学生过去三次考试中的成绩。但在这里，我们进一步假定可以获得每一位学生过去100次考试中的成绩。 那这样的修改会带来什么样的变化呢？ 如果仔细想一想，其实也很容易想得到。我们设想一下这样的两种场景。假设我们知道某一位学生过去三次的考试成绩比较优异，但老师却告诉我们这位学生能力其实不怎么样，那这时候我们很可能就去相信老师了，毕竟仅仅通过三次考试的成绩很难对一个学生有全面的了解。但相反，假设我们了解到这位学生在过去100次考试中全部获得了班里第一名，但同时老师又告诉我们这位学生的能力其实不怎么样，那这时候我们会有什么样的反应？ 两三次考试或许可以算做是运气，但连续100次都是第一名这件事情很难再跟运气画等号吧？ 我们甚至可能会去怀疑老师的品德，是不是故意污蔑人家？ 这就是说，当我们观测到的数据越来越多的时候，我们从数据中获取的信息的置信度是越高的，相反老师提供的反馈（Prior）的重要性就会逐渐降低。理想情况下，当我们拥有无穷多的数据样本时，MAP会逼近MLE估计，道理都是一样的。 Q: 为什么贝叶斯估计会比MLE, MAP难？ 回顾一下，MLE 和MAP都是在寻找一个最优秀的学生。贝叶斯估计则是在估计每一位学生的权重。第一种情况下，为了寻找最优秀的学生，我们只需知道学生之间的“相对”优秀程度。这个怎么理解呢？ 比如一个班里有三个学生A,B,C，我们知道学生A比B优秀，同时知道B比C优秀，那这时候就可以推断出学生A是最优秀的，我们并不需要明确知道A的成绩是多少，B的成绩是多少….. 但在贝叶斯估计模式下，我们必须要知道每一个学生的绝对权重，因为最后我们获得的答案是所有学生给出的答案的加权平均，而且所有学生的权重加起来要保证等于1(任何一个分布的积分和必须要等于1）。 假设我们知道每一位学生的能力值，a1, a2,…. an，这个能作为权重吗？ 显然不能。为了获得权重，有一种最简单的方法就是先求和，然后再求权重。比如先计算 a1+…+an = S, 再用a1/S 作为权重。这貌似看起来也不难啊，只不过多做了一个加法操作？ 我们很容易看出这个加法操作的时间复杂度是O(n)，依赖于总体学生的数量。如果我们的假设空间只有几百名学生，这个是不成问题的。 但实际中，比如我们假设我们的模型用的是支持向量机，然后把假设空间里的每一个可行解比喻成学生，那这个假设空间里有多少个学生呢？ 是无数个！！， 也就是说需要对无穷多个数做这种加法操作。 当然，这种加法操作会以积分(integeral)的方式存在，但问题是这种积分通常没有一个closed-form的解，你必须要去近似地估计才可以，这就是MCMC或者Variational方法做的事情，不在这里多做解释。 本文几点重要的Take-aways： 每一个模型定义了一个假设空间，一般假设空间都包含无穷的可行解； MLE不考虑先验（prior)，MAP和贝叶斯估计则考虑先验（prior）； MLE、MAP是选择相对最好的一个模型（point estimation）， 贝叶斯方法则是通过观测数据来估计后验分布(posterior distribution)，并通过后验分布做群体决策，所以后者的目标并不是在去选择某一个最好的模型； 当样本个数无穷多的时候，MAP理论上会逼近MLE； 贝叶斯估计复杂度大，通常用MCMC等近似算法来近似； 最后贴一张总结的图: 理论分析一、机器学习 核心思想是从past experience中学习出规则，从而对新的事物进行预测。对于监督学习来说，有用的样本数目越多，训练越准确。 用下图来表示机器学习的过程及包含的知识： 简单来说就是： 首先要定义我们的假设空间（Model assumption）：如线性分类，线性回归，逻辑回归，SVM，深度学习网络等。 如何衡量我们学出来的模型的好坏？定义损失函数（目标函数），lost function，如square loss 如何对假设的模型做优化，及optimization过程。简单说，就是选择一种算法（如：梯度下降，牛顿法等），对目标函数进行优化，最终得到最优解； 不同的模型使用不同的算法，如逻辑回归通常用梯度下降法解决，神经网络用反向传播算法解决，贝叶斯模型则用MCMC来解决。 机器学习 = 模型 + 优化（不同算法） 还有一个问题，模型的复杂度怎么衡量？因为复杂的模型容易出现过拟合（overfitting）。解决过拟合的方就是加入正则项（regularization） 以上问题都解决之后，我们怎么判断这个解就是真的好的呢？用 交叉验证（cross-validation） 来验证一下。 二、ML vs MAP vs Bayesian ML（最大似然估计）：就是给定一个模型的参数，然后试着最大化p(D|参数)。即给定参数的情况下，看到样本集的概率。目标是找到使前面概率最大的参数。 逻辑回归都是基于ML做的； 缺点：不会把我们的先验知识加入模型中。 MAP（最大后验估计）：最大化p(参数|D)。 Bayesian：我们的预测是考虑了所有可能的参数，即所有的参数空间（参数的分布）。 ML和MAP都属于同一个范畴，称为（freqentist），最后的目标都是一样的：找到一个最优解，然后用最优解做预测。 三、ML 我们需要去最大化p(D|参数)，这部分优化我们通常可以用把导数设置为0的方式去得到。然而，ML估计不会把先验知识考虑进去，而且很容易造成过拟合现象。 举个例子，比如对癌症的估计，一个医生一天可能接到了100名患者，但最终被诊断出癌症的患者为5个人，在ML估计的模式下我们得到的得到癌症的概率为0.05。 这显然是不太切合实际的，因为我们根据已有的经验，我们知道这种概率会低很多。然而ML估计并没有把这种知识融入到模型里。 四、MAP 通过上面的推导我们可以发现，MAP与ML最大的不同在于p(参数)项，所以可以说MAP是正好可以解决ML缺乏先验知识的缺点，将先验知识加入后，优化损失函数。 其实p(参数)项正好起到了正则化的作用。如：如果假设p(参数)服从高斯分布，则相当于加了一个L2 norm；如果假设p(参数)服从拉普拉斯分布，则相当于加了一个L1 norm。 五、Bayesian 再次强调一下： ML和MAP只会给出一个最优的解， 然而贝叶斯模型会给出对参数的一个分布，比如对模型的参数, 假定参数空间里有参数1,参数2, 参数3,…参数N，贝叶斯模型学出来的就是这些参数的重要性（也就是分布），然后当我们对新的样本预测的时候，就会让所有的模型一起去预测，但每个模型会有自己的权重（权重就是学出来的分布）。最终的决策由所有的估计根据其权重做出决策。 模型的ensemble的却大的优点为它可以reduce variance, 这根投资很类似，比如我们投资多种不同类型的股票，总比投资某一个股票时的风险会低。 六、上面提到了frequentist和bayesian，两者之间的区别是什么？ 用一个简答的例子来再总结一下。 比如你是班里的班长，你有个问题想知道答案，你可以问所有的班里的学生。 一种方案是，问一个学习最好的同学。 另一种方案是，问所有的同学，然后把答案综合起来，但综合的时候，会按照每个同学的成绩好坏来做个权重。 第一种方案的思想类似于ML,MAP，第二种方案类似于贝叶斯模型。 七、Bayesian的难点 所以整个贝叶斯领域的核心技术就是要近似的计算 p($\theta$|D），我们称之为 bayesian inference ,说白了，这里的核心问题就是要近似这个复杂的积分（integral), 一种解决方案就是使用蒙特卡洛算法。比如我想计算一个公司所有员工的平均身高，这个时候最简答粗暴的方法就是让行政去一个一个去测量，然后计算平均值。但想计算所有中国人的平均身高，怎么做？（显然一个个测量是不可能的） 即采样。我们随机的选取一些人测量他们的身高，然后根据他们的身高来估计全国人民的审稿。当然采样的数目越多越准确，采样的数据越有代表性越准确。这就是蒙特卡洛算法的管家思想。 再例： 假设我们不知道π，但是想计算圆的面积。也可以通过采样的方法近似得到。随机再下图所示的正方形中撒入一些点，记落入红色区域的点的个数为n1,落入白色区域的个数为n2，则四分之一圆的面积就为n1/(n1+n2).——蒙特卡洛思想 那么，如何对连续函数估计呢？采样n多个数据，逼近最后的积分值。 假设我们要计算 f(x)的期望值， 我们也有p(x)这种分布，这个时候我们就可以不断的从p(x)这个分布里做一些采样，比如 x1,x2,…xn, 然后用这些采样的值去算f(x), 所以最后得到的结果就是 (f(x1) + f(x2),, + f(xn))/ n 然鹅，上面例子中提到的采样都是独立的。也就是每个样本跟其他的样本都是独立的，不影响彼此之间的采样。然而，在现实问题上，有些时候我们想加快有效样本的采样速度。这个问题讨论的就是怎么优化采样过程了，也是机器学习里一个比较大的话题了。 重申一下，用上面提到的采样方式我们可以去近似估计复杂的积分，也可以计算圆的面积，也可以计算全国人口的平均身高。但这个采样方式是独立的，有些时候，我们希望我们用更少的样本去更准确的近似某一个目标，所以就出现了sampling这种一个领域的研究，就是在研究以什么样的方式优化整个采样过程，使得过程更加高效。 MCMC这种采样方法，全称为markov chain monte carlo采样方法，就是每个采样的样本都是互相有关联性的。 但是MCMC算法需要在整个数据集上计算。也就是说为了得到一个样本，需要用所有的数据做迭代。这样当N很大时，显然不适用。而且限制了贝叶斯方法发展的主要原因就是计算复杂度太高。因此现在贝爷第领域人们最关心的问题是：怎么去优化采样，让它能够在大数据环境下学习出贝叶斯模型？ 降低迭代复杂度的一个实例： 对于逻辑回归，使用梯度下降法更新参数时，有批量梯度下降法（即使用整个数据集去更新参数），为了降低计算复杂度，人们使用了随机梯度下降法，即随机从数据集中选取样本来更新参数。 所以，能否将此思想用于MCMC采样中呢？ Yes！langevin dynamic（MCMC算法中的一种），和stochastic optimizaiton(比如随机梯度下降法)可以结合在一起用。这样，我们就可以通过少量的样本去做采样，这个时候采样的效率就不在依赖于N了，而是依赖于m, m是远远小于N。 参考文献[1] 贪心科技. 机器学习中的MLE、MAP和贝叶斯估计[DB/OL]. https://zhuanlan.zhihu.com/p/37543542, 2018-06-20. [2] 江湖小妞. 贝叶斯思想以及与最大似然估计、最大后验估计的区别[DB/OL].http://www.cnblogs.com/little-YTMM/p/5399532.html, 2018-06-20.]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>贝叶斯估计</tag>
        <tag>最大似然估计</tag>
        <tag>最大后验估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用神经网络识别手写数字——代码实现]]></title>
    <url>%2F2018%2F06%2F19%2F%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[部分代码解读在之前描述 MNIST 数据时，我说它分成了 60,000 个训练图像和 10,000 个测试图像。这是官方的 MNIST 的描述。实际上，我们将用稍微不同的方法对数据进行划分。我们将测试集保持原样，但是将 60,000 个图像的 MNIST 训练集分成两个部分：一部分 50,000 个图像，我们将用来训练我们的神经网络，和一个单独的 10,000 个图像的验证集。在本章中我们不使用验证数据，但是在本书的后面我们将会发现它对于解决如何去设置某些神经网络中的超参数是很有用的，例如学习率等，这些参数不被我们的学习算法直接选择。尽管验证数据不是原始 MNIST 规范的一部分，然而许多人以这种方式使用 MNIST，并且在神经网络中使用验证数据是很普遍的。从现在起当我提到“MNIST 训练数据”时，我指的是我们的 50,000 个图像数据集，而不是原始的 60,000图像数据集 除了 MNIST 数据，我们还需要一个叫做 Numpy 的 Python 库，用来做快速线性代数。如果你没有安装过 Numpy。在列出一个完整的代码清单之前，让我解释一下神经网络代码的核心特性。核心片段是一个 Network 类，我们用来表示一个神经网络。这是我们用来初始化一个 Network 对象的代码： 神经网络架构12345678class Network(object): def __init__(self, sizes): self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] 在这段代码中，列表 sizes 包含各层神经元的数量。例如，如果我们想创建一个在第一层有 2 个神经元，第二层有 3 个神经元，最后层有 1 个神经元的 Network 对象，我们应这样写代码： 1net = Network([2, 3, 1]) Network 对象中的权重和偏置都是被随机初始化的，使用 Numpy 的 np.random.randn! 函数来生成均值为 0，标准差为 1 的高斯分布。这样的随机初始化给了我们的随机梯度下降算法一个起点。 激活函数我们从定义 S型函数开始： 12def sigmoid(z): return 1.0/(1.0+np.exp(-z)) 前馈神经网络然后对 Network 类添加一个 feedforward 方法，对于网络给定一个输入 $a$，返回对应的输出,这个方法所做的是对每一层应用方程。 12345def feedforward(self, a): """Return the output of the network if "a" is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a 随机梯度下降算法当然，我们想要 Network 对象做的主要事情是学习。为此我们给它们一个实现随机梯度下降算法的 SGD 方法。代码如下。其中一些地方看似有一点神秘，我会在代码后面逐个分析。 123456789101112131415161718192021222324def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """Train the neural network using mini-batch stochastic gradient descent. The "training_data" is a list of tuples "(x, y)" representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If "test_data" is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.""" if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format( j, self.evaluate(test_data), n_test) else: print "Epoch &#123;0&#125; complete".format(j) training_data 是一个 (x, y) 元组的列表，表示训练输入和其对应的期望输出。变量 epochs 和 mini_batch_size 正如你预料的迭代期数量，和采样时的小批量数据的大小。eta 是学习率，$\eta$。如果给出了可选参数 test_data，那么程序会在每个训练器后评估网络，并打印出部分进展。这对于追踪进度很有用，但相当拖慢执行速度。 代码如下工作。在每个周期，它首先随机地将训练数据打乱，然后将它分成多个适当大小的小批量数据。这是一个简单的从训练数据的随机采样方法。然后对于每一个小批量数据我们应用一次梯度下降。这是通过代码 self.update_mini_batch(mini_batch, eta) 完成的，它仅仅使用 mini_batch 中的训练数据，根据单次梯度下降的迭代更新网络的权重和偏置。这是 update_mini_batch 方法的代码： 123456789101112131415def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The "mini_batch" is a list of tuples "(x, y)", and "eta" is the learning rate.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] 大部分工作由这行代码完成： 1delta_nabla_b, delta_nabla_w = self.backprop(x, y) 这行调用了一个称为反向传播的算法，一种快速计算代价函数的梯度的方法。因此 update_mini_batch 的工作仅仅是对 mini_batch 中的每一个训练样本计算梯度，然后适当地更新 self.weights 和 self.biases。 我现在不会列出 self.backprop 的代码。我们将在下章中学习反向传播是怎样工作的，包括 self.backprop 的代码。现在，就假设它按照我们要求的工作，返回与训练样本 $x$ 相关代价的适当梯度。 Network.py 完整代码让我们看一下完整的程序，包括我之前忽略的文档注释。除了 self.backprop，程序已经有了足够的文档注释，所有的繁重工作由 self.SGD 和 self.update_mini_batch 完成，对此我们已经有讨论过。self.backprop 方法利用一些额外的函数来帮助计算梯度，即 sigmoid_prime，它计算 $\sigma$ 函数的导数，以及 self.cost_derivative，这里我不会对它过多描述。你能够通过查看代码或文档注释来获得这些的要点（或者细节）。我们将在下章详细地看它们。注意，虽然程序显得很长，但是很多代码是用来使代码更容易理解的文档注释。实际上，程序只包含 74 行非空、非注释的代码。所有的代码可以在 GitHub 上找到。 注意作者的代码是 Python2 版本的，下面替换成 Python3 版本的（仅仅针对不同 Python 版本语法进行了代码修改，原理没有任何修改）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142"""network.py~~~~~~~~~~A module to implement the stochastic gradient descent learningalgorithm for a feedforward neural network. Gradients are calculatedusing backpropagation. Note that I have focused on making the codesimple, easily readable, and easily modifiable. It is not optimized,and omits many desirable features."""#### Libraries# Standard libraryimport random# Third-party librariesimport numpy as npclass Network(object): def __init__(self, sizes): """The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.""" self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): """Return the output of the network if ``a`` is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ``test_data`` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.""" test_data = list(test_data) training_data = list(training_data) if test_data: n_test = len(test_data) n = len(training_data) for j in range(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print("Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format( j, self.evaluate(test_data), n_test)) else: print("Epoch &#123;0&#125; complete".format(j)) def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforwar activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in range(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """Return the number of test inputs for which the neural network outputs the correct result. Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y)#### Miscellaneous functionsdef sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) 运行结果这个程序对识别手写数字效果如何？好吧，让我们先加载 MNIST 数据。我将用下面所描述的一小段辅助程序 mnist_loader.py 来完成。我们在一个 Python shell 中执行下面的命令， 123&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper() 在加载完 MNIST 数据之后，我们将设置一个有 30 个隐藏层神经元的 Network。我们在导入如上所列的名为 network 的 Python 程序后做， 12&gt;&gt;&gt; import network&gt;&gt;&gt; net = network.Network([784, 30, 10]) 最后，我们将使用随机梯度下降来从 MNIST training_data 学习超过 30 次 epoch，mini-batch 大小为 10，学习率 $\eta = 3.0$， 1&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data) 打印内容显示了在每轮训练期后神经网络能正确识别测试图像的数量。正如你所见到，在仅仅一次 epoch后，达到了 10,000 中选中的 9,129 个。而且数目还在持续增长， 1234567Epoch 0: 9129 / 10000Epoch 1: 9295 / 10000Epoch 2: 9348 / 10000...Epoch 27: 9528 / 10000Epoch 28: 9542 / 10000Epoch 29: 9534 / 10000 更确切地说，经过训练的网络给出的识别率约为 95% 在峰值时为 95.42%（“Epoch 28”）作为第一次尝试，这是非常令人鼓舞的。然而我应该提醒你，如果你运行代码然后得到的结果和我的不完全一样，那是因为我们使用了（不同的）随机权重和偏置来初始化我们的网络。 让我们重新运行上面的实验，将隐藏神经元数量改到 100。 12&gt;&gt;&gt; net = network.Network([784, 100, 10])&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data) 果然，它将结果提升至 96.59%。至少在这种情况下，使用更多的隐藏神经元帮助我们得到了更好的结果。（注意，有的反馈表明本实验在结果上有相当多的变化，而且一些训练运行给出的结果相当糟糕。使用第三章所介绍的技术将大大减少我们网络上这些不同训练运行性能的差别。） 当然，为了获得这些准确性，我不得不对训练的迭代期数量，mini-batch 大小和 学习率 $\eta$ 做特别的选择。正如我上面所提到的，这些在我们的神经网络中被称为超参数，以区别于通过我们的学习算法所学到的参数权重和偏置。如果我们选择了糟糕的\超参数，我们会得到较差的结果。假如我们定 学习率 为 $\eta = 0.001$, 结果则不太令人鼓舞了， 1234567Epoch 0: 1139 / 10000Epoch 1: 1136 / 10000Epoch 2: 1135 / 10000...Epoch 27: 2101 / 10000Epoch 28: 2123 / 10000Epoch 29: 2142 / 10000 然而，你可以看到网络的性能随着时间的推移慢慢地变好了。这表明应该增大 学习率，例如 $\eta = 0.01$。如果我们那样做了，我们会得到更好的结果，这表明我们应该再次增加\gls*{学习率}。（如果改变能够改善一些事情，试着做更多！）如果我们这样做几次，我们最终会得到一个像 $\eta = 1.0$ 的 学习率（或者调整到$3.0$），这跟我们之前的实验很接近。因此即使我们最初选择了糟糕的超参数，我们至少获得了足够的信息来帮助我们改善对于超参数的选择。通常，调试一个神经网络是具有挑战性的。 从这得到的教训是调试一个神经网络不是琐碎的，就像常规编程那样，它是一门艺术。你需要学习调试的艺术来获得神经网络更好的结果。更普通的是，我们需要启发式方法来选择好的超参数和好的结构。我们将在整本书中讨论这些，包括上面我是怎么样选择超参数的。 补充：加载训练数据的代码前文中，我跳过了如何加载 MNIST 数据的细节。这很简单。这里列出了完整的代码。用于存储 MNIST 数据的数据结构在文档注释中有详细描述，都是简单的类型，元组和 Numpy ndarry 对象的列表（如果你不熟悉 ndarray，那就把它们看成向量）： 原来的 mnist_loader 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677"""mnist_loader~~~~~~~~~~~~A library to load the MNIST image data. For details of the datastructures that are returned, see the doc strings for ``load_data``and ``load_data_wrapper``. In practice, ``load_data_wrapper`` is thefunction usually called by our neural network code."""#### Libraries# Standard libraryimport pickleimport gzip# Third-party librariesimport numpy as npdef load_data(): """Return the MNIST data as a tuple containing the training data, the validation data, and the test data. The ``training_data`` is returned as a tuple with two entries. The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image. The second entry in the ``training_data`` tuple is a numpy ndarray containing 50,000 entries. Those entries are just the digit values (0...9) for the corresponding images contained in the first entry of the tuple. The ``validation_data`` and ``test_data`` are similar, except each contains only 10,000 images. This is a nice data format, but for use in neural networks it's helpful to modify the format of the ``training_data`` a little. That's done in the wrapper function ``load_data_wrapper()``, see below. """ f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = pickle.load(f, encoding='bytes') f.close() return (training_data, validation_data, test_data)def load_data_wrapper(): """Return a tuple containing ``(training_data, validation_data, test_data)``. Based on ``load_data``, but the format is more convenient for use in our implementation of neural networks. In particular, ``training_data`` is a list containing 50,000 2-tuples ``(x, y)``. ``x`` is a 784-dimensional numpy.ndarray containing the input image. ``y`` is a 10-dimensional numpy.ndarray representing the unit vector corresponding to the correct digit for ``x``. ``validation_data`` and ``test_data`` are lists containing 10,000 2-tuples ``(x, y)``. In each case, ``x`` is a 784-dimensional numpy.ndarry containing the input image, and ``y`` is the corresponding classification, i.e., the digit values (integers) corresponding to ``x``. Obviously, this means we're using slightly different formats for the training data and the validation / test data. These formats turn out to be the most convenient for use in our neural network code.""" tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = list(zip(training_inputs, training_results)) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = list(zip(validation_inputs, va_d[1])) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = list(zip(test_inputs, te_d[1])) return (training_data, validation_data, test_data)def vectorized_result(j): """Return a 10-dimensional unit vector with a 1.0 in the jth position and zeroes elsewhere. This is used to convert a digit (0...9) into a corresponding desired output from the neural network.""" e = np.zeros((10, 1)) e[j] = 1.0 return e 怎么做到识别手写数字的？ 迈向深度学习虽然我们的神经网络给出了令人印象深刻的表现，但这样的表现带有几分神秘。网络中的权重和偏置是被自动发现的。这意味着我们不能立即解释网络怎么做的、做了什么。我们能否找到一些方法来理解我们的网络通过什么原理分类手写数字？并且，在知道了这些原理后，我们能做得更好吗？ 为了让这些问题更具体，我们假设数十年后神经网络引发了人工智能（AI）。到那个时候，我们能明白这种智能网络的工作机制吗？或许，因为有着自动学习得到的权重和偏置，这些是我们无法理解的，这样的神经网络对我们来说是不透明的。在人工智能的早期研究阶段，人们希望在构建人工智能的努力过程中，也同时能够帮助我们理解智能背后的机制，以及人类大脑的运转方式。但结果可能是我们既不能够理解大脑的机制，也不能够理解人工智能的机制。 为解决这些问题，让我们重新思考一下我在本章开始时所给的人工神经元的解释，作为一种衡量证据的方法。假设我们要确定一幅图像是否显示有人脸： 我们可以用解决手写识别问题的相同方式来攻克这个问题。网络的输入是图像中的像素，网络的输出是一个单个的神经元用于表明“是的，这是一张脸”或“不，这不是一张脸”。 假设我们就采取了这个方法，但接下来我们先不去使用一个学习算法。而是去尝试亲手设计一个网络，并为它选择合适的权重和偏置。我们要怎样做呢？暂时先忘掉神经网络，我们受到启发的一个想法是将这个问题分解成子问题：图像的左上角有一个眼睛吗？右上角有一个眼睛吗？中间有一个鼻子吗？下面中央有一个嘴吗？上面有头发吗？诸如此类。 如果一些问题的回答是“是”，或者甚至仅仅是“可能是”，那么我们可以作出结论这个图像可能是一张脸。相反地，如果大多数这些问题的答案是“不是”，那么这张图像可能不是一张脸。 当然，这仅仅是一个粗略的想法，而且它存在许多缺陷。也许有个人是秃头，没有头发。也许我们仅仅能看到脸的部分，或者这张脸是有角度的，因此一些面部特征是模糊的。不过这个想法表明了如果我们能够使用神经网络来解决这些子问题，那么我们也许可以通过将这些解决子问题的网络结合起来，构成一个人脸检测的神经网络。下图是一个可能的结构，其中的方框表示子网络。注意，这不是一个人脸检测问题的现实的解决方法，而是为了帮助我们构建起网络如何运转的直观感受。下图是这个网络的结构： 子网络也可以被继续分解，这看上去很合理。假设我们考虑这个问题：“左上角有一个眼睛吗？”。 这个问题可以被分解成这些子问题：“有一个眉毛吗？”，“有睫毛吗？”，“有虹膜吗？”，等等。当然这些问题也应该包含关于位置的信息，诸如“在左上角有眉毛，上面有虹膜吗？”，但是让我们先保持简单。回答问题“左上角有一个眼睛吗？”的网络能够被分解成： 这些子问题也同样可以继续被分解，并通过多个网络层传递得越来越远。最终，我们的子网络可以回答那些只包含若干个像素点的简单问题。举例来说，这些简单的问题可能是询问图像中的几个像素是否构成非常简单的形状。这些问题就可以被那些与图像中原始像素点相连的单个神经元所回答。 最终的结果是，我们设计出了一个网络，它将一个非常复杂的问题，这张图像是否有一张人脸，分解成在单像素层面上就可回答的非常简单的问题。它通过一系列多层结构来完成，在前面的网络层，它回答关于输入图像非常简单明确的问题，在后面的网络层，它建立了一个更加复杂和抽象的层级结构。包含这种多层结构，两层或更多隐藏层的网络被称为深度神经网络。 当然，我没有提到如何去递归地分解成子网络。手工设计网络中的权重和偏置无疑是不切实际的。取而代之的是，我们希望使用学习算法来让网络能够自动从训练数据中学习权重和偏置。这样，形成一个概念的层次结构。80年代和 90年代的研究人员尝试了使用随机梯度下降和反向传播来训练深度网络。不幸的是，除了一些特殊的结构，他们并没有取得很好的效果。虽然网络能够学习，但是学习速度非常缓慢，不适合在实际中使用。 自 2006 年以来，人们已经开发了一系列技术使深度神经网络能够学习。这些深度学习技术基于随机梯度下降和反向传播，并引进了新的想法。这些技术已经使更深（更大）的网络能够被训练~——~现在训练一个有 5 到 10 层隐藏层的网络都是很常见的。而且事实证明，在许多问题上，它们比那些浅层神经网络，例如仅有一个隐藏层的网络，表现的更加出色。当然，原因是深度网络能够构建起一个复杂的概念的层次结构。这有点像传统编程语言使用模块化的设计和抽象的思想来创建复杂的计算机程序。将深度网络与浅层网络进行对比，有点像将一个能够进行函数调用的程序语言与一个不能进行函数调用的精简语言进行对比。抽象在神经网络中的形式和传统的编程方式相比不同，但它同样重要。 参考文献[1] Michael Nielsen. CHAPTER 1 Using neural nets to recognize handwritten digits[DB/OL]. http://neuralnetworksanddeeplearning.com/chap1.html, 2018-06-19. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap1.tex, 2018-06-19. [3] skylook. neural-networks-and-deep-learning, mnist_loader.py[DB/OL]. https://github.com/skylook/neural-networks-and-deep-learning/blob/master/src/mnist_loader.py, 2018-06-19. [4] skylook. neural-networks-and-deep-learning, network.py[DB/OL]. https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py, 2018-06-19.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 1 Using neural nets to recognize handwritten digits</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>手写数字识别</tag>
        <tag>代码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用神经网络识别手写数字——梯度下降算法]]></title>
    <url>%2F2018%2F06%2F18%2F%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 1 Using neural nets to recognize handwritten digits 神经网络的架构假设我们有这样的网络： 前面提过，这个网络中最左边的称为输入层，其中的神经元称为输入神经元。最右边的，即输出层包含有输出神经元，在本例中，输出层只有一个神经元。中间层，既然这层中的神经元既不是输入也不是输出，则被称为隐藏层。“隐藏”这一术语也许听上去有些神秘，我第一次听到这个词，以为它必然有一些深层的哲学或数学涵意，但它实际上仅仅意味着“既非输入也非输出”。上面的网络仅有一个隐藏层，但有些网络有多个隐藏层。例如，下面的四层网络有两个隐藏层： 有些令人困惑的是，由于历史的原因，尽管是由 S 型神经元而不是感知器构成，这种多层网络有时被称为多层感知器或者 MLP。 在这本书中我不会使用 MLP 这个术语，因为我认为这会引起混淆，但这里想提醒你它的存在。 目前为止，我们讨论的神经网络，都是以上一层的输出作为下一层的输入。这种网络被称为前馈神经网络。这意味着网络中是没有回路的，信息总是向前传播，从不反向回馈。如果确实有回路，我们最终会有这样的情况：$\sigma$ 函数的输入依赖于输出。这将难于理解，所以我们不允许这样的环路。 然而，也有一些人工神经网络的模型，其中反馈环路是可行的。它们原理上比前馈网络更接近我们大脑的实际工作。并且循环网络能解决一些重要的问题，这些问题如果仅仅用前馈网络来解决，则更加困难。然而为了篇幅，本书将专注于使用更广泛的前馈网络。 一个简单的分类手写数字的网络我们将使用一个三层神经网络来识别单个数字： 网络的输入层包含给输入像素的值进行编码的神经元。就像下一节会讨论的，我们给网络的训练数据会有很多扫描得到的 $28 \times 28$ 的手写数字的图像组成，所有输入层包含有 $784 = 28 \times 28$ 个神经元。为了简化，上图中我已经忽略了 $784$ 中大部分的输入神经元。输入像素是灰度级的，值为 $0.0$ 表示白色，值为 $1.0$ 表示黑色，中间数值表示逐渐暗淡的灰色。 网络的第二层是一个隐藏层。我们用 $n$ 来表示神经元的数量，我们将给 $n$ 实验不同的数值。示例中用一个小的隐藏层来说明，仅仅包含 $n=15$ 个神经元。 网络的输出层包含有 $10$ 个神经元。如果第一个神经元激活，即输出 $\approx 1$，那么表明网络认为数字是一个 $0$。如果第二个神经元激活，就表明网络认为数字是一个 $1$。依此类推。更确切地说，我们把输出神经元的输出赋予编号 $0$ 到 $9$，并计算出那个神经元有最高的激活值。比如，如果编号为 $6$ 的神经元激活，那么我们的网络会猜到输入的数字是 $6$。其它神经元相同。 为什么用 10 个输出神经元你可能会好奇为什么我们用 $10$ 个输出神经元。毕竟我们的任务是能让神经网络告诉我们哪个数字（ $0, 1, 2, \ldots, 9$ ）能和输入图片匹配。一个看起来更自然的方式就是使用 $4$ 个输出神经元，把每一个当做一个二进制值，结果取决于它的输出更靠近 $0$ 还是$1$。四个神经元足够编码这个问题了，因为 $2^4 = 16$ 大于 $10$ 种可能的输入。为什么我们反而要用 $10$ 个神经元呢？这样做难道效率不低吗？ 最终的判断是基于经验主义的：我们可以实验两种不同的网络设计，结果证明对于这个特定的问题而言，$10$ 个输出神经元的神经网络比4个的识别效果更好。但是令我们好奇的是为什么使用 $10$ 个输出神经元的神经网络更有效呢。 有没有什么启发性的方法能提前告诉我们用10个输出编码比使用 $4$个输出编码更有好呢？ 为了理解为什么我们这么做，我们需要从根本原理上理解神经网络究竟在做些什么。首先考虑有 $10$ 个神经元的情况。我们首先考虑第一个输出神经元，它告诉我们一个数字是不是0。它能那么做是因为可以权衡从隐藏层来的信息。隐藏层的神经元在做什么呢？假设隐藏层的第一个神经元只是用于检测如下的图像是否存在： 为了达到这个目的，它通过对此图像对应部分的像素赋予较大权重，对其它部分赋予较小的权重。同理，我们可以假设隐藏层的第二，第三，第四个神经元是为检测下列图片是否存在： 就像你能猜到的，这四幅图像组合在一起构成了前面显示的一行数字图像中的 $0$： 假设神经网络以上述方式运行，我们可以给出一个貌似合理的理由去解释为什么用 $10$ 个输出而不是 $4$ 个。如果我们有 $4$ 个输出，那么第一个输出神经元将会尽力去判断数字的最高有效位是什么。 把数字的最高有效位和数字的形状联系起来并不是一个简单的问题。很难想象出有什么恰当的历史原因，一个数字的形状要素会和一个数字的最高有效位有什么紧密联系。 上面我们说的只是一个启发性的方法。没有什么理由表明这个三层的神经网络必须按照我所描述的方式运行，即隐藏层是用来探测数字的组成形状。可能一个聪明的学习算法将会找到一些合适的权重能让我们仅仅用4个输出神经元就行。但是这个启发性的方法通常很有效，它会节省你大量时间去设计一个好的神经网络结构。 使用梯度下降算法进行学习现在我们有了神经网络的设计，它怎样可以学习识别数字呢？我们需要的第一样东西是一个用来学习的数据集称为训练数据集。我们将使用 MNIST 数据集。 我们将用符号 $x$ 来表示一个训练输入。为了方便，把每个训练输入 $x$ 看作一个 $28\times 28 = 784$ 维的向量。每个向量中的项目代表图像中单个像素的灰度值。我们用 $y= y(x)$ 表示对应的期望输出，这里 $y$ 是一个 $10$ 维的向量。例如，如果有一个特定的画成 $6$ 的训练图像，$x$ ，那么 $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$ 则是网络的期望输出。注意这里 $T$ 是转置操作，把一个行向量转换成一个列向量。 二次代价函数我们希望有一个算法，能让我们找到权重和偏置，以至于网络的输出$y(x)$ 能够拟合所有的训练输入$x$。为了量化我们如何实现这个目标，我们定义一个代价函数（有时被称为损失或目标函数。我们在这本书中可能会交换使用这几个术语）： C(w,b) \equiv \frac{1}{2n} \sum_x \| y(x) - a\|^2这里 $w$ 表示所有的网络中权重的集合，$b$ 是所有的偏置，$n$ 是训练输入数据的个数，$a$ 是表示当输入为 $x$ 时输出的向量，求和则是在总的训练输入$x$ 上进行的。当然，输出 $a$ 取决于 $x$, $w$ 和 $b$，但是为了保持符号的简洁性，我没有明确地指出这种依赖关系。符号 $|v|$ 是指向量 $v$ 的模。我们把 $C$ 称为二次代价函数；有时也被称为均方误差或者 MSE。观察二次代价函数的形式我们可以看到 $C(w,b)$ 是非负的，因为求和公式中的每一项都是非负的。此外，代价函数$C(w,b)$ 的值相当小，即 $C(w,b) \approx 0$，精确地说，是当对于所有的训练输入 $x$，$y(x)$ 接近于输出 $a$ 时。因此如果我们的学习算法能找到合适的权重和偏置，使得 $C(w,b) \approx 0$，它就能很好地工作。相反，当 $C(w,b)$ 很大时就不怎么好了，那意味着对于大量地输入， $y(x)$ 与输出 $a$ 相差很大。因此我们的训练算法的目的，是最小化权重和偏置的代价函数 $C(w,b)$。换句话说，我们想要找到一系列能让代价尽可能小的权重和偏置。我们将采用称为梯度下降的算法来达到这个目的。 为什么使用二次代价函数为什么要介绍二次代价呢？毕竟我们最初感兴趣的内容不是能正确分类的图像数量吗？为什么不试着直接最大化这个数量，而是去最小化一个类似二次代价的间接评量呢？这么做是因为在神经网络中，被正确分类的图像数量所关于权重和偏置的函数并不是一个平滑的函数。 大多数情况下，对权重和偏置做出的微小变动完全不会影响被正确分类的图像的数量。这会导致我们很难去解决如何改变权重和偏置来取得改进的性能。而用一个类似二次代价的平滑代价函数则能更好地去解决如何用权重和偏置中的微小的改变来取得更好的效果。 这就是为什么我们首先专注于最小化二次代价，只有这样，我们之后才能测试分类精度。 即使已经知道我们需要使用一个平滑的代价函数，你可能仍然想知道为什么我们选择二次函数。这是临时想出来的吗？是不是我们选择另一个不同的代价函数将会得到完全不同的最小化的权重和偏置呢？这种顾虑是合理的，我们后面会再次回到这个代价函数，并做一些修改。尽管如此，二次代价函数让我们更好地理解神经网络中学习算法的基础，所以目前我们会一直使用它。 重复一下，我们训练神经网络的目的是找到能最小化二次代价函数 $C(w,b)$ 的权重和偏置。这是一个适定问题，但是现在它有很多让我们分散精力的结构，对权重 $w$ 和偏置 $b$的解释，晦涩不清的 $\sigma$ 函数，神经网络结构的选择，MNIST 等等。事实证明我们可以忽略结构中大部分，把精力集中在最小化方面来理解它。现在我们打算忘掉所有关于代价函数的具体形式、神经网络的连接等等。现在让我们想象只要最小化一个给定的多元函数。我们打算使用一种被称为梯度下降的技术来解决这样的最小化问题。然后我们回到在神经网络中要最小化的特定函数上来。 梯度下降算法好了，假设我们要最小化某些函数，$C(v)$。它可以是任意的多元实值函数，$v = v_1,v_2, \ldots$。注意我们用 $v$ 代替了 $w$ 和 $b$ 以强调它可能是任意的函数，我们现在先不局限于神经网络的环境。为了最小化 $C(v)$，想象 $C$ 是一个只有两个变量$v1$ 和 $v2$ 的函数： 为什么要使用梯度下降算法一种解决这个问题的方式是用微积分来解析最小值。我们可以计算导数去寻找 $C$ 的极值点。运气好的话，$C$ 是一个只有一个或少数几个变量的函数。但是变量过多的话那就是噩梦。而且神经网络中我们经常需要大量的变量——最大的神经网络有依赖数亿权重和偏置的代价函数，极其复杂。用微积分来计算最小值已经不可行了。 好吧，微积分是不能用了。幸运的是，有一个漂亮的推导法暗示有一种算法能得到很好的效果。首先把我们的函数想象成一个山谷。只要瞄一眼上面的绘图就不难理解。我们想象有一个小球从山谷的斜坡滚落下来。我们的日常经验告诉我们这个球最终会滚到谷底。也许我们可以用这一想法来找到函数的最小值？我们会为一个（假想的）球体随机选择一个起始位置，然后模拟球体滚落到谷底的运动。我们可以通过计算 $C$ 的导数（或者二阶导数）来简单模拟——这些导数会告诉我们山谷中局部“形状”的一切，由此知道我们的球将怎样滚动。 注意小球不必遵常规的循物理学理论，我们在这里就是上帝，能够支配球体可以如何滚动，那么我们将会采取什么样的运动学定律来让球体能够总是滚落到谷底呢？我们让小球要往代价函数梯度的反方向滚动。 为什么小球要往梯度的反方向滚动为了更精确地描述这个问题，让我们思考一下，当我们在 $v1$ 和 $v2$ 方向分别将球体移动一个很小的量，即 $\Delta v1$ 和 $\Delta v2$ 时，球体将会发生什么情况。微积分告诉我们 $C$ 将会有如下变化： \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2我们要寻找一种选择 $\Delta v_1$ 和 $\Delta v_2$ 的方法使得 $\Delta C$ 为负；即，我们选择它们是为了让球体滚落。为了弄明白如何选择，需要定义 $\Delta v$ 为 $v$ 变化的向量，$\Delta v \equiv (\Delta v_1, \Delta v_2)^T$，$T$ 是转置符号。我们也定义 $C$ 的梯度为偏导数的向量，$\left(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2}\right)^T$。我们用 $\nabla C$ 来表示梯度向量，即： \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T有了这些定义，$\Delta C$ 的表达式可以被重写为： \Delta C \approx \nabla C \cdot \Delta v这个表达式解释了为什么 $\nabla C$ 被称为梯度向量：$\nabla C$ 把 $v$ 的变化关联为 $C$ 的变化，正如我们期望的用梯度来表示。但是这个方程真正让我们兴奋的是它让我们看到了如何选取 $\Delta v$ 才能让 $\Delta C$ 为负数。假设我们选取： \Delta v = -\eta \nabla C这里的 $\eta$ 是个很小的正数（称为学习率）。方程$\Delta v = -\eta \nabla C$告诉我们 $\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta |\nabla C|^2$。由于 $| \nabla C |^2 \geq 0$，这保证了$\Delta C \leq 0$，即，如果我们按照方程$\Delta v = -\eta \nabla C$的规则去改变 $v$，那么 $C$ 会一直减小，不会增加。（当然，要在方程$\Delta C \approx \nabla C \cdot \Delta v$的近似约束下）。这正是我们想要的特性！因此我们把方程$\Delta v = -\eta \nabla C$ 用于定义球体在梯度下降算法下的“运动定律”。也就是说，我们用方程$\Delta v = -\eta \nabla C$计算 $\Delta v$，来移动球体的位置 $v$： v \rightarrow v' = v -\eta \nabla C然后我们用它再次更新规则来计算下一次移动。如果我们反复持续这样做，我们将持续减小$C$ 直到,正如我们希望的,获得一个全局的最小值。 总结一下，梯度下降算法工作的方式就是重复计算梯度 $\nabla C$，然后沿着相反的方向移动，沿着山谷“滚落”。我们可以想象它像这样： 注意具有这一规则的梯度下降并不是模仿实际的物理运动。在现实中一个球体有动量，使得它岔开斜坡滚动，甚至（短暂地）往山上滚。只有在克服摩擦力的影响，球体才能保证滚到山谷。相比之下，我们选择 $\Delta v$ 规则只是说：“往下，现在”。这仍然是一个寻找最小值的非常好的规则！ 为了使梯度下降能够正确地运行，我们需要选择足够小的学习率 $\eta$ 使得方程$\Delta C \approx \nabla C \cdot \Delta v$ 能得到很好的近似。如果不这样，我们会以 $\Delta C &gt; 0$ 结束，这显然不好。同时，我们也不想 $\eta$ 太小，因为这会使 $\Delta v$ 的变化极小，梯度下降算法就会运行得非常缓慢。在真正的实现中，$\eta$ 通常是变化的，以至方程$\Delta C \approx \nabla C \cdot \Delta v$能保持很好的近似度，但算法又不会太慢。我们后面会看这是如何工作的。 梯度下降算法的形式化证明我已经解释了具有两个变量的函数 $C$ 的梯度下降。但事实上，即使 $C$ 是一个具有更多变量的函数也能很好地工作。我们假设 $C$ 是一个有 $m$ 个变量 $v_1,\ldots,v_m$ 的多元函数。那么对 $C$ 中自变量的变化 $\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$，$\Delta C$ 将会变为： \Delta C \approx \nabla C \cdot \Delta v这里的梯度 $\nabla C$ 是向量 \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots, \frac{\partial C}{\partial v_m}\right)^T正如两个变量的情况，我们可以选取 \Delta v = -\eta \nabla C而且 $\Delta C$ 的（近似）表达式 $\Delta C \approx \nabla C \cdot \Delta v$ 保证是负数。这给了我们一种方式从梯度中去取得最小值，即使 $C$ 是任意的多元函数，我们也能重复运用更新规则 v \rightarrow v' = v-\eta \nabla C你可以把这个更新规则看做定义梯度下降算法。这给我们提供了一种方式去通过重复改变 $v$ 来找到函数 $C$ 的最小值。这个规则并不总是有效的，有几件事能导致错误，让我们无法从梯度下降来求得函数 $C$ 的全局最小值，这个观点我们会在后面的章节中去探讨。但在实践中，梯度下降算法通常工作地非常好，在神经网络中这是一种非常有效的方式去求代价函数的最小值，进而促进网络自身的学习。 事实上，甚至有一种观点认为梯度下降法是求最小值的最优策略。假设我们正在努力去改变 $\Delta v$ 来让 $C$ 尽可能地减小。这相当于最小化 $\Delta C \approx \nabla C \cdot \Delta v$。我们首先限制步长为小的固定值，即 $| \Delta v | = \epsilon$，$ \epsilon &gt; 0$。当步长固定时，我们要找到使得 $C$ 减小最大的下降方向。可以证明，使得 $\nabla C \cdot \Delta v$ 取得最小值的 $\Delta v$ 为 $\Delta v = - \eta \nabla C$，这里 $\eta = \epsilon / |\nabla C|$ 是由步长限制 $|\Delta v| = \epsilon$ 所决定的。因此，梯度下降法可以被视为一种在 $C$ 下降最快的方向上做微小变化的方法。 人们已经研究出很多梯度下降的变化形式，包括一些更接近真实模拟球体物理运动的变化形式。这些模拟球体的变化形式有很多优点，但是也有一个主要的缺点：它最终必需去计算$C$ 的二阶偏导，这代价可是非常大的。 为了理解为什么这种做法代价高，假设我们想求所有的二阶偏导 $\partial^2 C/ \partial v_j \partial v_k$。如果我们有上百万的变量$v_j$，那我们必须要计算数万亿（即百万次的平方）级别的二阶偏导（实际上，更接近万亿次的一半，因为$\partial^2 C/ \partial v_j \partial v_k = \partial^2 C/ \partial v_k \partial v_j$。同样，你知道怎么做。！）这会造成很大的计算代价。不过也有一些避免这类问题的技巧，寻找梯度下降算法的替代品也是个很活跃的研究领域。但在这本书中我们将主要用梯度下降算法（包括变化形式）使神经网络学习。 随机梯度下降算法神经网络中如何使用梯度下降算法我们怎么在神经网络中用梯度下降算法去学习呢？其思想就是利用梯度下降算法去寻找能使得方程代价函数取得最小值的权重 $w_k$ 和偏置 $b_l$。为了清楚这是如何工作的，我们将用权重和偏置代替变量 $v_j$。也就是说，现在“位置”变量有两个分量组成：$w_k$ 和 $b_l$，而梯度向量 $\nabla C$ 则有相应的分量 $\partial C / \partial w_k$和$\partial C / \partial b_l$。用这些分量来写梯度下降的更新规则，我们得到： w_k \rightarrow w_k' = w_k-\eta \frac{\partial C}{\partial w_k}b_l \rightarrow b_l' = b_l-\eta \frac{\partial C}{\partial b_l}通过重复应用这一更新规则我们就能“让球体滚下山”，并且有望能找到代价函数的最小值。换句话说，这是一个能让神经网络学习的规则。 为什么使用随机梯度下降算法应用梯度下降规则有很多挑战。我们将在下一章深入讨论。但是现在只提及一个问题。为了理解问题是什么，我们先回顾 $C(w,b) \equiv \frac{1}{2n} \sum_x | y(x) - a|^2$ 中的二次代价函数。注意这个代价函数有着这样的形式 $C = \frac{1}{n} \sum_x C_x$，即，它是遍及每个训练样本代价 $C_x \equiv \frac{|y(x)-a|^2}{2}$ 的平均值。在实践中，为了计算梯度 $\nabla C$，我们需要为每个训练输入 $x$ 单独地计算梯度值 $\nabla C_x$，然后求平均值，$\nabla C = \frac{1}{n} \sum_x \nabla C_x$。不幸的是，当训练输入的数量过大时会花费很长时间，这样会使学习变得相当缓慢。 有种叫做随机梯度下降的算法能够加速学习。其思想就是通过随机选取小量训练输入样本来计算 $\nabla C_x$，进而估算梯度 $\nabla C$。通过计算少量样本的平均值我们可以快速得到一个对于实际梯度 $\nabla C$ 的很好的估算，这有助于加速梯度下降，进而加速学习过程。 怎么使用随机梯度下降算法准确地说，随机梯度下降通过随机选取小量的 $m$ 个训练输入来工作。我们将这些随机的训练输入标记为$X_1, X_2, \ldots, X_m$，并把它们称为一个小批次。假设样本数量 $m$ 足够大，我们期望 $\nabla C_{X_j}$ 的平均值大致相等于整个 $\nabla C_x$的平均值，即， \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C这里的第二个求和符号是在整个训练数据上进行的。交换两边我们得到 \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}}证实了我们可以通过仅仅计算随机选取的小批量数据的梯度来估算整体的梯度。 为了将其明确地和神经网络的学习联系起来，假设 $w_k$ 和 $b_l$ 表示我们神经网络中权重和偏置。随机梯度下降通过随机地选取并训练输入的小批量数据来工作， w_k \rightarrow w_k' = w_k-\frac{\eta}{m}\sum_j \frac{\partial C_{X_j}}{\partial w_k}b_l \rightarrow b_l' = b_l-\frac{\eta}{m}\sum_j \frac{\partial C_{X_j}}{\partial b_l}其中两个求和符号是在当前小批量数据中的所有训练样本 $X_j$ 上进行的。然后我们再挑选另一随机选定的小批量数据去训练。直到我们用完了所有的训练输入，这被称为完成了一个训练周期。然后我们就会开始一个新的训练周期。 另外值得提一下，对于改变代价函数大小的参数，和用于计算权重和偏置 的小批量数据的更新规则，会有不同的约定。在方程$C(w,b) \equiv \frac{1}{2n} \sum_x | y(x) - a|^2$ 中，我们通过因子 $\frac{1}{n}$ 来改变整个代价函数的大小。人们有时候忽略 $\frac{1}{n}$，直接取单个训练样本的代价总和，而不是取平均值。这对我们不能提前知道训练数据数量的情况下特别有效。例如，这可能发生在有更多的训练数据是实时产生的情况下。同样，小批量数据的更新规则有时也会舍弃前面的 $\frac{1}{m}$。从概念上这会有一点区别，因为它等价于改变了学习率 $\eta$ 的大小。但在对不同工作进行详细对比时，需要对它警惕。 随机梯度下降算法与梯度下降算法比较我们可以把随机梯度下降想象成一次民意调查：在一个小批量数据上采样比对一个完整数据集进行梯度下降分析要容易得多，正如进行一次民意调查比举行一次全民选举要更容易。例如，如果我们有一个规模为 $n = 60,000$ 的训练集，就像 MNIST，并选取 小批量数据大小为 $m = 10$，这意味着在估算梯度过程中加速了 $6,000$ 倍！当然，这个估算并不是完美的，存在统计波动，但是没必要完美：我们实际关心的是在某个方向上移动来减少 $C$，而这意味着我们不需要梯度的精确计算。在实践中，随机梯度下降是在神经网络的学习中被广泛使用、十分有效的技术，它也是本书中展开的大多数学习技术的基础。 梯度下降算法一个极端的版本是把小批量数据的大小设为 $1$。即，假设一个训练输入 $x$，我们按照规则 $w_k \rightarrow w_k’ = w_k - \eta \partial C_x / \partial w_k$ 和 $b_l \rightarrow b_l’ = b_l - \eta \partial C_x / \partialb_l$ 更新我们的权重和偏置。然后我们选取另一个训练输入，再一次更新权重和偏置。如此重复。这个过程被称为在线、online、on-line、或者递增学习。在 online 学习中，神经网络在一个时刻只学习一个训练输入（正如人类做的）。 理解梯度和导数CSDN ： [机器学习] ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent） 知乎 ： 如何直观形象的理解方向导数与梯度以及它们之间的关系？ 知乎 ： 什么是全导数？ 拓展阅读全局最优解？为什么 SGD 能令神经网络的损失降到零 参考文献[1] Michael Nielsen. CHAPTER 1 Using neural nets to recognize handwritten digits[DB/OL]. http://neuralnetworksanddeeplearning.com/chap1.html, 2018-06-18. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap1.tex, 2018-06-18.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 1 Using neural nets to recognize handwritten digits</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络架构</tag>
        <tag>二次代价函数</tag>
        <tag>梯度下降算法</tag>
        <tag>随机梯度下降算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用神经网络识别手写数字——感知器]]></title>
    <url>%2F2018%2F06%2F17%2F%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E2%80%94%E2%80%94%E6%84%9F%E7%9F%A5%E5%99%A8%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 1 Using neural nets to recognize handwritten digits 本书序言在传统的编程方法中，我们告诉计算机做什么，把大问题分成许多小的、精确定义的任务，计算机可以很容易地执行。相比之下，在神经网络中，我们不告诉计算机如何解决我们的问题。相反，它从观测数据中学习，找出它自己的解决问题的方法。 一个以原理为导向的方法本书一个坚定的信念，是让读者更好地去深刻理解神经网络和深度学习，而不是像一张冗长的洗衣单一样模糊地列出一堆想法。如果你很好理解了核心理念，你就可以很快地理解其他新的推论。用编程语言对比，把这理解为掌握一种新语言的核心语法、库和数据结构。你可能仍然只是``知道’’整个编程语言的一小部分—-许多编程语言有巨大的标准库—-但新的库和数据结构可以很快且容易被理解。 这就意味着这本书的重点不是作为一个如何使用一些特定神经网络库的教程。如果你主要想围绕着某个程序库的方式去学习，那不要读这本书！找到你想学习的程序库，并通过教程和文档来完成。注意这点。虽然这也许能很快解决你的问题，但是，如果你想理解神经网络中究竟发生了什么，如果你想要了解今后几年都不会过时的原理，那么只是学习些热门的程序库是不够的。你需要领悟让神经网络工作的原理。技术来来去去，但原理是永恒的。 一个动手实践的方法我们将通过攻克一个具体的问题：教会计算机识别手写数字的问题，来学习神经网络和深度学习的核心理论。 这个问题用常规的方法来编程解决是非常困难的。然而，正如我们所看到的，它可以很好地利用一个简单的神经网络来解决，只需几十行代码，没有特别的库。更多的是，我们会通过多次迭代来改进程序，逐步融入神经网络和深度学习的核心思想。 难得有一本书能兼顾理论和动手实践。但是我相信，如果我们建立了神经网络的基本思路，你会学到最好的。我们将开发实际能用的代码，而不仅仅是抽象的理论，这些代码你可以探索和扩展。这样你就可以理解其基础，不论是理论还是实践，并且扩展和提高你的知识面。 使用神经网络识别手写数字——感知器本章我们将实现一个可以识别手写数字的神经网络。这个程序仅仅 74 行，不使用特别的神经网络库。然而，这个短小的网络不需要人类帮助便可以超过 96% 的准确率识别数字。而且，在后面的章节，我们会发展出将准确率提升到 99% 的技术。实际上，最优的商业神经网络已经足够好到被银行和邮局分别用在账单核查和识别地址上了。 手写识别常常被当成学习神经网络的原型问题，因此我们聚焦在这个问题上。作为一个原型，它具备一个关键点：挑战性，识别手写数字并不轻松，但也不会难到需要超级复杂的解决方法，或者超大规模的计算资源。另外，这其实也是一种发展出诸如深度学习更加高级的技术的方法。所以，整本书我们都会持续地讨论手写数字识别问题。本书后面部分，我们会讨论这些想法如何用在其他计算机视觉的问题或者语音、自然语言处理和其他一些领域中。 当然，如果仅仅为了编写一个计算机程序来识别手写数字，本章的内容可以简短很多！但前进的道路上，我们将扩展出很多关于神经网络的关键的思想，其中包括两个重要的人工神经元感知机和 S 型神经元），以及标准的神经网络学习算法，即随机梯度下降算法。自始至终，我专注于解释事情的原委，并构筑你对神经网络的直观感受。这需要一个漫长的讨论，而不是仅仅介绍些基本的技巧，但这对于更深入的理解是值得的。作为收益，在本章的最后，我们会准备好了解什么是深度学习，以及它为什么很重要。 感知器什么是神经网络？一开始，我将解释一种被称为“感知器”的人工神经元。今天，使用其它人工神经元模型更为普遍，在这本书中，以及更多现代的神经网络著作中，主要使用的是一种叫做 S 型的神经元模型。我们很快会讲到 S 型神经元。但是要理解为什么 S 型神经元被定义为那样的方式，值得花点时间先来理解下感知器。 感知器的定义感知器是如何工作的呢？一个感知器接受几个二进制输入，$x_1,x_2,\ldots$，并产生一个二进制输出： 示例中的感知器有三个输入，$x_1,x_2,x_3$。通常可以有更多或更少输入。Rosenblatt 提议一个简单的规则来计算输出。他引入权重，$w_1,w_2,\ldots$，表示相应输入对于输出重要性的实数。神经元的输出，$0$ 或者 $1$，则由分配权重后的总和 $\sum_j w_j x_j$ 小于或者大于一些阈值决定。和权重一样，阈值是一个实数，一个神经元的参数。用更精确的代数形式： \text{output} = \begin{cases} 0 & \quad \text{if } \sum_j w_j x_j \leq \text{ threshold} \\ 1 & \quad \text{if } \sum_j w_j x_j > \text{ threshold} \\ \end{cases}这就是一个感知器所要做的所有事情！ 目前为止我把像 $x_1$ 和 $x_2$ 这样的输入画成感知器网络左边浮动的变量。实际上，可以画一层额外的感知器输入层来方便对输入编码： 这种对有一个输出但没有输入的感知器的标记法， 是一种标准。它并不实际表示一个感知器没有输入。为了看清它，假设我们确实有一个没有输入的感知器。那么加权和 $\sum_j w_j x_j$ 会总是为零，并且感知器在 $b &gt; 0$ 时输出 $1$，当 $b \leq 0$时输出 $0$。那样，感知器会简单输出一个固定值，而不是期望值（上例中的 $x_1$）。倒不如完全不把输入感知器看作感知器，而是简单定义为输出期望值的特殊单元，$x_1, x_2,\ldots$。 感知器做决策的原理随着权重和阈值的变化，你可以得到不同的决策模型。我们来看一个由感知器构成的网络。 在这个网络中，第一列感知器我们称其为第一层感知器通过权衡输入依据做出三个非常简单的决定。那第二层的感知器呢？每一个都在权衡第一层的决策结果并做出决定。以这种方式，一个第二层中的感知器可以比第一层中的做出更复杂和抽象的决策。在第三层中的感知器甚至能进行更复杂的决策。以这种方式，一个多层的感知器网络可以从事复杂巧妙的决策。 顺便提一下，当我定义感知器时我说的是感知器只有一个输出。在上面的网络中感知器看上去像是有多个输出。实际上，他们仍然是单输出的。多个感知器输出箭头仅仅便于说明一个感知器的输出被用于其它感知器的输入。 它和把单个输出线条分叉相比，显得讨巧些。 让我们简化感知器的数学描述。条件 $\sum_j w_j x_j$ 看上去有些冗长，我们可以创建两个符号的变动来简化。第一个变动是把 $\sum_j w_j x_j$ 改写成点乘，$w\cdot x \equiv \sum_j w_j x_j$，这里 $w$ 和 $x$ 对应权重和输入的向量。第二个变动是把阈值移到不等式的另一边，并用感知器的偏置 $b \equiv -threshold$ 代替。用偏置而不是%阈值，那么感知器的规则可以重写为 \text{output} = \begin{cases} 0 & \quad \text{if } w\cdot x + b \leq 0 \\ 1 & \quad \text{if } w\cdot x + b > 0 \end{cases}我们可以把偏置看作一种表示让感知器输出 $1$（或者用生物学的术语，即激活感知器）有多容易的估算。对于具有一个非常大偏置的感知器来说，输出 $1$ 是很容易的。但是如果偏置是一个非常小的负数，输出$1$ 则很困难。很明显，引入偏置只是我们描述感知器的一个很小的变动，但是我们后面会看到它引导更进一步的符号简化。因此，在这本书的后续部分，我们不再用阈值，而总是使用偏置。 感知器与逻辑运算我已经描述过感知器是一种权衡依据来做出决策的方法。感知器被采用的另一种方式，是计算基本的逻辑功能，即我们通常认为的运算基础，例如“与”，“或”和“与非”。例如，假设我们有个两个输入的感知器，每个权重为 $-2$，整体的偏置为 $3$。这是我们的感知器， 这样我们得到：输入 $00$ 产生输出 $1$，即 (-2)0 + (-2)0 + 3 = 3 是正数。这里我用 $$ 符号来显式地表示乘法。但是输入 $11$ 产生输出 0，即 (-2)1 + (-2)*1 + 3 = -1 是负数。如此我们的感知器实现了一个与非门！ 与非门的例子显示了我们可以用感知器来计算简单的逻辑功能。实际上，我们完全能用感知器网络来计算任何逻辑功能。原因是与非门是通用运算，那样，我们能在多个与非门之上构建出任何运算。 感知器运算的通用性既是令人鼓舞的，又是令人失望的。令人鼓舞是因为它告诉我们感知器网络能和其它计算设备一样强大。但是它也令人失望，因为它看上去只不过是一种新的与非门。这简直不算个大新闻！ 然而，实际情况比这一观点认为的更好。其结果是我们可以设计学习算法，能够自动调整人工神经元的权重和偏置。这种调整可以自动响应外部的刺激，而不需要一个程序员的直接干预。这些学习算法是我们能够以一种根本区别于传统逻辑门的方式使用人工神经元。有别于显式地设计与非门或其它门，我们的神经网络能简单地学会解决问题，这些问题有时候直接用传统的电路设计是很难解决的。 S 型神经元学习算法听上去非常棒。但是我们怎样给一个神经网络设计这样的算法呢？假设我们有一个感知器网络，想要用它来解决一些问题。 引入 S 型神经元的原因例如，网络的输入可以是一幅手写数字的扫描图像。我们想要网络能学习权重和偏置，这样网络的输出能正确分类这些数字。为了看清学习是怎样工作的，假设我们把网络中的权重（或者偏置）做些微小的改动。就像我们马上会看到的，这一属性会让学习变得可能。这里简要示意我们想要的（很明显这个网络对于手写识别还是太简单了！）： 如果对权重（或者偏置）的微小的改动真的能够仅仅引起输出的微小变化，那我们可以利用这一事实来修改权重和偏置，让我们的网络能够表现得像我们想要的那样。例如，假设网络错误地把一个“9”的图像分类为“8”。我们能够计算出怎么对权重和偏置做些小的改动，这样网络能够接近于把图像分类为“9”。然后我们要重复这个工作，反复改动权重和偏置来产生更好的输出。这时网络就在学习。 问题是当我们给实际网络加上感知器时，结果并不像我们想象的那样。实际上，网络中单个感知器上一个权重或偏置的微小改动有时候会引起那个感知器的输出完全翻转，如 $0$ 变到 $1$。那样的翻转可能接下来引起其余网络的行为以极其复杂的方式完全改变。因此，虽然你的“9”可能被正确分类，网络在其它图像上的行为很可能以一些很难控制的方式被完全改变。这使得逐步修改%权重和偏置来让网络接近期望行为变得困难。也许有其它聪明的方式来解决这个问题。但是目前为止，我们还没发现有什么办法能让感知器网络进行学习。 我们可以引入一种称为 S 型神经元的新的人工神经元来克服这个问题。S 型神经元和感知器类似，但是经过修改后，权重和偏置的微小改动只引起输出的微小变化。这对于让神经元网络学习起来是很关键的。 好了, 让我来描述下 S 型神经元。我们用描绘感知器的相同方式来描绘 S 型神经元： 正如一个感知器， S 型神经元有多个输入，$x_1,x_2,\ldots$。但是这些输入可以取 $0$ 和$1$ 中的任意值，而不仅仅是 $0$ 或 $1$。例如，$0.638\ldots$ 是一个 S 型神经元的有效输入。同样， S 型神经元对每个输入有权重，$w_1,w_2,\ldots$，和一个总的偏置，$b$。但是输出不是 $0$ 或 $1$。相反，它现在是 $\sigma(w \cdot x+b)$，这里 $\sigma$ 被称为 S型函数（顺便提一下，$\sigma$ 有时被称为逻辑函数，而这种新的神经元类型被称为逻辑神经元。既然这些术语被很多从事于神经元网络的人使用，记住它是有用的。然而，我们将继续使用 S 型这个术语。）定义为： \sigma(z) \equiv \frac{1}{1+e^{-z}}把它们放在一起来更清楚地说明，一个具有输入 $x_1,x_2,\ldots$，权重$w_1,w_2,\ldots$，和偏置 $b$ 的 S 型神经元的输出是： \frac{1}{1+\exp(-\sum_j w_j x_j-b)}S 型神经元的性质初看上去， S 型神经元和感知器有很大的差别。如果你不熟悉 S型函数的代数形式，它看上去晦涩难懂又令人生畏。实际上，感知器和 S 型神经元之间有很多相似的地方，跨过理解上的障碍，S 型神经元的代数形式具有很多技术细节。 为了理解和感知器模型的相似性，假设 $z \equiv w \cdot x + b$ 是一个很大的正数。那么 $e^{-z} \approx 0$ 而 $\sigma(z) \approx 1$。即，当 $z = w \cdotx+b$ 很大并且为正， S 型神经元的输出近似为 $1$，正好和感知器一样。相反地，假设 $z = w \cdot x+b$ 是一个很大的负数。那么$e^{-z} \rightarrow \infty$，$\sigma(z) \approx 0$。所以当 $z = w \cdot x +b$ 是一个很大的负数， S 型神经元的行为也非常近似一个感知器。只有在 $w \cdot x+b$ 取中间值时，和感知器模型有比较大的偏离。 $\sigma$ 的代数形式又是什么？我们怎样去理解它呢？实际上，$\sigma$ 的精确形式不重要，重要的是这个函数绘制的形状。是这样： 这个形状是阶跃函数平滑后的版本： 如果 $\sigma$ 实际是个阶跃函数，既然输出会依赖于 $w\cdot x+b$ 是正数还是负数。（实际上，当 $w \cdot x +b = 0$ ，感知器输出 $0$，而同时阶跃函数输出$1$。所以严格地说，我们需要修改阶跃函数来符合这点。但是你知道怎么做。）那么S型神经元会成为一个感知器。利用实际的 $\sigma$ 函数，我们得到一个，就像上面说明的，平滑的感知器。确实，$\sigma$ 函数的平滑特性，正是关键因素，而不是其细部形式。$\sigma$ 的平滑意味着权重和%偏置的微小变化，即 $\Delta w_j$ 和 $\Delta b$，会从神经元产生一个微小的输出变化 $\Delta output$。实际上，微积分告诉我们 $\Delta output$ 可以很好地近似表示为： \Delta output \approx \sum_j \frac{\partial \, output}{\partial w_j} \Delta w_j + \frac{\partial \, output}{\partial b} \Delta b其中求和是在所有权重 $w_j$ 上进行的，而 $\partial \, output /\partial w_j$ 和$\partial \, output /\partial b$ 符号表示 $output$ 分别对于 $w_j$ 和 $b$ 的偏导数。如果偏导数这个概念让你不安，不必惊慌。上面全部用偏导数的表达式看上去很复杂，实际上它的意思非常简单（这可是个好消息）：$\Delta output$ 是一个反映权重和偏置变化，即 $\Delta w_j$ 和$\Delta b$，的线性函数。利用这个线性特性，我们比较容易细微地修改权重和偏置的值，从而获得我们需要的细微的输出变化。所以，因为 S 型神经元具有与感知器类似的本质行为，它们可以帮助我们了解权重和偏置的变化如何影响输出值。 如果对 $\sigma$ 来说重要的是形状而不是精确的形式，那为什么要在公式$\sigma(z) \equiv \frac{1}{1+e^{-z}}$中给 $\sigma$ 使用特定的形式呢？事实上，在下文我们还将不时地考虑一些神经元，它们给其它激活函数 $f(\cdot)$ 输出是 $f(w \cdot x + b)$。当我们使用一个不同的激活函数，最大的变化是公式$\Delta output \approx \sum_j \frac{\partial \, output}{\partial w_j} \Delta w_j + \frac{\partial \, output}{\partial b} \Delta b$ 中用于偏导数的特定值的改变。事实证明当我们后面计算这些偏导数，用 $\sigma$ 会简化数学计算，这是因为指数在求导时有些可爱的属性。无论如何，$\sigma$ 在神经网络的工作中被普遍使用，并且是这本书中我们最常使用的激活函数。 S 型神经元与感知器的关系一个感性的认识是：S 型神经元是阶跃函数平滑后的版本。 但可以在数学上证明如下两点： 假设我们把一个感知器网络中的所有权重和偏置乘以一个正的常数，$c&gt;0$。可以证明网络的行为并没有改变。 假设我们有上题中相同的设置，一个感知器网络。同样假设所有输入被选中。我们不需要实际的输入值，仅仅需要固定这些输入。假设对于网络中任何特定感知器的输入 $x$， 权重和偏置遵循 $w \cdot x + b\neq 0$。现在用 S 型神经元替换所有网络中的感知器，并且把权重和偏置乘以一个正的常量 $c&gt;0$。证明在 $c \rightarrow \infty$的极限情况下， S 型神经元网络的行为和感知器网络的完全一致。当一个感知器的 $w \cdot x + b = 0$ 时又为什么会不同？ 解释 S 型神经元的输出我们应该如何解释一个 S 型神经元的输出呢？很明显，感知器和 S 型神经元之间一个很大的不同是 S 型神经元不仅仅输出 $0$ 或 $1$。它可以输出 $0$ 到 $1$ 之间的任何实数，所以诸如 $0.173\ldots$ 和 $0.689\ldots$ 的值是合理的输出。这是非常有用的，例如，当我们想要输出来表示一个神经网络的图像像素输入的平均强度。但有时候这会是个麻烦。假设我们希望网络的输出表示“输入图像是一个9”或“输入图像不是一个9”。很明显，如果输出是 $0$ 或 $1$ 是最简单的，就像用感知器。但是在实践中，我们可以设定一个约定来解决这个问题，例如，约定任何至少为 $0.5$ 的输出为表示 “这是一个9”，而其它小于 $0.5$ 的输出为表示“不是一个9”。当我们正在使用这样的约定时，我总会清楚地提出来，这样就不会引起混淆。 人工神经元的其它模型到现在，我们使用的神经元都是 S 型神经元。理论上讲，从这样类型的神经元构建起来的神经网络可以计算任何函数。实践中，使用其他模型的神经元有时候会超过 S 型网络。取决于不同的应用，基于其他类型的神经元的网络可能会学习得更快，更好地泛化到测试集上，或者可能两者都有。让我们给出一些其他的模型选择，便于了解常用的模型上的变化。 可能最简单的变种就是 tanh 神经元，使用双曲正切函数替换了 S 型函数。输入为 $x$，权重向量为 $w$，偏置为 $b$ 的 tanh 神经元的输出是 \tanh(w \cdot x+b)这其实和 S 型神经元关系相当密切。回想一下 $\tanh$ 函数的定义： \tanh(z) \equiv \frac{e^z-e^{-z}}{e^z+e^{-z}}进行简单的代数运算，我们可以得到 \sigma(z) = \frac{1+\tanh(z/2)}{2}也就是说，$\tanh$ 仅仅是 S 型函数的按比例变化版本。我们同样也能用图像看看 $\tanh$ 的形状： 这两个函数之间的一个差异就是 $\tanh$ 神经元的输出的值域是 $(-1, 1)$ 而非 $(0,1)$。这意味着如果你构建基于 $\tanh$ 神经元，你可能需要正规化最终的输出（取决于应用的细节，还有你的输入），跟 sigmoid 网络略微不同。 类似于 S 型神经元，基于 S 型神经元的网络可以在理论上，计算任何将输入映射到$(-1, 1)$ 的函数（对于 tanh 和 S 型神经元，这个说法有一些技术上的预先声明。然而，非正式地，通常可以把神经网络看做可以以任意精度近似任何函数。）而且，诸如反向传播和随机梯度这样的想法也能够轻松地用在 tanh-neuron 神经元构成的网络上的。 参考文献[1] Michael Nielsen. CHAPTER 1 Using neural nets to recognize handwritten digits[DB/OL]. http://neuralnetworksanddeeplearning.com/chap1.html, 2018-06-17. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap1.tex, 2018-06-17. [3] Michael Nielsen. CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-28. [4] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-28.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 1 Using neural nets to recognize handwritten digits</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>感知器</tag>
        <tag>Sigmoid函数</tag>
        <tag>S 型神经元</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[README]]></title>
    <url>%2F2018%2F05%2F20%2FREADME%2F</url>
    <content type="text"><![CDATA[yuanxiaosc.github.io个人博客；机器学习；深度学习；Python学习；]]></content>
  </entry>
  <entry>
    <title><![CDATA[学习资源归类]]></title>
    <url>%2F2017%2F07%2F28%2F%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E5%BD%92%E7%B1%BB%EF%BC%88%E7%BD%AE%E9%A1%B6%EF%BC%89%2F</url>
    <content type="text"><![CDATA[学习手册导航LATEX学习手册MarkDown学习手册NumPy学习手册1.0版 AIhttp://tools.google.com/seedbank/Collection of Interactive Machine Learning Examples JupyterGitHub | 教程 | data-science-ipython-notebooks 数据科学常用库的 jupyter notebook 教程 TensorFlowGoogle | 官网 | tensorflow 掘金翻译 | 翻译 | TensorFlow 官方文档中文版 香港科技大学计算机系教授 Sung KimGitHub | 课程代码 | DeepLearningZeroToAll 香港科技大学TensorFlow三天速成课件 PytorchFacebook | 官网 | PyTorch PyTorch 中文网 | 中文网 | PyTorch GitHub | 课程代码 | PyTorchZeroToAll PyTorchZeroToAll课件 Python知乎 | 代码教程 | 13 个 python3 才能用的特性 模型选择模型选择的一些基本思想和方法 PycharmWindows下的Pycharm远程连接虚拟机中Centos下的Python环境 面试算法工程师（机器学习） GitHub | 中山大学 郑永川 | 2018秋招笔记 书籍 Stanford | 书籍 | UFLDL Tutorial 本教程将教你无监督的特征学习和深度学习的主要思想。 O’Reilly book | 书籍 | Hands-on Machine Learning with Scikit-Learn and TensorFlow GitHub | 书籍代码 | Hands-on Machine Learning with Scikit-Learn and TensorFlowApachecn | 书籍翻译 | Sklearn 与 TensorFlow 机器学习实用指南 书籍下载区]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
</search>
