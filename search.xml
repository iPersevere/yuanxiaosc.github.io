<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Beginning Application Development with TensorFlow and Keras（路易斯卡佩罗）]]></title>
    <url>%2F2018%2F08%2F08%2FBeginning%20Application%20Development%20with%20TensorFlow%20and%20Keras%EF%BC%88%E8%B7%AF%E6%98%93%E6%96%AF%E5%8D%A1%E4%BD%A9%E7%BD%97%EF%BC%89%2F</url>
    <content type="text"><![CDATA[购买地址《Beginning Application Development with TensorFlow and Keras》| Luis Capelo | May 2018 阅读和下载地址PDF 书籍配套代码GitHub 代码整理Jupyter nbviewer 读书笔记]]></content>
      <categories>
        <category>深度学习</category>
        <category>Beginning Application Development with TensorFlow and Keras（路易斯卡佩罗）</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文资源]]></title>
    <url>%2F2018%2F08%2F07%2F%E8%AE%BA%E6%96%87%2F</url>
    <content type="text"><![CDATA[论文实现谷歌全attention机器翻译模型Transformer的TensorFlow实现 强化学习Deepmind | AlphaGo Zero: Learning from scratch | 2017 论文翻译 自然语言处理NLP 研究灵感库 GitHub | 进展 | NLP-progress跟踪自然语言处理进展，包括数据集和最常见的NLP任务的当前最先进的状态。机器之心 | 翻译 | NLP-progress Sentiance | 2018-05-03 | Loc2Vec: Learning location embeddings with triplet-loss networks 机器之心 | 翻译 | 使用三重损失网络学习位置嵌入：让位置数据也能进行算术运算 以自监督的方式学习位置数据并从中提取见解。Sentiance 开发了一款能接收加速度计、陀螺仪和位置信息等智能手机传感器数据并从中提取出行为见解的平台。该平台能学习用户的模式，并能预测和解释事情发生的原因和时间。 2018-07-09-9 | NLP领域的ImageNet时代到来：词嵌入「已死」，语言模型当立 计算机视觉领域常使用在 ImageNet 上预训练的模型，它们可以进一步用于目标检测、语义分割等不同的 CV 任务。而在自然语言处理领域中，我们通常只会使用预训练词嵌入向量编码词汇间的关系，因此也就没有一个能用于整体模型的预训练方法。Sebastian Ruder 表示语言模型有作为整体预训练模型的潜质，它能由浅到深抽取语言的各种特征，并用于机器翻译、问答系统和自动摘要等广泛的 NLP 任务。Ruder 同样展示了用语言模型做预训练模型的效果，并表示 NLP 领域中的「ImageNet」终要到来。 达观数据 | 2018-07-25 | NLP概述和文本自动分类算法详解 图像处理知乎 | 进展 | 深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读 2018-07-10-6 | OpenAI提出可逆生成模型Glow 目前，生成对抗网络 GAN 被认为是在图像生成等任务上最为有效的方法，越来越多的学者正朝着这一方向努力：在计算机视觉顶会 CVPR 2018 上甚至有 8% 的论文标题中包含 GAN。近日来自 OpenAI 的研究科学家 Diederik Kingma 与 Prafulla Dhariwal 却另辟蹊径，提出了基于流的生成模型 Glow。据介绍，该模型不同于 GAN 与 VAE，而在生成图像任务上也达到了令人惊艳的效果。 知识图谱2018-01-25 | DKN：深度知识网络新闻推荐在线新闻推荐系统旨在解决新闻的信息爆炸并为用户提供个性化推荐。一般来说，新闻语言高度浓缩，充满知识实体和常识。然而，现有方法不知道这种外部知识，并且不能完全发现新闻之间的潜在知识级连接。因此，用户的推荐结果仅限于简单模式，无法合理扩展。此外，新闻推荐还面临着新闻时间敏感度高，用户兴趣多样化的挑战。为了解决上述问题，本文提出了一种深度知识感知网络（DKN），它将知识图谱表示结合到新闻推荐中。&gt;]]></content>
      <categories>
        <category>论文</category>
        <category>论文资源</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Numpy 的副作用]]></title>
    <url>%2F2018%2F08%2F02%2FNumpy%E7%9A%84%E5%89%AF%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[array_split(X, X.shape[0], axis=0)将数组切分时，会保留原始的数组维度。 concatenate(x, axis=0)用于将多个数组进行连接，这与stack函数很容易混淆，他们之间的区别是concatenate会把当前要匹配的元素降一维，即去掉最外面那一层括号。 Reshape、转置、旋转、轴对换区别1import numpy as np 12X = np.arange(0,15,1).reshape((3,5))X array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 1X.reshape((5,3)) array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) 1X.T array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12# 轴对换X.swapaxes(0,1) array([[ 0, 5, 10], [ 1, 6, 11], [ 2, 7, 12], [ 3, 8, 13], [ 4, 9, 14]]) 12# 顺时针旋转 1 个 90 度np.rot90(X, 1, axes=(1,0)) array([[10, 5, 0], [11, 6, 1], [12, 7, 2], [13, 8, 3], [14, 9, 4]]) 1np.rot90(X, 1, axes=(0,1)) array([[ 4, 9, 14], [ 3, 8, 13], [ 2, 7, 12], [ 1, 6, 11], [ 0, 5, 10]]) 12C2 = np.rot90(X, 2, axes=(1,0))C2 array([[14, 13, 12, 11, 10], [ 9, 8, 7, 6, 5], [ 4, 3, 2, 1, 0]]) 1np.rot90(C2, 2, axes=(0,1)) array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]]) 12X2 = np.arange(0,24,1).reshape((2,3,4))X2 array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) 1X2.swapaxes(1,2) array([[[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]], [[12, 16, 20], [13, 17, 21], [14, 18, 22], [15, 19, 23]]]) 1X2.transpose(0,2,1) array([[[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]], [[12, 16, 20], [13, 17, 21], [14, 18, 22], [15, 19, 23]]])]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hands-on Machine Learning 课后习题解答]]></title>
    <url>%2F2018%2F08%2F01%2FHands-on%20Machine%20Learning%20with%20Scikit-Learn%20and%20TensorFlow%20%E4%B9%A0%E9%A2%98%E8%A7%A3%E7%AD%94%2F</url>
    <content type="text"><![CDATA[本文是 《Hands-on Machine Learning with Scikit-Learn and TensorFlow 》的课后习题解答！ 把练习和答案分开是为了督促学习和思考，提倡独立思考，主动学习，而不是背答案！文章前半段中文翻译有问题的地方欢迎评论指正，英文原文在后面。 CHAPTER 1 The Machine Learning Landscape练习本章中，我们学习了一些机器学习中最为重要的概念。下一章，我们会更加深入，并写一些代码。开始下章之前，确保你能回答下面的问题： 如何定义机器学习？ 机器学习可以解决的四类问题？ 什么是带标签的训练集？ 最常见的两个监督任务是什么？ 指出四个常见的非监督任务？ 要让一个机器人能在各种未知地形行走，你会采用什么机器学习算法？ 要对你的顾客进行分组，你会采用哪类算法？ 垃圾邮件检测是监督学习问题，还是非监督学习问题？ 什么是在线学习系统？ 什么是核外学习？ 什么学习算法是用相似度做预测？ 模型参数和学习算法的超参数的区别是什么？ 基于模型学习的算法搜寻的是什么？最成功的策略是什么？基于模型学习如何做预测？ 机器学习主要的挑战是什么？ 如果模型在训练集上表现好，但推广到新实例表现差，问题是什么？给出三个可能的解决方案。 什么是测试集，为什么要使用它？ 验证集的目的是什么？ 如果用测试集调节超参数，会发生什么？ 什么是交叉验证，为什么它比验证集好？ 练习解答 机器学习是关于构建可以从数据中学习的模型。学习意味着在某些任务中，根据一些绩效衡量，模型可以更好。 机器学习非常适用于： 我们没有算法解决方案的复杂问题； 可以替换手工调整规则的长列表； 构建适应波动环境的系统； 最后帮助人类学习（例如，数据挖掘） 。提示：不要把所以问题往机器学习上套，比如做网页，比如已经有高效算法的（图联通判断）。 标记的训练集是一个训练集，其中包含每个实例的所需解决方案（例如标签）。 两个最常见的监督任务是回归和分类。 常见的无监督任务包括聚类，可视化，降维和关联规则学习。 强化学习如果我们希望机器人学会在各种未知的地形中行走，那么学习可能会表现得最好，因为这通常是强化学习所解决的问题类型。有可能将问题表达为监督或半监督学习问题，但这种解决方式不太自然。 如果您不知道如何定义组，则可以使用聚类算法（无监督学习）将客户划分为类似客户的集群。但是，如果您知道您希望拥有哪些组，那么您可以将每个组的许多示例提供给分类算法（监督学习），并将所有客户分类到这些组中。 垃圾邮件检测是一种典型的监督学习问题：算法会输入许多电子邮件及其标签（垃圾邮件或非垃圾邮件）。 在线学习系统可以逐步学习，而不是批量学习系统。这使它能够快速适应不断变化的数据和自治系统，以及对大量数据的培训。 核外算法可以处理大量无法容纳在计算机主存中的数据。核心学习算法将数据分成小批量，并使用在线学习技术从这些小批量中学习。 基于实例的学习系统用心学习训练数据;然后，当给定一个新实例时，它使用相似性度量来查找最相似的学习实例并使用它们进行预测。 模型具有一个或多个模型参数，其确定在给定新实例的情况下它将预测什么（例如，线性模型的斜率）。学习算法试图找到这些参数的最佳值，以便模型很好地推广到新实例。超参数是学习算法本身的参数，而不是模型的参数（例如，要应用的正则化的量）。 基于模型的学习算法搜索模型参数的最佳值，使得模型将很好地推广到新实例。我们通常通过最小化成本函数来训练这样的系统，该成本函数测量系统在对训练数据进行预测时的糟糕程度，以及如果模型正规化则对模型复杂性的惩罚。为了进行预测，我们使用学习算法找到的参数值将新实例的特征提供给模型的预测函数。 机器学习中的一些主要挑战是缺乏数据，数据质量差，非代表性数据，无法提供信息的特征，过于简单的模型以及过度拟合训练数据的模型，以及过度复杂的模型过度拟合数据。 如果一个模型在训练数据上表现很好，但对新实例表现不佳，那么该模型可能会过度拟合训练数据（或者我们对训练数据非常幸运）。过度拟合的可能解决方案是获得更多数据，简化模型（选择更简单的算法，减少所使用的参数或特征的数量，或使模型正规化），或减少训练数据中的噪声。 测试集用于估计模型在生产中启动之前模型将对新实例进行的泛化错误。 验证集用于比较模型。它可以选择最佳模型并调整超参数。 如果使用测试集调整超参数，则存在过度拟合测试集的风险，并且您测量的泛化错误将是乐观的（您可能会启动比预期更差的模型）。 交叉验证是一种技术，可以比较模型（模型选择和超参数调整），而无需单独的验证集。这节省了宝贵的培训数据。 ExercisesIn this chapter we have covered some of the most important concepts in Machine Learning. In the next chapters we will dive deeper and write more code, but before we do, make sure you know how to answer the following questions: How would you define Machine Learning? Can you name four types of problems where it shines? What is a labeled training set? What are the two most common supervised tasks? Can you name four common unsupervised tasks? What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? What type of algorithm would you use to segment your customers into multiple groups? Would you frame the problem of spam detection as a supervised learning prob‐lem or an unsupervised learning problem? What is an online learning system? What is out-of-core learning? What type of learning algorithm relies on a similarity measure to make predic‐tions? What is the difference between a model parameter and a learning algorithm’s hyperparameter? What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions? Can you name four of the main challenges in Machine Learning? If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions? What is a test set and why would you want to use it? What is the purpose of a validation set? What can go wrong if you tune hyperparameters using the test set? What is cross-validation and why would you prefer it to a validation set? Exercise Solutions Machine Learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure. Machine Learning is great for complex problems for which we have no algorith‐mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. The two most common supervised tasks are regression and classification. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning. Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to express the problem as a supervised or semisupervised learning problem, but it would be less natural. If you don’t know how to define the groups, then you can use a clustering algo‐rithm (unsupervised learning) to segment your customers into clusters of similar customers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups. Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam). An online learning system can learn incrementally, as opposed to a batch learn‐ing system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data. Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer’s main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini-batches. An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned instances and uses them to make predictions. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the sys‐tem is at making predictions on the training data, plus a penalty for model com‐plexity if the model is regularized. To make predictions, we feed the new instance’s features into the model’s prediction function, using the parameter val‐ues found by the learning algorithm. Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple mod‐els that underfit the training data, and excessively complex models that overfit the data. If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters. If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect). Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate vali‐dation set. This saves precious training data. Chapter 13: Convolutional Neural Networks练习 CNN相对于完全连接的DNN有什么优势可用于图像分类？ 考虑由三个卷积层组成的CNN，每个卷积层具有3×3个内核，步长为2，以及SAME填充。最下层输出100个特征图，中间一个输出200，顶部输出400.输入图像是200×300像素的RGB图像。 CNN中的参数总数是多少？如果我们使用32位浮点数，那么在对单个实例进行预测时，该网络至少需要多少RAM？什么时候对50个图像的小批量培训？ 如果您的GPU在训练CNN时内存不足，您可以尝试解决问题的五件事情是什么？ 为什么要添加最大池化层而不是具有相同步幅的卷积层？ 您希望何时添加本地响应规范化层？ 与LeNet-5相比，您能说出AlexNet的主要创新吗？ GoogLeNet和ResNet的主要创新如何？ 练习解答1.这是CNN相对于完全连接的DNN进行图像分类的主要优点： 因为连续的层只是部分连接，并且因为它重复使用其权重，所以CNN的参数比完全连接的DNN少得多，这使得训练速度更快，降低了过度拟合的风险，并且需要的训练数据要少得多。 当CNN学习了可以检测特定功能的内核时，它可以在图像的任何位置检测到该功能。相反，当DNN在一个位置学习一个特征时，它只能在该特定位置检测到它。由于图像通常具有非常重复的特征，因此使用较少的训练示例，CNN能够比DNN更好地用于图像处理任务（例如分类）。 最后，DNN没有关于如何组织像素的先验知识;它不知道附近的像素是否接近。 CNN的架构嵌入了这一先验知识。较低层通常识别图像的小区域中的特征，而较高层将较低层特征组合成较大特征。这适用于大多数自然图像，使CNN与DNN相比具有决定性的先机性。 让我们计算CNN有多少参数。由于其第一个卷积层具有3×3个内核，并且输入具有三个通道（红色，绿色和蓝色），因此每个特征图具有3×3×3个权重，加上偏置项。这是每个功能图的28个参数。由于该第一卷积层具有100个特征映射，因此它具有总共2,800个参数。第二卷积层具有3×3个核，其输入是前一层的100个特征映射的集合，因此每个特征映射具有3×3×100 = 900个权重，加上偏差项。由于它有200个特征图，因此该层具有901×200 = 180,200个参数。最后，第三个和最后一个卷积层也有3×3个核，其输入是前一个层的200个特征映射的集合，因此每个特征映射具有3×3×200 = 1,800个权重，加上一个偏置项。由于它有400个特征图，因此该图层总共有1,801×400 = 720,400个参数。总而言之，CNN有2,800 + 180,200 + 720,400 = 903,400个参数。现在让我们计算这个神经网络在对单个实例进行预测时需要多少RAM（至少）。首先让我们计算每一层的特征图大小。由于我们使用2和SAME填充的步幅，因此要素图的水平和垂直尺寸在每一层被除以2（必要时向上舍入），因此输入通道为200×300像素，第一层的特征地图是100×150，第二层的特征地图是50×75，第三层的特征地图是25×38。因为32位是4个字节而第一个卷积层有100个特征地图，所以第一层需要4 x 100×150×100 = 600万字节（约5.7 MB，考虑到1 MB = 1,024 KB和1 KB = 1,024字节）。第二层占用4×50×75×200 = 300万字节（约2.9MB）。最后，第三层占用4×25×38×400 = 1,520,000字节（约1.4MB）。但是，一旦计算了一个层，就可以释放前一层占用的内存，因此如果一切都经过优化，只需要6 + 9 = 1500万字节（约14.3 MB）的RAM（第二层时）刚刚计算过，但第一层占用的内存尚未释放）。但是等等，你还需要添加CNN参数占用的内存。我们之前计算过它有903,400个参数，每个参数使用4个字节，所以这增加了3,613,600个字节（大约3.4 MB）。所需的总RAM是（至少）18,613,600字节（约17.8 MB）。最后，让我们计算在50个图像的小批量训练CNN时所需的最小RAM量。在训练期间，TensorFlow使用反向传播，这需要保留在前向传递期间计算的所有值，直到反向传递开始。因此，我们必须计算单个实例的所有层所需的总RAM，并将其乘以50。那时让我们开始以兆字节而不是字节计数。我们之前计算过，每个实例的三层分别需要5.7,2.9和1.4 MB。每个实例总共10.0 MB。因此，对于50个实例，总RAM为500 MB。再加上输入图像所需的RAM，即50×4×200×300×3 = 36百万字节（约34.3 MB），加上模型参数所需的RAM，大约3.4 MB（之前计算过）加上一些用于渐变的RAM（我们将忽略它们，因为它们可以逐渐释放，因为反向传播在反向传递过程中向下传播）。我们总共大约500.0 + 34.3 + 3.4 = 537.7 MB。这真的是一个乐观的最低限度。 如果您的GPU在训练CNN时内存不足，可以尝试解决问题的五件事情（除了购买具有更多RAM的GPU）： 减少小批量。 在一个或多个图层中使用更大的步幅减少维度。 删除一个或多个图层。 使用16位浮点数而不是32位浮点数。 在多个设备上分发CNN。 最大池层根本没有参数，而卷积层有很多参数（参见前面的问题）。 局部响应归一化层使得最强烈激活的神经元在相同位置但在相邻特征图中抑制神经元，这促使不同的特征图专门化并将它们分开，迫使它们探索更广泛的特征。 它通常在较低层中使用，以具有较大的低级特征池，上层可以构建在其上。 与LeNet-5相比，AlexNet的主要创新是：（1）它更大更深，（2）它将卷积层直接叠加在一起，而不是在每个卷积层的顶部堆叠汇集层。 GoogLeNet的主要创新是引入了初始模块，这使得有可能拥有比以前的CNN架构更深的网络，参数更少。 最后，ResNet的主要创新是跳过连接的引入，这使得它可以超越100层。 可以说，它的简洁性和一致性也相当具有创新性。 Exercises What are the advantages of a CNN over a fully connected DNN for image classi‐fication? Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN?If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images? If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem? Why would you want to add a max pooling layer rather than a convolutional layer with the same stride? When would you want to add a local response normalization layer? Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet and ResNet? Exercise Solutions These are the main advantages of a CNN over a fully connected DNN for image classification: Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data. When a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples. Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN’s architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start com‐pared to DNNs. Let’s compute how many parameters the CNN has. Since its first convolutional layer has 3 × 3 kernels, and the input has three channels (red, green, and blue), then each feature map has 3 × 3 × 3 weights, plus a bias term. That’s 28 parame‐ters per feature map. Since this first convolutional layer has 100 feature maps, it has a total of 2,800 parameters. The second convolutional layer has 3 × 3 kernels, and its input is the set of 100 feature maps of the previous layer, so each feature map has 3 × 3 × 100 = 900 weights, plus a bias term. Since it has 200 feature maps, this layer has 901 × 200 = 180,200 parameters. Finally, the third and last convolutional layer also has 3 × 3 kernels, and its input is the set of 200 feature maps of the previous layers, so each feature map has 3 × 3 × 200 = 1,800 weights, plus a bias term. Since it has 400 feature maps, this layer has a total of 1,801 × 400 = 720,400 parameters. All in all, the CNN has 2,800 + 180,200 + 720,400 = 903,400 parameters.Now let’s compute how much RAM this neural network will require (at least) when making a prediction for a single instance. First let’s compute the feature map size for each layer. Since we are using a stride of 2 and SAME padding, the horizontal and vertical size of the feature maps are divided by 2 at each layer (rounding up if necessary), so as the input channels are 200 × 300 pixels, the first layer’s feature maps are 100 × 150, the second layer’s feature maps are 50 × 75, and the third layer’s feature maps are 25 × 38. Since 32 bits is 4 bytes and the first convolutional layer has 100 feature maps, this first layer takes up 4 x 100 × 150 × 100 = 6 million bytes (about 5.7 MB, considering that 1 MB = 1,024 KB and 1 KB = 1,024 bytes). The second layer takes up 4 × 50 × 75 × 200 = 3 million bytes (about 2.9 MB). Finally, the third layer takes up 4 × 25 × 38 × 400 = 1,520,000 bytes (about 1.4 MB). However, once a layer has been computed, the memory occupied by the previous layer can be released, so if everything is well optimized, only 6 + 9 = 15 million bytes (about 14.3 MB) of RAM will be required (when the second layer has just been computed, but the memory occupied by the first layer is not released yet). But wait, you also need to add the memory occupied by the CNN’s parameters. We computed earlier that it has 903,400 parameters, each using up 4 bytes, so this adds 3,613,600 bytes (about 3.4 MB). The total RAM required is (at least) 18,613,600 bytes (about 17.8 MB).Lastly, let’s compute the minimum amount of RAM required when training the CNN on a mini-batch of 50 images. During training TensorFlow uses backpropa‐gation, which requires keeping all values computed during the forward pass until the reverse pass begins. So we must compute the total RAM required by all layers for a single instance and multiply that by 50! At that point let’s start counting in megabytes rather than bytes. We computed before that the three layers require respectively 5.7, 2.9, and 1.4 MB for each instance. That’s a total of 10.0 MB per instance. So for 50 instances the total RAM is 500 MB. Add to that the RAM required by the input images, which is 50 × 4 × 200 × 300 × 3 = 36 million bytes (about 34.3 MB), plus the RAM required for the model parameters, which is about 3.4 MB (computed earlier), plus some RAM for the gradients (we will neglect them since they can be released gradually as backpropagation goes down the layers during the reverse pass). We are up to a total of roughly 500.0 + 34.3 + 3.4 = 537.7 MB. And that’s really an optimistic bare minimum. If your GPU runs out of memory while training a CNN, here are five things you could try to solve the problem (other than purchasing a GPU with more RAM): Reduce the mini-batch size. Reduce dimensionality using a larger stride in one or more layers. Remove one or more layers. Use 16-bit floats instead of 32-bit floats. Distribute the CNN across multiple devices. A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few (see the previous questions). A local response normalization layer makes the neurons that most strongly acti‐vate inhibit neurons at the same location but in neighboring feature maps, which encourages different feature maps to specialize and pushes them apart, forcing them to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon. The main innovations in AlexNet compared to LeNet-5 are (1) it is much larger and deeper, and (2) it stacks convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. The main innovation in GoogLeNet is the introduction of inception modules, which make it possible to have a much deeper net than previous CNN architectures, with fewer parameters. Finally, ResNet’s main innovation is the introduction of skip connec‐tions, which make it possible to go well beyond 100 layers. Arguably, its simplic‐ity and consistency are also rather innovative. Chapter 14: Recurrent Neural Networks练习 你能想象 seq2seq RNN 的几个应用吗？ seq2vec 的 RNN 呢？vex2seq 的 RNN 呢？ 为什么人们使用编解码器 RNN 而不是简单的 seq2seq RNN 来自动翻译？ 如何将卷积神经网络与 RNN 结合，来对视频进行分类？ 使用 dynamic_rnn() 而不是 static_rnn() 构建 RNN 有什么好处？ 你如何处理长度可变的输入序列？ 那么长度可变输出序列呢？ 在多个 GPU 上分配深层 RNN 的训练和执行的常见方式是什么？ 练习解答 以下是一些RNN应用程序： 对于序列到序列的RNN：预测天气（或任何其他时间序列），机器翻译（使用编码器 - 解码器架构），视频字幕，语音到文本，音乐生成（或其他序列生成），识别 一首歌的和弦。 对于序列到矢量RNN：按音乐类型对音乐样本进行分类，分析书评的情绪，根据大脑植入物的读数预测失语症患者正在考虑的单词，预测概率 用户希望根据她的观看历史观看电影（这是协作过滤的许多可能实现之一）。 对于矢量到序列RNN：图像字幕，基于当前艺术家的嵌入创建音乐播放列表，基于一组参数生成旋律，在图片中定位行人。 一般来说，如果你一次翻译一个单词，结果将是非常可怕的。 例如，法语句子“Je vous en prie”的意思是“欢迎你”，但如果你一次翻译一个词，你会得到“我在祷告。”嗯？ 首先阅读整个句子然后翻译它会好得多。 普通的序列到序列RNN将在读取第一个字之后立即开始翻译句子，而编码器 - 解码器RNN将首先读取整个句子然后翻译它。 也就是说，人们可以想象一个简单的序列到序列的RNN，只要不确定接下来要说什么就会输出静音（就像人类翻译者必须翻译直播时那样）。 为了基于视觉内容对视频进行分类，一种可能的架构可以是（比方说）每秒一帧，然后通过卷积神经网络运行每一帧，将CNN的输出馈送到序列到矢量RNN ，最后通过softmax层运行其输出，为您提供所有类概率。 对于培训，您只需使用交叉熵作为成本函数。 如果您也想将音频用于分类，您可以将每秒音频转换为摄谱仪，将此摄谱仪输入CNN，并将此CNN的输出馈送到RNN（以及其他CNN的相应输出））。 使用dynamic_rnn（）而不是static_rnn（）构建RNN具有以下几个优点： 它基于while_loop（）操作，该操作能够在反向传播期间将GPU的内存交换到CPU的内存，从而避免内存不足错误。 它可以说更容易使用，因为它可以直接将单个张量作为输入和输出（涵盖所有时间步骤），而不是张量列表（每个时间步长一个）。 无需堆叠，取消堆叠或转置。 它生成一个较小的图形，更容易在TensorBoard中可视化。 5.要处理可变长度输入序列，最简单的选项是在调用static_rnn（）或dynamic_rnn（）函数时设置sequence_length参数。 另一种选择是填充较小的输入（例如，用零）以使它们与最大输入相同（如果输入序列都具有非常相似的长度，则这可能比第一选项快）。 要处理可变长度输出序列，如果事先知道每个输出序列的长度，可以使用sequence_length参数（例如，考虑序列到序列的RNN，用暴力标记视频中的每一帧 得分：输出序列与输入序列的长度完全相同）。 如果您事先不知道输出序列的长度，则可以使用填充技巧：始终输出相同大小的序列，但忽略序列结束标记之后的任何输出（通过在计算时忽略它们） 成本函数）。 要在多个GPU上分发深度RNN的训练和执行，常见的技术就是将每个层放在不同的GPU上（参见第12章）。 Exercises Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN? Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation? How could you combine a convolutional neural network with an RNN to classify videos? What are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()? How can you deal with variable-length input sequences? What about variable-length output sequences? What is a common way to distribute training and execution of a deep RNN across multiple GPUs? Exercise Solutions Here are a few RNN applications: For a sequence-to-sequence RNN: predicting the weather (or any other time series), machine translation (using an encoder–decoder architecture), video captioning, speech to text, music generation (or other sequence generation), identifying the chords of a song. For a sequence-to-vector RNN: classifying music samples by music genre, ana‐lyzing the sentiment of a book review, predicting what word an aphasic patient is thinking of based on readings from brain implants, predicting the probabil‐ity that a user will want to watch a movie based on her watch history (this is one of many possible implementations of collaborative filtering). For a vector-to-sequence RNN: image captioning, creating a music playlist based on an embedding of the current artist, generating a melody based on a set of parameters, locating pedestrians in a picture (e.g., a video frame from a self-driving car’s camera). In general, if you translate a sentence one word at a time, the result will be terri‐ble. For example, the French sentence “Je vous en prie” means “You are welcome,” but if you translate it one word at a time, you get “I you in pray.” Huh? It is much better to read the whole sentence first and then translate it. A plain sequence-to-sequence RNN would start translating a sentence immediately after reading the first word, while an encoder–decoder RNN will first read the whole sentence and then translate it. That said, one could imagine a plain sequence-to-sequence RNN that would output silence whenever it is unsure about what to say next (just like human translators do when they must translate a live broadcast). To classify videos based on the visual content, one possible architecture could be to take (say) one frame per second, then run each frame through a convolutional neural network, feed the output of the CNN to a sequence-to-vector RNN, and finally run its output through a softmax layer, giving you all the class probabili‐ties. For training you would just use cross entropy as the cost function. If you wanted to use the audio for classification as well, you could convert every second of audio to a spectrograph, feed this spectrograph to a CNN, and feed the output of this CNN to the RNN (along with the corresponding output of the other CNN). Building an RNN using dynamic_rnn() rather than static_rnn() offers several advantages: It is based on a while_loop() operation that is able to swap the GPU’s memory to the CPU’s memory during backpropagation, avoiding out-of-memory errors. It is arguably easier to use, as it can directly take a single tensor as input and output (covering all time steps), rather than a list of tensors (one per time step). No need to stack, unstack, or transpose. It generates a smaller graph, easier to visualize in TensorBoard. To handle variable length input sequences, the simplest option is to set the sequence_length parameter when calling the static_rnn() or dynamic_rnn() functions. Another option is to pad the smaller inputs (e.g., with zeros) to make them the same size as the largest input (this may be faster than the first option if the input sequences all have very similar lengths). To handle variable-length out‐put sequences, if you know in advance the length of each output sequence, you can use the sequence_length parameter (for example, consider a sequence-to-sequence RNN that labels every frame in a video with a violence score: the output sequence will be exactly the same length as the input sequence). If you don’t know in advance the length of the output sequence, you can use the padding trick: always output the same size sequence, but ignore any outputs that come after the end-of-sequence token (by ignoring them when computing the cost function). To distribute training and execution of a deep RNN across multiple GPUs, a common technique is simply to place each layer on a different GPU (see Chap‐ter 12). Chapter 15: Autoencoders练习 自动编码器使用的主要任务是什么？ 假设你想训练一个分类器，你有很多未标记的训练数据，但是只有几千个标记的实例。自动编码器如何帮助？你将如何进行？ 如果一个自动编码器完美地重建输入，它一定是好的吗？自动编码器？如何评价自动编码器的性能？ 什么是欠完备和过完备的自动编码器？过度完备的自动编码器的主要风险是什么？超完备自动编码器的主要风险是什么？ 如何在堆叠式自动编码器中系紧砝码？这样做有什么意义呢？ 什么是一种常见的技术来可视化的特点，学习下层的堆叠自动编码器？更高层怎么办？ 什么是生成模型？你能说出一种生成式自动编码器吗？ 练习解答 以下是自动编码器用于的一些主要任务： 特征提取 无人监督的预训练 维度降低 生成模型 异常检测（自动编码器通常不利于重建异常值） 如果你想训练一个分类器，你有大量的未标记的训练数据，但是只有几千个标记的实例，那么你可以首先在完整的数据集（标记和未标记）上训练一个深度的自动编码器，然后再将其下半部分用于分类器（即，重复使用编码层）。编码层，包括使用标记数据训练分类器。如果您的标记数据很少，则可能需要在训练分类器时冻结重复使用的层。 自动编码器完美地重建其输入的事实并不一定如此意味着它是一个很好的自动编码器;也许它只是一个过度完整的自动编码器学会了将其输入复制到编码层然后再输出到输出。实际上，即使编码层包含单个神经元，也是可能的对于一个非常深的自动编码器来学习将每个训练实例映射到不同的编码（例如，第一个实例可以映射到0.001，第二个实例可以映射到0.002，即第三到0.003，等等），它可以“用心”学习重建右边每个编码的训练实例。它将完美地重建其输入没有真正学习数据中任何有用的模式。在实践中这样的映射不太可能发生，但它说明了完美的重建不是这样的事实保证自动编码器学到了什么有用的东西。但是，如果它产生非常糟糕的重建，然后它几乎保证是一个糟糕的自动编码器。为了评估自动编码器的性能，一种选择是测量重建损失（例如，计算MSE，输出的均方值）减去输入）。再次，高重建损失是一个很好的迹象自动编码器很糟糕，但重建损失很小并不能保证好。您还应该根据它将使用的内容来评估自动编码器对于。例如，如果您将其用于无人监督的分类器预训练，那么你还应该评估分类器的性能。 欠完全自动编码器是一种编码层小于编码层的编码器输入和输出层。 如果它更大，那么它是一个过完备的自动编码器。欠完全自动编码器的主要风险是它可能无法完成重建输入。 过度完整的自动编码器的主要风险是它可能只是将输入复制到输出，而不学习任何有用的功能。 要将编码器层及其相应解码器层的权重联系起来简单地使解码器权重等于编码器权重的转置。这会将模型中的参数数量减少一半，通常会进行培训通过较少的训练数据更快地收敛，并降低过度拟合的风险训练集。 为了可视化由堆叠自动编码器的下层学习的特征，通常的技术是通过将每个权重向量重新整形为输入图像的大小来简单地绘制每个神经元的权重（例如，对于MNIST，重塑一个权重向量）。 形状[784]至[28,28]）。 为了可视化更高层学习的特征，一种技术是显示最能激活每个神经元的训练实例。 生成模型是能够随机生成类似于训练实例的输出的模型。 例如，一旦在MNIST数据集上成功训练，生成模型可用于随机生成数字的真实图像。 输出分布通常类似于训练数据。 例如，由于MNIST包含每个数字的许多图像，因此生成模型将输出大致相同数量的每个数字的图像。 一些生成模型可以参数化。例如，仅生成某种输出。 生成自动编码器的一个例子是变分自动编码器。 Exercise What are the main tasks that autoencoders are used for? Suppose you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances. How can autoencoders help? How would you proceed? If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder? What are undercomplete and overcomplete autoencoders? What is the main risk of an excessively undercomplete autoencoder? What about the main risk of an overcomplete autoencoder? How do you tie weights in a stacked autoencoder? What is the point of doing so? What is a common technique to visualize features learned by the lower layer of a stacked autoencoder? What about higher layers? What is a generative model? Can you name a type of generative autoencoder? Exercise Solutions Here are some of the main tasks that autoencoders are used for: Feature extraction Unsupervised pretraining Dimensionality reduction Generative models Anomaly detection (an autoencoder is generally bad at reconstructing outliers) If you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances, then you could first train a deep autoencoder on the full dataset (labeled + unlabeled), then reuse its lower half for the classifier (i.e., reuse the layers up to the codings layer, included) and train the classifier using the labeled data. If you have little labeled data, you probably want to freeze the reused layers when training the classifier. The fact that an autoencoder perfectly reconstructs its inputs does not necessarily mean that it is a good autoencoder; perhaps it is simply an overcomplete autoen‐coder that learned to copy its inputs to the codings layer and then to the outputs. In fact, even if the codings layer contained a single neuron, it would be possible for a very deep autoencoder to learn to map each training instance to a different coding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the third to 0.003, and so on), and it could learn “by heart” to reconstruct the right training instance for each coding. It would perfectly reconstruct its inputs without really learning any useful pattern in the data. In practice such a mapping is unlikely to happen, but it illustrates the fact that perfect reconstructions are not a guarantee that the autoencoder learned anything useful. However, if it produces very bad reconstructions, then it is almost guaranteed to be a bad autoencoder. To evaluate the performance of an autoencoder, one option is to measure the reconstruction loss (e.g., compute the MSE, the mean square of the outputs minus the inputs). Again, a high reconstruction loss is a good sign that the autoencoder is bad, but a low reconstruction loss is not a guarantee that it is good. You should also evaluate the autoencoder according to what it will be used for. For example, if you are using it for unsupervised pretraining of a classifier, then you should also evaluate the classifier’s performance. An undercomplete autoencoder is one whose codings layer is smaller than the input and output layers. If it is larger, then it is an overcomplete autoencoder. The main risk of an excessively undercomplete autoencoder is that it may fail to reconstruct the inputs. The main risk of an overcomplete autoencoder is that it may just copy the inputs to the outputs, without learning any useful feature. To tie the weights of an encoder layer and its corresponding decoder layer, you simply make the decoder weights equal to the transpose of the encoder weights. This reduces the number of parameters in the model by half, often making train‐ing converge faster with less training data, and reducing the risk of overfitting the training set. To visualize the features learned by the lower layer of a stacked autoencoder, a common technique is simply to plot the weights of each neuron, by reshaping each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape [784] to [28, 28]). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic images of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative model would output roughly the same number of images of each digit. Some generative models can be parametrized—for example, to generate only some kinds of outputs. An example of a generative autoencoder is the variational autoencoder. Chapter 16: Reinforcement Learning练习1.您如何定义强化学习？ 它与常规监督或无监督学习有什么不同？2.您能想到本章未提及的RL的三种可能应用吗？ 对于他们每个人来说，环境是什么？ 代理商是什么？可能的行动是什么？ 有什么奖励？3.折扣率是多少？ 如果修改计数率，最优政策会改变吗？4.您如何衡量强化学习代理的表现？5.什么是信用分配问题？ 什么时候发生？ 你怎么能减轻它？6.使用重放内存有什么意义？7.什么是非策略RL算法？ 练习解答 强化学习是机器学习的一个领域，旨在创建能够以最大化奖励的方式在环境中采取行动的代理。 RL与常规监督和无监督学习之间存在许多差异。以下是一些： 在有监督和无监督学习中，目标通常是在数据中找到模式。在强化学习中，目标是找到一个好的策略。 与监督学习不同，代理人没有明确给出“正确”的答案。它必须通过反复试验来学习。 与无监督学习不同，通过奖励有一种监督形式。我们不会告诉代理如何执行任务，但我们会告诉它何时进行任务或何时失败。 强化学习代理需要在探索环境，寻找获得奖励的新方法以及利用已经知道的奖励来源之间找到适当的平衡点。相比之下，有监督和无监督的学习系统通常不需要担心探索;他们只是根据他们给出的训练数据。 在有监督和无监督的学习中，训练实例通常是独立的（事实上，它们通常是洗牌的）。在强化学习中，连续观察通常不是独立的。在移动之前，代理可能会在环境的同一区域停留一段时间，因此连续的观察将非常相关。在某些情况下，使用重放存储器来确保训练算法获得相当独立的观察。 除了第16章中提到的那些之外，以下是强化学习的一些可能应用： 音乐个性化 环境是用户的个性化网络电台。代理是决定该用户接下来要播放的歌曲的软件。其可能的行动是播放目录中的任何歌曲（它必须尝试选择用户将喜欢的歌曲）或播放广告（它必须尝试选择用户将被介入的广告）。每次用户收听歌曲时获得小奖励，每次用户收听广告时获得更大奖励，当用户跳过歌曲或广告时获得负奖励，如果用户离开则获得非常负面奖励。 营销 环境是贵公司的营销部门。代理商是一个软件，根据他们的个人资料和购买历史记录定义应向哪些客户发送邮件活动（对于每个客户，它有两个可能的操作：发送或不发送）。它会对邮寄广告系列的费用产生负面回报，并对此广告系列产生的估算收入产生积极回报。 产品交付 让代理商控制一批运货卡车，决定他们应该在油库接收什么，他们应该去哪里，他们应该放下什么，等等。对于按时交付的每种产品，他们都会得到积极的回报，对于延迟交付，他们会得到负面的回报。 在估算行动的价值时，强化学习算法通常会将此行为带来的所有奖励加起来，给予即时奖励更多的权重，减少后期奖励的权重（考虑到行动对近期的影响大于在遥远的未来）。为了对此进行建模，通常在每个时间步应用折扣率。例如，在折扣率为0.9的情况下，当您估算行动的价值时，两个时间段后收到的100的奖励仅计为0.92×100 = 81。您可以将计算率视为衡量未来相对于现在的估值程度的指标：如果它非常接近1，那么未来的估值几乎与现在一样多。如果它接近0，那么只有直接奖励很重要。当然，这极大地影响了最优政策：如果你重视未来，你可能愿意为最终奖励的前景忍受很多直接的痛苦，而如果你不重视未来，你就会抓住您可以找到的任何直接奖励，永远不会投资于未来。 要衡量强化学习代理的表现，您可以简单地总结其获得的奖励。 在模拟环境中，您可以运行许多epi-sodes并查看平均得到的总奖励（并且可能会查看最小值，最大值，标准差等）。 信用分配问题是，当强化学习代理收到奖励时，它无法直接了解其先前的哪些行为对此奖励有贡献。 它通常发生在一个动作与所产生的奖励之间存在很大的延迟时（例如，在Atari的乒乓球比赛期间，在球员击球之前和赢得该球的那一刻之间可能会有几十个时间步长）。 减轻它的一种方法是在可能的情况下为代理人提供短期奖励。 这通常需要有关任务的先验知识。 例如，如果我们想要建立一个学会下棋的代理人，而不是仅在它赢得比赛时给予奖励，我们可以在每次捕获对手的棋子时给予奖励。 代理人通常可以在一段时间内保持在其环境的同一区域，因此在这段时间内，它的所有经验都非常相似。 这可以在学习算法中引入一些偏差。 它可能会调整这个环境区域的政策，但一旦离开这个区域就不会表现良好。 要解决此问题，您可以使用重放内存; 代理人不会仅使用最直接的学习经验，而是根据过去经验的缓冲来学习，最近也不是最近的经历（也许这就是为什么我们在晚上做梦：重播我们当天的经历并更好地学习 他们？）。 非策略RL算法学习最优策略的值（即，如果代理最佳地行为，则可以为每个状态预期的折扣奖励的总和），而与代理实际行为的方式无关。 Q-Learning是这种算法的一个很好的例子。 相反，on-policy算法学习代理实际执行的策略的值。 Exercises How would you define Reinforcement Learning? How is it different from regular supervised or unsupervised learning? Can you think of three possible applications of RL that were not mentioned in this chapter? For each of them, what is the environment? What is the agent?What are possible actions? What are the rewards? What is the discount rate? Can the optimal policy change if you modify the dis‐count rate? How do you measure the performance of a Reinforcement Learning agent? What is the credit assignment problem? When does it occur? How can you allevi‐ate it? What is the point of using a replay memory? What is an off-policy RL algorithm? Exercise Solutions Reinforcement Learning is an area of Machine Learning aimed at creating agents capable of taking actions in an environment in a way that maximizes rewards over time. There are many differences between RL and regular supervised and unsupervised learning. Here are a few: In supervised and unsupervised learning, the goal is generally to find patterns in the data. In Reinforcement Learning, the goal is to find a good policy. Unlike in supervised learning, the agent is not explicitly given the “right” answer. It must learn by trial and error. Unlike in unsupervised learning, there is a form of supervision, through rewards. We do not tell the agent how to perform the task, but we do tell it when it is making progress or when it is failing. A Reinforcement Learning agent needs to find the right balance between exploring the environment, looking for new ways of getting rewards, and exploiting sources of rewards that it already knows. In contrast, supervised and unsupervised learning systems generally don’t need to worry about explora‐tion; they just feed on the training data they are given. In supervised and unsupervised learning, training instances are typically inde‐pendent (in fact, they are generally shuffled). In Reinforcement Learning, con‐secutive observations are generally not independent. An agent may remain in the same region of the environment for a while before it moves on, so consecu‐tive observations will be very correlated. In some cases a replay memory is used to ensure that the training algorithm gets fairly independent observa‐tions. Here are a few possible applications of Reinforcement Learning, other than those mentioned in Chapter 16: Music personalization The environment is a user’s personalized web radio. The agent is the software deciding what song to play next for that user. Its possible actions are to play any song in the catalog (it must try to choose a song the user will enjoy) or to play an advertisement (it must try to choose an ad that the user will be inter‐ested in). It gets a small reward every time the user listens to a song, a larger reward every time the user listens to an ad, a negative reward when the user skips a song or an ad, and a very negative reward if the user leaves. Marketing The environment is your company’s marketing department. The agent is the software that defines which customers a mailing campaign should be sent to, given their profile and purchase history (for each customer it has two possi‐ble actions: send or don’t send). It gets a negative reward for the cost of the mailing campaign, and a positive reward for estimated revenue generated from this campaign. Product delivery Let the agent control a fleet of delivery trucks, deciding what they should pick up at the depots, where they should go, what they should drop off, and so on. They would get positive rewards for each product delivered on time, and negative rewards for late deliveries. When estimating the value of an action, Reinforcement Learning algorithms typ‐ically sum all the rewards that this action led to, giving more weight to immediate rewards, and less weight to later rewards (considering that an action has more influence on the near future than on the distant future). To model this, a discount rate is typically applied at each time step. For example, with a discount rate of 0.9, a reward of 100 that is received two time steps later is counted as only 0.92 × 100 = 81 when you are estimating the value of the action. You can think of the dis‐count rate as a measure of how much the future is valued relative to the present: if it is very close to 1, then the future is valued almost as much as the present. If it is close to 0, then only immediate rewards matter. Of course, this impacts the optimal policy tremendously: if you value the future, you may be willing to put up with a lot of immediate pain for the prospect of eventual rewards, while if you don’t value the future, you will just grab any immediate reward you can find, never investing in the future. To measure the performance of a Reinforcement Learning agent, you can simply sum up the rewards it gets. In a simulated environment, you can run many epi‐sodes and look at the total rewards it gets on average (and possibly look at the min, max, standard deviation, and so on). The credit assignment problem is the fact that when a Reinforcement Learning agent receives a reward, it has no direct way of knowing which of its previous actions contributed to this reward. It typically occurs when there is a large delay between an action and the resulting rewards (e.g., during a game of Atari’s Pong, there may be a few dozen time steps between the moment the agent hits the ball and the moment it wins the point). One way to alleviate it is to provide the agent with shorter-term rewards, when possible. This usually requires prior knowledge about the task. For example, if we want to build an agent that will learn to play chess, instead of giving it a reward only when it wins the game, we could give it a reward every time it captures one of the opponent’s pieces. An agent can often remain in the same region of its environment for a while, so all of its experiences will be very similar for that period of time. This can intro‐duce some bias in the learning algorithm. It may tune its policy for this region of the environment, but it will not perform well as soon as it moves out of this region. To solve this problem, you can use a replay memory; instead of using only the most immediate experiences for learning, the agent will learn based on a buffer of its past experiences, recent and not so recent (perhaps this is why we dream at night: to replay our experiences of the day and better learn from them?). An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of discounted rewards that can be expected for each state if the agent acts opti‐mally), independently of how the agent actually acts. Q-Learning is a good exam‐ple of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploi‐tation.]]></content>
      <categories>
        <category>机器学习</category>
        <category>hands-on-ml-with-sklearn-and-tf(Aurelien Geron)</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ALL_Tensorflow]]></title>
    <url>%2F2018%2F07%2F27%2FALL_Tensorflow%2F</url>
    <content type="text"><![CDATA[教程·短文GitHub | EffectiveTensorflow | 20180727]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四川大学研究生软件项目管理期末考试分析]]></title>
    <url>%2F2018%2F07%2F09%2F%E5%9B%9B%E5%B7%9D%E5%A4%A7%E5%AD%A6%E7%A0%94%E7%A9%B6%E7%94%9F%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[目标 Understanding the role of software project management in software development Understanding why software project management practices are important Knowing good basic software project management practices Possessing basic skills using software management tools and practices Having applied those skills in software developing with realistic challenges 理解软件项目管理在软件开发中的作用理解为什么软件项目管理实践很重要了解良好的基本软件项目管理实践拥有使用软件管理工具和实践的基本技能将这些技能应用到软件开发中，并具有现实的挑战 Course outline (24 hours) week 1: Introduction of Software Project Management week 2: Project evaluation and programme management week 3: Overview of project planning &amp; Selection of project approach week 4: Software effort estimation week 5: Activity planning week 6: Risk management week 7: Resource allocation week 8: Monitoring and control: contracts, people, team, and quality week 9-17: Team work week 18: Finial Exam 第1周：软件项目管理的引入第2周：项目评估和方案管理第3周：项目规划和项目方法选择的概述第4周：软件工作评估星期5:活动计划第6周:风险管理第七周:资源分配第8周：监控和控制：合同、人员、团队和质量上行线周:团队合作精神18周:顶尖考试 week 2:Project evaluation and programme management项目评估和方案管理We plan to invest 5000 for a project and wish to get 12500 profit after five years. Suppose that there is 2500 profit for every year and the interest rate is 12%. Please compute the NPV and DPBP for the project.我们计划为一个项目投资5000美元，并希望在五年后获得12500美元的利润。 假设每年有2500美元的利润，利率为12％。 请计算项目的NPV和DPBP。 Year Cash-flow Discount Factor Discount cash flow 0 -5000 1.0000 -5000 1 2500 0.8929 2232.14 2 2500 0.7972 1992.98 3 2500 0.7118 1779.45 4 2500 0.6355 1588.84 5 2500 0.5674 1418.53 NP 7500 NPV 4011.94 净利润 $\text{NP(Net profit)}=sum(\text{cash-flow})=7500$ 年平均利润 $\text{average annual profit}=\dfrac{NP}{years}=\dfrac{7500}{5}=1500$ 投资回报率 $\text{ROI(Return on investment )}=\dfrac{\text{Average annual profit}}{\text{Total investment}}=\dfrac{1500}{10000}=0.15$ 折扣因子 Discount factor$\text{Discount factor}=1 / {(1+r)}^t$r is the interest rate (e.g. 10% is 0.10)t is the number of years r is the interest rate (e.g. 10% is 0.10)t is the number of years $\text{DPBP(Dynamic Pay Back Period)} = \text{year.when(sum(Discount cash flow)&gt;0)}=3 years$ 知识点 Year 0’ represents all the costs before system is operation ‘Cash-flow’ is value of income less outgoing Net profit value of all the cash-flows for the lifetime of the application Internal rate of return (IRR) is the discount rate that would produce an NPV of 0 for the project 内部收益率 Internal rate of return (IRR) 某项目期初投资200万，以后的10年每年都有30万的现金流，求该项目的内部收益率（IRR）。（注：利率插值区间宽度小于1%即可）解答：内部收益率（IRR），是指项目投资实际可望达到的收益率，实质上，它是能使项目的净现值等于零时的折现率。-200+[30/(1+IRR)+30/(1+IRR)^2+….+30/(1+IRR)^10]=0 , IRR=8.14%注意：将IRR=8.14%代入上式后结果为0.0375。另外，用Excel函数IRR求得结果为8.14%。 week 3: Overview of project planning &amp; Selection of project approach第3周：项目规划和项目方法选择的概述An invoicing system is to have the following components: amend invoice, produce invoice, produce monthly statements, record cash payment, clear paid invoices from database, create customer records, delete customer. 发票系统具有以下组成部分：修改发票，生成发票，生成月结单，记录现金支付，清除数据库中的付款发票，创建客户记录，删除客户。 (a) What physical dependencies govern the order in which these transactions are implemented?哪些物理依赖关系决定了这些交易的实施顺序？ Create customer 2. Delete customer 3. Produce invoice Amend invoice 5. Payment 6.Clear paid invoices Produce monthly statements (b) How could the system be broken down into increments which would be of some value to the users (hint – think about the problems of taking existing details onto a database when a system is first implemented).如何将系统分解为对用户有一定价值的增量（提示 - 考虑在首次实施系统时将现有细节带入数据库的问题）。 increment 1: Create customer, Delete customer increment 2: Produce invoice, Amend invoice increment 3: Payment, Clear paid invoices increment 4: Produce monthly statements 知识点 产品分解结构 Products CAN BE deliverable or intermediate产品可以是可交付的或中间的 Could be Products(among other things)physical thing (‘installed pc’),a document (‘logical data structure’)a person (‘trained user’)a new version of an old product (‘updated software’) The following are NOT normally products:activities (e.g. ‘training’)events (e.g. ‘interviews completed’)resources and actors (e.g. ‘software developer’) - may be exceptions to this 产品流程图 瀑布模型imposes structure on the projectevery stage needs to be checked and signed offBUTlimited scope for iteration“经典”模型将结构强加于项目每个阶段都需要检查和签名但有限的空间迭代 V-process model Reasons for prototyping原型设计的理由 learning by doing improved communication improved user involvement a feedback loop is established reduces the need for documentation reduces maintenance costs i.e. changes after the application goes live prototype can be used for producing expected results在实践中学习改进的通信改进的用户参与建立了一个反馈回路减少对文档的需求减少维护成本，即应用程序运行后的更改原型可以用于产生预期的结果 Prototyping: some dangers原型:一些危险 users may misunderstand the role of the prototype lack of project control and standards possible additional expense of building prototype focus on user-friendly interface could be at expense of machine efficiency用户可能误解了原型的作用缺乏项目控制和标准建筑原型的额外费用专注于用户友好的界面可能会牺牲机器的效率 渐进的过程 Incremental approach:benefits增量的方法:好处 feedback from early stages used in developing latter stages shorter development thresholds user gets some benefits earlier project may be put aside temporarily reduces ‘gold-plating’开发后阶段使用的早期阶段的反馈缩短开发阈值用户可以更早获得一些好处项目可以暂时搁置减少“画蛇添足” BUT there are some possible disadvantages loss of economy of scale ‘software breakage’ steps ideally 1% to 5% of the total project non-computer steps should be included ideal if a step takes one month or less:not more than three months each step should deliver some benefit to the user some steps will be physically dependent on others规模经济损失“软件破损each step should deliver some benefit to the usersome steps will be physically dependent on others每一步都应该给用户带来一些好处有些步骤将在物理上依赖于他人 V/C ratios: V is a score 1-10 representing value to customerC is a score 0-10 representing value to developers ‘Agile’ methods“敏捷”的方法 structured development methods have some perceived disadvantages： produce large amounts of documentation which can be largely unread documentation has to be kept up to date division into specialist groups and need to follow procedures stifles communication users can be excluded from decision process long lead times to deliver anything etc. etc结构化开发方法有一些明显的缺点产生大量的文档，这些文档大部分都是未读的文档必须保持最新划分为专家小组并需要遵循程序抑制沟通用户可以被排除在决策过程之外交付任何东西的时间很长，等等 The Manifesto for Agile Software Development敏捷软件开发的宣言“We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a planThat is, while there is value in the items on the right, we value the items on the left more.” by Kent Beck et al. 个体交互优于流程和工具 软件产品优于详尽的文档 客户合作优于合同谈判 对变更作出响应优于计划 Eight core DSDM principles八个核心共享原则 Focus on business need Deliver on time Collaborate Never compromise quality Develop iteratively Build incrementally from firm foundations Communication continuously Demonstrate control1。专注于业务需求2。按时交货3。合作4。永不妥协的质量5。迭代开发6。从坚实的基础逐步构建7。不断的沟通8。显示控制 time-box fixed deadline by which something has to be delivered时间盒固定期限，必须交付一些东西 Chapter5 Software effort estimationChapter5软件工作量评估 Given the project data below: What items are size drivers?哪些项目是规模驱动的 inputs, outputs, entity accesses (system users for certain aspects) What items are productivity drivers?哪些项目是生产率驱动的 Programming language What are the productivity rates for programming languages x, y and z?编程语言 x, y, z的生产率分别是多少 x: 10 FPs/day y: 7 FPs/day z: 12 FPs/day x:[(210x0.58+420x0.26+40x1.66)/30+(469x0.58+1406x0.26+125x1.66)/85]/2d)y:[(5130.58+12830.26+761.66)/108+(6600.58+23100.26+881.66)/161+(16000.58+32000.26+237*1.66)/308]/3=[7.012+7.015+6.991]=7.006 比例系数需要记住inputs, outputs, entity accesses：0.58,0.26,1.66 What would be the estimated effort for projects X and Y using a Mark II function point count?使用Mark II功能点计数的项目X和Y的估计工作量是多少？ FP of Project X is 261.8 and FP of Project Y is 704.66. Using the productivity rate for programming language y, the estimate for Project X would be 262/7 i.e. 37 days, and for Project Y 705/7 i.e. 101 days.项目X的FP为261.8，项目Y的FP为704.66。 使用编程语言y的生产率，项目X的估计值为262/7，即37天，而项目Y 705/7，即101天。 What would be the estimated effort for X and Y using an approximate analogy approach? 使用近似类比方法对X和Y的估计工作量是多少？ Project X seems closest to Project 5 which provides an estimate of 22 days, and Project Y seems to be closest to Project 3 which gives an estimate of 108 days.项目X似乎最接近项目5，其提供22天的估计，而项目Y似乎最接近项目3，其估计为108天。 知识点estimated effort = (system size) / productivity e.g.system size = lines of codeproductivity = lines of code per dayproductivity = (system size) / effortbased on past projects估计工作量=（系统大小）/生产率如。系统大小=代码行生产力=每天的代码行数生产力=（系统大小）/努力 Some models focus on task or system size e.g. Function Points一些模型关注任务或系统大小，例如功能点FPs originally used to estimate Lines of Code, rather than effortFPs最初用于估计代码行，而不是工作 Function points Mark II功能点马克二世 For each transaction, countdata items input (Ni)data items output (No)entity types accessed (Ne) Chapter6 Activity PlanningChapter6活动计划 Create a PERT activity network using above data. Calculate the earliest and latest start and end dates and the float associated with each activity. From this identify the critical path.使用上述数据创建PERT活动网络。 计算最早和最晚的开始和结束日期以及与每项活动相关的浮动。 由此确定关键路径。 Answer: 知识点 Activity ‘write report software’Earliest start (ES)Earliest finish (EF) = ES + durationLatest finish (LF) = latest task can be completed without affecting project endLatest start (LS) = LF - duration 活动区间（最左下角的值） = 持续时间 + 最右下角的值 Chapter7 Risk Management第7章风险管理 Using the activity times above: Calcaulate the expected duration and standard deviation for each activity Identify the critical path Draw up an activity diagram applying critical chain principles for this project:Local the places where buffers will need to be located.Assess the size of the buffersStart all activities as later as possible.使用上面的活动时间：1。对每个活动的预期持续时间和标准偏差进行计算2。确定关键路径3。为这个项目绘制一个应用关键链原理的活动图：本地的缓冲区需要被定位的地方。评估缓冲区的大小尽可能晚地开始所有活动。 Activity $t_e$ $s$ $b$ A 10 0.67 2 B 15 1.67 5 C 7 0.67 2 D 10 0.67 2 E 6 1.00 3 A: $t_e=\dfrac{(a+4m+b)}{6}=\dfrac{(8+4*10+12)}{6}=10$,$s=\dfrac{(b-a)}{6}=\dfrac{(12-8)}{6}=0.67$,$b=\dfrac{(b-a)}{2}=2.$ Project buffer = sum(b:where b is key point)/2 = (2+5+2+3)/2=6 Subsidiary chain’s feeding buffer=(b:not key point)/2 = 2/2=1 知识点Using PERT to evaluate the effects of uncertainty使用PERT图来评估不确定性的影响 Three estimates are produced for each activityMost likely time (m)Optimistic time (a)Pessimistic (b)‘expected time’ t_e=\dfrac{(a+4m+b)}{6}‘activity standard deviation’ s=\dfrac{(b-a)}{6} What would be the expected duration of the chain A + B + C? Answer: 12.66 + 10.33 + 25.66 = 48.65 What would be the standard deviation for A + B+ C? Answer: square root of (12 + 12 + 32)= 3.32 Say the target for completing A+B+C was 52 days (T)Calculate the z value thus z = (T – te)/s In this example z = (52-48.33)/3.32 = 1.01 Look up in table of z values – see next overhead Risk exposure (RE)= (potential damage) x (probability of occurrence) Risk reduction leverage =(REbefore- REafter)/ (cost of risk reduction) REbeforeis risk exposure before risk reduction e.g. 1% chance of a fire causing £200k damageREafter is risk exposure after risk reduction e.g. fire alarm costing £500 reduces probability of fire damage to 0.5%RRL = (1% of £200k-0.5% of £200k)/£500 = 2RRL &gt; 1.00 therefore worth doing在降低风险之前，rebefore是风险暴露，例如，1%的可能性发生火灾，造成200 k的损失REafter是在风险降低后的风险敞口，例如，火灾报警成本为500英镑，将火灾损失的可能性降低到0.5%。RRL = (1% of £200k-0.5% of £200k)/£500 = 2RRL &gt; 1.00 因此值得做 Chapter9 Monitoring &amp; ControlChapter9监测与控制 一个项目涉及到四个软件模块的设计和构建，分别称为A、B、C和D。每个模块的估计工作量为60小时，B为30小时，C为40小时，d为45。正在进行这项工作的组织假设是为了获得价值分析（EVA），设计占了30%的工作量，编码40%，测试30%。在这个EVA进行的当天，这个项目应该已经完成了。事实上，情况如下： 实际工作时间显示任务已经完成。1。计算进度和成本差异。2。计算成本性能和进度性能指标。3。从这些数据中可以得出一般的结论这个项目吗? 详细分析已知|model|Estimated effort|Estimated design|Design(actual hours)|Estimated code|Code (actual hours)|Estimated test|Test (actual hours)||-|-|-|-|-|-|-|-||||30%||40%||30%|||A|60||25||40||Not completed||B|30||15||15||15||C|40||15||Not completed||Not completed||D|45||10||Not completed||Not completed| 首先按题目给的比例 30% of the effort, coding 40% and testing 30%,把 Estimated design、Estimated code 和 Estimated test 填写了：|model|Estimated effort|Estimated design|Design(actual hours)|Estimated code|Code (actual hours)|Estimated test|Test (actual hours)||-|-|-|-|-|-|-|-||||30%||40%||30%|||A|60|18|25|24|40|18|Not completed||B|30|9|15|12|15|9|15||C|40|12|15|16|Not completed|12|Not completed||D|45|13.5|10|18|Not completed|13.5|Not completed| Planned value (PV) 计划价值 $PV = sum(\text{Estimated effort)} =60+30+40+45=175$ Earned value (EV) 预算成本（不包括没完成的项目） $EV=sum(\text{Estimated effort)}-\text{Not completed Estimated item}$eg. EV = 175 - (18+16+12+18+13.5)=175-77.5=97.5 或者 竖着加刚才计算出的那三列，同样不包括没完成的项目eg. 18+9+12+13.5=52.5 24+12=36 1552.5+36+15=97.5 Actual cost (AC) 实际成本 $AC=sum\text{(actual hours)}=25+15+15+10+40+15+15=135$ Schedule variance (SV) 进度偏差 $SV=EV-PV=97.5-175=-77.5$ Schedule performance indicator (SPI) 进度绩效指标 $SPI=EV/PV=97.5/175=0.56$ Cost variance (CV) 成本偏差 $CV=EV-AC=97.5-135=-37.5$ Cost performance indicator (CPI) 成本绩效指数 $CPI = EV/AC=97.5/135=72.2$ 注意：都是用预算成本 EV 做分子 综上可知 中英词汇对照表 英文词汇 中文解释 Net profit(NP) 净利润 Cost benefit analysis (CBA)| 成本效益分析Return on investment (ROI) |投资回报率(ROI)Net present value (NPV) |净现值(NPV)Internal rate of return (IRR) |内部收益率（IRR）product breakdown structure(PBS)|产品分解结构Product description (PD)|产品描述(PD)PFD(Product Flow Diagram)|PFD(产品流程图)Gantt charts|甘特图Function Points（FPs）|功能点Risk exposure (RE)|风险承担Risk Reduction Leverage(RRL)|减少风险杠杆(RRL)Earned Value Analysis (EVA)|挣值分析(EVA)Planned value (PV) |计划价值Earned value (EV) |预算成本Actual cost (AC) | 实际成本Schedule variance (SV)| 进度偏差Schedule performance indicator (SPI)| 进度绩效指标Cost variance (CV)| 成本偏差Cost performance indicator (CPI)| 成本绩效指数customized off-the-shelf (COTS) |定制的现成的Invitation to tender (ITT)|招标(ITT)Memoranda of agreement (MoA)|备忘录 补充知识名词解释 名词 解释 项目 项目是一系列具有特定目标,有明确开始和终止日期，资金有限，消耗资源的活动和任务。 检查点 指在规定的时间间隔内对项目进行检查，比较实际与计划之间的差异，并根据差异进行调整。可将检查点看作是一个 固定 “ 采样 ” 时点，而时间间隔根据项目周期长短不同而不同，频度过小会失去意义，频度过大会增加管理成本。常见的间隔是每周一次，项目经理需要召开例会并上交周报。 里程碑 重要的检查点是里程碑，重要的需要客户确认的里程碑，就是基线。在我们实际的项目中，周例会是检查点的表现形式，高层的阶段汇报会是基线的表现形式。 基线 基线是经过评审和批准的配置项的集合，其作用是明确划分项目各阶段，确定各阶段的结束点。在项目的开发过程中，最基本的基线有需求基线、概要设计基线、详细设计基线、代码基线、测试基线、交付基线 CPM|即关键路径法(Critical Path Method），又称关键线路法。|PERT|Program Evaluation Review Technique计划评审技术，是一种任务工期估算的图示法。| 简答题请简述项目管理的9个知识领域和5个过程组。Please briefly describe 9 knowledge areas and 5 process groups of project management. 项目管理的九大知识领域包括项目整体管理、项目范围管理、项目时间管理、项目费用管理、项目质量管理、项目人力资源管理、项目沟通管理、项目风险管理和项目采购管理。项目管理5个过程组包括启动过程组、规划过程组、执行过程组、监督和控制过程组、收尾过程组。 项目管理有哪些不同类型的组织形式？What are the different types of organizational forms of project management? 项目管理组织分为：职能型矩阵型（包括弱矩阵型、平衡矩阵型、强矩阵型、复合矩阵型）项目新 请简述项目经理应该具备的素质。Please give a brief account of the qualities that the project manager should have. 项目经理应具备的素质包括以下几点：a) 广博的知识：包括项目管理知识、IT行业知识、客户行业知识。b) 丰富的经历c) 良好的协调能力d) 良好的职业道德e) 良好的沟通和表达能力f) 良好的领导能力 需求开发过程包括哪些过程？What processes do the requirements development process include? 需求开发包括：需求获取，需求分析，需求规格说明和需求验证等几个过程。 项目经理在需求变更管理中的职责和目标是什么？What are the responsibilities and objectives of project managers in requirements change management? 项目经理在需求彼岸管理中的职责是：a) 负责协调变更的需求并对变更的需求有拒绝的权利b) 负责对变更的需求部分设计的修改c) 保证项目的开发与需求的一致性d) 确定开发进度是否需要进行变更e) 分配新需求给相关开发人员项目经理在需求变更管理的目标：1、相关的干系人必须清楚地了解发生的变更。2、变更处于有效的管理中。3、尽量降低变更带来的风险。 什么是项目时间管理？以及项目管理包括哪些主要过程。What is project time management? And what are the main processes that project management includes. 项目时间管理是：“按时、保质地完成项目”大概是每一位项目经理最希望做到的。但工期托延的情况却时常发生。因而合理地安排项目时间是项目管理中一项关键内容，它的目的是保证按时完成项目、合理分配资源、发挥最佳工作效率。它的主要工作包括定义项目活动、任务、活动排序、每项活动的合理工期估算、制定项目完整的进度计划、资源共享分配、监控项目进度等内容。 综合题假如您是一个项目经理，负责一个基于web的图书管理系统软件的开发，问：应该如何分解此项目所应该包括的工作？请按WBS为该项目制定一份工作的分解计划。If you are a project manager, you are responsible for the development of a web based library management system software, and ask: how should you break down the work that the project should include? Please work out a breakdown plan for the project according to WBS. 答：我作为一个项目经理我按如下方式进行项目分解工作，1、首先做好用户调研计划、调研方式，通过多次迭代方式进行调研，形成最终的用户需求说明书；通过UML工具按模块进行用例图，通过用例图来确定系统范围及边界。2、同项目组开发人员进行沟通，确定项目所拥有的人力资源，根据建设的内容确定人力资源同项目建设内容搭配，根据模块的特性配备不同技术能力、沟通能力的技术人员，实现技术人员使用最优化。3、根据项目里程碑时间，以终为始划分项目开发计划里程碑，在做项目开发计划的根据实际情况酌情调整部分时间。4、做好项目开发计划，公布给项目开发组成员，并且要求严格按照进行执行；在中间发现进度出现问题，及时进行调整。]]></content>
      <categories>
        <category>考试</category>
      </categories>
      <tags>
        <tag>软件项目管理</tag>
        <tag>考试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分类、多分类与多标签问题的区别——对应损失函数的选择]]></title>
    <url>%2F2018%2F07%2F01%2F%E4%BA%8C%E5%88%86%E7%B1%BB%E3%80%81%E5%A4%9A%E5%88%86%E7%B1%BB%E4%B8%8E%E5%A4%9A%E6%A0%87%E7%AD%BE%E9%97%AE%E9%A2%98%E7%9A%84%E5%8C%BA%E5%88%AB%E2%80%94%E2%80%94%E5%AF%B9%E5%BA%94%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[二分类、多分类与多标签分类问题使用不同的激活函数和损失函数，结论见文末总结。 二分类、多分类与多标签的基本概念二分类：表示分类任务中有两个类别，比如我们想识别一幅图片是不是猫。也就是说，训练一个分类器，输入一幅图片，用特征向量x表示，输出是不是猫，用y=0或1表示。二类分类是假设每个样本都被设置了一个且仅有一个标签 0 或者 1。 多类分类(Multiclass classification): 表示分类任务中有多个类别, 比如对一堆水果图片分类, 它们可能是橘子、苹果、梨等. 多类分类是假设每个样本都被设置了一个且仅有一个标签: 一个水果可以是苹果或者梨, 但是同时不可能是两者。 多标签分类(Multilabel classification): 给每个样本一系列的目标标签. 可以想象成一个数据点的各属性不是相互排斥的(一个水果既是苹果又是梨就是相互排斥的), 比如一个文档相关的话题. 一个文本可能被同时认为是宗教、政治、金融或者教育相关话题。 多分类问题与二分类问题关系 首先，两类问题是分类问题中最简单的一种。其次，很多多类问题可以被分解为多个两类问题进行求解（请看下文分解）。所以，历史上有很多算法都是针对两类问题提出的。下面我们来分析如何处理多分类问题： 直接分成多类比如使用 Softmax 回归。 一对一的策略给定数据集D这里有N个类别，这种情况下就是将这些类别两两配对，从而产生N(N−1)2个二分类任务，在测试的时候把样本交给这些分类器，然后进行投票。 一对其余策略将每一次的一个类作为正例，其余作为反例，总共训练N个分类器。测试的时候若仅有一个分类器预测为正的类别则对应的类别标记作为最终分类结果，若有多个分类器预测为正类，则选择置信度最大的类别作为最终分类结果。 多标签问题与二分类问题关系面临的问题：图片的标签数目不是固定的，有的有一个标签，有的有两个标签，但标签的种类总数是固定的，比如为5类。 解决该问题：采用了标签补齐的方法，即缺失的标签全部使用0标记，这意味着，不再使用one-hot编码。例如：标签为：-1,1,1,-1,1 ;-1表示该类标签没有，1表示该类标签存在，则这张图片的标签编码为： 0 0 0 0 00 1 0 0 00 0 1 0 00 0 0 0 00 0 0 0 1 2.如何衡量损失？ 计算出一张图片各个标签的损失，然后取平均值。 3.如何计算精度 计算出一张图片各个标签的精度，然后取平均值。 该处理方法的本质：把一个多标签问题，转化为了在每个标签上的二分类问题。 损失函数的选择问题基于逻辑回归的二分类问题对于logistic回归，有： h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}逻辑回归有以下优点： 它的输入范围是 $-\infty \to+\infty$，而之于刚好为（0，1），正好满足概率分布为（0，1）的要求。我们用概率去描述分类器，自然比单纯的某个阈值要方便很多； 它是一个单调上升的函数，具有良好的连续性，不存在不连续点。 对数损失函数（logarithmic loss function) 或对数似然损失函数(log-likehood loss function) 。 L(Y,P(Y|X)) = -logP(Y|X)逻辑回归中，采用的则是对数损失函数。根据上面的内容，我们可以得到逻辑回归的对数似然损失函数cost function： cost(h_{\theta}(x),y) = \begin{cases} -log(h_{\theta}(x)) & \text {if y=1} \\ -log(1-h_{\theta}(x)) & \text{if y=0} \end{cases}将以上两个表达式合并为一个，则单个样本的损失函数可以描述为： cost(h_{\theta}(x),y) = -y_ilog(h_{\theta}(x)) - (1-y_i)log(1-h_{\theta}(x))这就是逻辑回归最终的损失函数表达式。 基于 Softmax 的多分类问题softmax层中的softmax 函数是logistic函数在多分类问题上的推广，它将一个N维的实数向量压缩成一个满足特定条件的N维实数向。压缩后的向量满足两个条件： 向量中的每个元素的大小都在[0,1] 向量所有元素的和为 1 因此，softmax适用于多分类问题中对每一个类别的概率判断，softmax的函数公式如下： a^L_j = \frac {e^{z^L_j}}{\sum_k e^{z^L_k}}基于 Softmax 的多分类问题采用的是 log似然代价函数（log-likelihood cost function）来解决。 单个样本的 log似然代价函数的公式为： C = - \sum_i y_i log a_i其中， $y_i$ 表示标签向量的第 $i$ 个分量。因为往往只有一个分量为 1 其余的分量都为 0，所以可以去掉损失函数中的求和符号，化简为， C \equiv -\ln a_j其中， $a_j$ 是向量 $y$ 中取值为 1 对应的第 $j$ 个分量的值。 交叉熵损失函数与 log 似然代价函数关系 本质一样有的文献中也称 log 似然代价函数为交叉熵损失函数，这两个都是交叉熵损失函数，但是看起来长的却有天壤之别。为什么同是交叉熵损失函数，长的却不一样呢？ cost(h_{\theta}(x),y) = -y_ilog(h_{\theta}(x)) - (1-y_i)log(1-h_{\theta}(x))C = - \sum_i y_i log a_i因为这两个交叉熵损失函数对应不同的最后一层的输出。第一个对应的最后一层是 sigmoid，用于二分类问题，第二个对应的最后一层是 softmax，用于多分类问题。但是它们的本质是一样的，请看下面的分析。 首先来看信息论中交叉熵的定义： -\int p(x)\text{log}g(x)dx交叉熵是用来描述两个分布的距离的，神经网络训练的目的就是使 g(x) 逼近 p(x)。 sigmoid + 对数损失函数先看看 sigmoid 作为神经网络最后一层的情况。sigmoid 作为最后一层输出的话，那就不能吧最后一层的输出看作成一个分布了，因为加起来不为 1。现在应该将最后一层的每个神经元看作一个分布，对应的 target 属于二项分布(target的值代表是这个类的概率)，那么第 i 个神经元交叉熵为 y_i\text{log}(h_{\theta})+(1-y_i)\text{log}(1-h_{\theta})其实这个式子可以用求和符号改写， C = - \sum_i log a_i其中， cost(h_{\theta}(x),y) = \begin{cases} -log(a_i) & a_i=h_{\theta} & \text {if } y_i=1 \\ -log(a_i) & a_i=1-h_{\theta} & \text{if } y_i=0 \end{cases}Softmax + 交叉熵现在来看 softmax 作为神经网络最后一层的情况。g(x)是什么呢？就是最后一层的输出 y 。p(x)是什么呢？就是我们的one-hot标签。我们带入交叉熵的定义中算一下，就会得到： C = - \sum_i y_i log a_i交叉熵损失函数与 log 似然损失函数的总结注意到不管是交叉熵损失函数与 log 似然损失函数，交叉熵损失函数用于二分类问题， log 似然损失函数用于多分类，但是对于某一个样本只属于一个类别，只有一个标签。如果用 one-hot 编码样本的标签那么，对于标签向量只有一个分量的值为 1 其余的值都为 0。 所以不管是交叉熵损失函数与 log 似然损失函数，都可以化简为， C \equiv -\ln a_j其中， $a_j$ 是向量 $y$ 中取值为 1 对应的第 $j$ 个分量的值。这两个长的不一样的损失函数实际上是对应的不同的输出层。本质上是一样的。 我的建议是，采用 Kears 中的命名方法，对于二分类的交叉熵损失函数称之为 “二分类交叉熵损失函数（binary_crossentropy）” ，对于多分类的交叉熵损失函数称之为 “多类别交叉熵损失函数（categorical_crossentropy）”。 在 Kears 中也有提示（注意: 当使用categorical_crossentropy损失时，你的目标值应该是分类格式 (即，如果你有10个类，每个样本的目标值应该是一个10维的向量，这个向量除了表示类别的那个索引为1，其他均为0)。 为了将 整数目标值 转换为 分类目标值，你可以使用Keras实用函数to_categorical：） 一种更直接的证明方法，数学公式， 内容来源于 Hands-on Machine Learning with Scikit-Learn and TensorFlow 141 页。 多标签分类 + 二分类交叉熵损失函数多标签问题与二分类问题关系在上文已经讨论过了，方法是计算一个样本各个标签的损失（输出层采用sigmoid函数），然后取平均值。把一个多标签问题，转化为了在每个标签上的二分类问题。 总结 分类问题名称 输出层使用激活函数 对应的损失函数 二分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy） 多分类 Softmax函数 多类别交叉熵损失函数（categorical_crossentropy） 多标签分类 sigmoid函数 二分类交叉熵损失函数（binary_crossentropy） 参考文献[1] François Chollet‏. Keras Document[DB/OL]. https://keras.io/, 2018-07-01. [2] 目力过人. 多标签分类（multilabel classification ）[DB/OL]. https://blog.csdn.net/bemachine/article/details/10471383, 2018-07-01. [3] Inside_Zhang.【联系】二项分布的对数似然函数与交叉熵（cross entropy）损失函数[DB/OL]. https://blog.csdn.net/lanchunhui/article/details/75433608, 2018-07-01. [4] ke1th. 两种交叉熵损失函数的异同[DB/OL]. https://blog.csdn.net/u012436149/article/details/69660214, 2018-07-01. [5] bitcarmanlee. logistic回归详解一：为什么要使用logistic函数[DB/OL]. https://blog.csdn.net/bitcarmanlee/article/details/51154481, 2018-07-01. [6] Aurélien Géron. Hands-On Machine Learning with Scikit-Learn and TensorFlow[M]. America: O’Reilly Media, 2017-03-10, 140-141.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络可以计算任何函数的可视化证明]]></title>
    <url>%2F2018%2F06%2F29%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 4 A visual proof that neural nets can compute any function 神经网络的一个最显著的事实就是它可以计算任何的函数。不管这个函数是什么样的，总会确保有一个神经网络能够对任何可能的输入 $x$，其值 $f(x)$ （或者某个足够准确的近似）是网络的输出。 表明神经网络拥有一种普遍性。不论我们想要计算什么样的函数，我们都确信存在一个神经网络可以计算它。 而且，这个普遍性定理甚至在我们限制了神经网络只在输入层和输出层之间存在一个中间层的情况下成立。所以即使是很简单的网络架构都极其强大。普遍性定理在使用神经网络的人群中是众所周知的。但是它为何正确却不被广泛地理解。现有的大多数的解释都具有很强的技术性。 如果你是数学家，这个证明应该不大难理解，但对于大多数人还是很困难的。这不得不算是一种遗憾，因为这个普遍性背后的原理其实是简单而美妙的。在这一章，我给出这个普遍性定理的简单且大部分为可视化的解释。我们会一步步深入背后的思想。你会理解为何神经网络可以计算任何的函数。你会理解到这个结论的一些局限。并且你还会理解这些结论如何和深度神经网络关联的。 神经网络拥有强大的算法来学习函数。学习算法和普遍性的结合是一种有趣的混合。直到现在，本书一直是着重谈学习算法。到了本章，我们来看看普遍性，看看它究竟意味着什么。 两个预先声明在解释为何普遍性定理成立前，我想要提下关于非正式的表述“神经网络可以计算任何函数”的两个预先声明。 第一点，这句话不是说一个网络可以被用来准确地}计算任何函数。而是说，我们可以获得尽可能好的一个近似。通过增加隐藏元的数量，我们可以提升近似的精度。为了让这个表述更加准确，假设我们给定一个需要按照目标精度 $\epsilon &gt; 0$ 的函数 $f(x)$。通过使用足够多的隐藏神经元使得神经网络的输出 $g(x)$ 对所有的 $x$，满足 $|g(x) - f(x)| &lt; \epsilon$ 从而实现近似计算。换言之，近似对每个可能的输入都是限制在目标准确度范围内的。 第二点，就是可以按照上面的方式近似的函数类其实是连续}函数。如果函数不是连续的，也就是会有突然、极陡的跳跃，那么一般来说无法使用一个神经网络进行近似。这并不意外，因为神经网络计算的就是输入的连续函数。然而，即使那些我们真的想要计算的函数是不连续的，一般来说连续的近似其实也足够的好了。如果这样的话，我们就可以用神经网络来近似了。实践中，这通常不是一个严重的限制。 总结一下，更加准确的关于普遍性定理的表述是包含一个隐藏层的神经网络可以被用来按照任意给定的精度来近似任何连续函数。 本章可视化证明的说明 我们会使用了两个隐藏层的网络来证明这个结果的弱化版本。然后我将简要介绍如何通过一些微调把这个解释适应于只使用一个隐藏层的网络的证明。 我们使用 S 型神经元作为神经网络的激活函数，在后面我们可以推广到其它激活函数。 证明的理论准备 S 型神经元作为神经网络的激活函数非常普遍，但是为了方便我们的证明，我们对 S 型神经元做一点点处理，把它变成阶跃函数（感知器）。实际上处理阶跃函数比一般的 S 型函数更加容易。原因是在输出层我们把所有隐藏神经元的贡献值加在一起。分析一串阶跃函数的和是容易的，相反，思考把一串 S 形状的曲线加起来是什么则要更困难些。所以假设我们的隐藏神经元输出阶跃函数会使事情更容易。更具体些，我们把权重 $w$ 固定在一个大的值，然后通过修改偏置设置阶跃函数的位置。当然，把输出作为一个阶跃函数处理只是一个近似，但是它是一个非常好的近似，现在我们把它看作是精确的。稍后我会再讨论偏离这种近似的影响。关于 S 型神经元和感知器的关系分析见 使用神经网络识别手写数字——感知器。 S 型函数变阶跃函数过程 S 型神经元的代数形式是， \sigma(z) \equiv \frac{1}{1+e^{-z}}其中， $z = w \cdot x+b$ S 型神经元的函数图形是， $x$ 取何值时阶跃会发生呢？换种方式，阶跃的位置如何取决于权重和偏置？ 因为 $z = w \cdot x+b$ 是一个直线方程， $w$ 是直线的斜率， $b$ 是直线的截距，直线与 $x$ 轴的交点是 $(-b/w, 0)$，令 $s=-b/w$。 假设 $z \equiv w \cdot x + b$ 是一个很大的正数。那么 $e^{-z} \approx 0$ 而 $\sigma(z) \approx 1$。即，当 $z = w \cdot x+b$ 很大并且为正， S 型神经元的输出近似为 $1$，正好和感知器一样。相反地，假设 $z = w \cdot x+b$ 是一个很大的负数。那么$e^{-z} \rightarrow \infty$，$\sigma(z) \approx 0$。所以当 $z = w \cdot x +b$ 是一个很大的负数。 根据上面两点可以知道，当 $|w|$ 的值很大时，$x$ 就能直接决定 S 型神经元的函数结果为零还是为一，根据第一点还可以知道 $s=-b/w$ 就是阶跃的分界点。当 $x&gt;s$ 时 S 型函数取 1，当 $x&lt;s$时 S 型函数取 0。注意，$s=-b/w$ 时我们将在后文修补阶跃函数中讨论。 S 型神经元的函数变成阶跃函数图形是： 下面的证明中，我们总是让 $|w|$ 的值很大，也就是说我们将一直使用变成阶跃函数的 S 型神经元。并且用 $s$ 表示阶跃函数阶跃的位置。这将用仅仅一个参数 $s$ 来极大简化我们描述隐藏神经元的方式，这就是阶跃位置，$s =-b/w$。 一个输入和一个输出的普遍性目前为止我们专注于仅仅从顶部隐藏神经元输出。让我们看看整个网络的行为。尤其，我们假设隐藏神经元在计算以阶跃点 $s_1$ （顶部神经元）和 $s_2$ （底部神经元）参数化的节约函数。它们各自有输出权重 $w_1$ 和 $w_2$。是这样的网络： 右边的绘图是隐藏层的加权输出 $w_1 a_1 + w_2 a_2$。这里 $a_1$ 和 $a_2$ 各自是顶部和底部神经元的输出。这些输出由 $a$ 表示，是因为它们通常被称为神经元的激活值。注意，注意整个网络的输出是 $\sigma(w_1 a_1+w_2 a_2 + b)$，其中 $b$ 是隐藏层的偏置。很明显，这不同于隐藏层加权后的输出，也就是我们这里的绘图。我们现在打算专注于隐藏层的加权输出，不一会就会考虑如何把它关联到整个网络的输出。 思考增加和减少每一个输出权重。注意，这如何调整从各自的隐藏神经元的贡献值。当一个权重是 0 时会发生什么？ 最后，试着设置 $w_1$ 为 $0.8$，$w_2$ 为 $-0.8$。你得到一个“凸起”的函数，它从点 $s_1$ 开始，到点 $s_2$ 结束，高为 $0.8$。例如，加权后的输出可能看起来像这样： 当然，我们可以调整为任意的凸起高度。让我们用一个参数，$h$，来表示高度。为了减少混乱我也会移除“$s_1 = \ldots$”和“$w_1 = \ldots$”的标记。 试着将 $h$ 值改大或改小，看看凸起的高度如何改变。试着把高度值改为负数，观察发生了什么。并且试着改变阶跃点来看看如何改变凸起的形状。我们可以用凸起制作的技巧来得到两个凸起，通过把两对隐藏神经元一起填充进同一个网络： 这里我抑制了权重，只是简单地在每对隐藏神经元上写了 $h$ 的值。试着增加和减少两个 $h$ 值，观察它如何改变图形。通过修改节点来移动凸起。更普遍地，我们可以利用这个思想来取得我们想要的任何高度的峰值。 其实到这里我们可以说已经证明了神经网络在一个输入和一个输出上的普遍性，因为从微积分的观点来看，只需要增加“凸起”的个数，众多“凸起”合在一起的图形就可更加接近需要近似的函数图形。无限多个“凸起”就能无限逼近于目标函数。一个示意的例子如下， 让我快速总结一下那是如何工作的。 第一层的权重都有一些大的，恒定的值，比如：$w = 1000$。 隐藏神经元上的偏置只是 $b = -w s$。例如，对于第二个隐藏神经元 $s = 0.2$ 变成了 $b = -1000 \times 0.2 = -200$。 最后一层的权重由 $h$ 值决定。例如，我们上面已经选择的第一个 $h$，$h = -0.5$，意味着顶部两个隐藏神经元的相应的输出权重是 $-0.5$ 和 $0.5$。如此等等，确定整个层的输出权重。 最后，输出神经元的偏置为 $0$。 这是所有要做的事情：现在我们有了一个可以很好计算我们原始目标函数的神经网络的完整的描述。而且我们理解如何通过提高隐层神经元的数目来提高近似的质量。 在本质上，我们使用我们的单层神经网络来建立一个函数的查找表。我们将能够建立这个思想，以提供普遍性的一般性证明。 多个输入变量的普遍性让我们把结果扩展到有很多个输入变量的情况下。这听上去挺复杂，但是所有我们需要的概念都可以在两个输入的情况下被理解。所以让我们处理两个输入的情况。我们从考虑当一个神经元有两个输入会发生什么开始： 这里，我们有输入 $x$ 和 $y$，分别对应于权重 $w_1$ 和 $w_2$，以及一个神经元上的偏置 $b$。让我们把权重 $w_2$ 设置为 $0$，然后反复琢磨第一个权重 $w_1$ 和偏置 $b$，看看他们如何影响神经元的输出： 正如我们前面讨论的那样，随着输入权重变大，输出接近一个阶跃函数。不同的是，现在的阶跃函数是在三个维度。也如以前一样，我们可以通过改变偏置的位置来移动阶跃点的位置。阶跃点的实际位置是 $s_x \equiv -b / w_1$。 我们可以用我们刚刚构造的阶跃函数来计算一个三维的凹凸函数。为此，我们使用两个神经元，每个计算一个$x$ 方向的阶跃函数。然后我们用相应的权重 $h$ 和 $-h$ 将这两个阶跃函数混合，这里 $h$ 是凸起的期望高度。所有这些在下面图示中说明： 试着改变高度 $h$ 的值。观察它如何和网络中的权重关联。并看看它如何改变右边凹凸函数的高度。 我们已经解决了如何制造一个 $x$ 方向的凹凸函数。当然，我们可以很容易地制造一个$y$ 方向的凹凸函数，通过使用 $y$ 方向的两个阶跃函数。回想一下，我们通过使 $y$输入的权重变大，$x$ 输入的权重为 $0$ 来这样做。这是结果： 这看上去和前面的网络一模一样！唯一的明显改变的是在我们的隐藏神经元上现在标记有一个小的 $y$。那提醒我们它们在产生 $y$ 方向的阶跃函数，不是 $x$ 方向的，并且 $y$上输入的权重变得非常大，$x$ 上的输入为 $0$，而不是相反。正如前面，我决定不去明确显示它，以避免图形杂乱。 让我们考虑当我们叠加两个凹凸函数时会发生什么，一个沿 $x$ 方向，另一个沿 $y$ 方向，两者都有高度 $h$： 为了简化图形，我丢掉了权重为 $0$ 的连接。现在，我在隐藏神经元上留下了 $x$ 和 $y$的标记，来提醒你凹凸函数在哪个方向上被计算。后面我们甚至为丢掉这些标记，因为它们已经由输入变量说明了。试着改变参数 $h$。正如你能看到，这引起输出权重的变化，以及 $x$ 和 $y$ 上凹凸函数的高度。 我们构建的有点像是一个塔型函数，如果我们能构建这样的塔型函数，那么我们能使用它们来近似任意的函数。 如果我们选择适当的阈值，比如，$3h/2$，这是高原的高度和中央塔的高度中间的值 ——我们可以把高原下降到零，并且依旧矗立着塔。 你能明白怎么做吗？试着用下面的网络做实验来解决。请注意， 我们现在正在绘制整个网络的输出，而不是只从隐藏层的加权输出。这意味着我们增加了一个偏置项到隐藏层的加权输出，并应用 S 型函数。 你能找到 $h$ 和 $b$ 的值，能产生一个塔型吗？这有点难，所以如果你想了一会儿还是困住，这是有两个提示：（1）为了让输出神经元显示正确的行为，我们需要输入的权重（所有 $h$ 或 $-h$）变得很大；（2）$b$ 的值决定了阈值的大小。 这是它看起来的样子，我们使用 $h = 10$： 甚至对于这个相对适中的 $h$ 值，我们得到了一个相当好的塔型函数。当然，我们可以通过更进一步增加 $h$ 并保持偏置$b = -3h/2$ 来使它如我们所希望的那样。 让我们尝试将两个这样的网络组合在一起，来计算两个不同的塔型函数。为了使这两个子网络更清楚，我把它们放在如下所示的分开的方形区域：每个方块计算一个塔型函数，使用上面描述的技术。图上显示了第二个隐藏层的加权输出，即，它是一个加权组合的塔型函数。 尤其你能看到通过修改最终层的权重能改变输出塔型的高度。同样的想法可以用在计算我们想要的任意多的塔型。我们也可以让它们变得任意细，任意高。通过使第二个隐藏层的加权输出为 $\sigma^{-1} \circ f$ 的近似，我们可以确保网络的输出可以是任意期望函数 $f$ 的近似。 让我们试试三个变量 $x_1, x_2, x_3$。下面的网络可以用来计算一个四维的塔型函数： 这里，$x_1, x_2, x_3$ 表示网络的输入。$s_1, t_1$ 等等是神经元的阶跃点~——~即，第一层中所有的权重是很大的，而偏置被设置为给出阶跃点 $s_1, t_1, s_2, \ldots$。第二层中的权重交替设置为 $+h, -h$，其中 $h$ 是一个非常大的数。输出偏置为 $-5h/2$。 这个网络计算这样一个函数，当三个条件满足时：$x_1$ 在 $s_1$ 和 $t_1$ 之间；$x_2$在$s_2$ 和 $t_2$ 之间；$x_3$ 在 $s_3$ 和 $t_3$ 之间，输出为 $1$。其它情况网络输出为 $0$。即，这个塔型在输入空间的一个小的区域输出为 $1$，其它情况输出 $0$。 通过组合许多个这样的网络我们能得到任意多的塔型，如此可近似一个任意的三元函数。对于 $m$ 维可用完全相同的思想。唯一需要改变的是将输出偏置设为 $(-m+1/2)h$，为了得到正确的夹在中间的行为来弄平高原。 好了，所以现在我们知道如何用神经网络来近似一个多元的实值函数。对于 $f(x_1,\ldots, x_m) \in R^n$ 的向量函数怎么样？当然，这样一个函数可以被视为 $n$ 个单独的实值函数： $f^1(x_1, \ldots, x_m)$， $f^2(x_1, \ldots, x_m)$ 等等。所以我们创建一个网络来近似 $f^1$，另一个来近似 $f^2$，如此等等。然后简单地把这些网络都组合起来。 所以这也很容易应付。 思考 我们已经看到如何使用具有两个隐藏层的网络来近似一个任意函数。你能否找到一个证明，证明只有一个隐藏层是可行的？作为一个提示，试着在只有两个输入变量的情况下工作，并证明：（a）可以得到一个不仅仅在 $x$ 和 $y$ 方向，而是在一个任意方向上的阶跃函数；（b）可以通过累加许多的源自（a）的结构，近似出一个塔型的函数，其形状是圆的，而不是方的；（c）使用这些圆形塔，可以近似一个任意函数。对于（c）可以使用本章稍后的一些思想。 S 型神经元的延伸我们已经证明了由 S 型神经元构成的网络可以计算任何函数。回想下在一个 S 型神经元中，输入$x_1, x_2, \ldots$ 导致输出 $\sigma(\sum_j w_j x_j + b)$，这里 $w_j$ 是权重，$b$ 是偏置，而 $\sigma$ 是 S 型函数： 如果我们考虑一个不同类型的神经元，它使用其它激活函数，比如如下的 $s(z)$，会怎样？ 更确切地说，我们假定如果神经元有输入 $x_1, x_2, \ldots$，权重 $w_1, w_2, \ldots$和偏置 $b$，那么输出为 $s(\sum_j w_j x_j + b)$。我们可以使用这个激活函数来得到一个阶跃函数，正如用 S 型函数做过的一样。 正如使用 S 型函数的时候，这导致激活函数收缩，并最终变成一个阶跃函数的很好的近似。试着改变偏置，然后你能看到我们可以设置我们想要的阶跃位置。所以我们能使用所有和前面相同的技巧来计算任何期望的函数。 $s(z)$ 需要什么样的性质来满足这样的结果呢？我们确实需要假定 $s(z)$ 在 $z\rightarrow -\infty$ 和 $z \rightarrow \infty$ 时是定义明确的。这两个界限是在我们的阶跃函数上取的两个值。我们也需要假定这两个界限彼此不同。如果它们不是这样，就没有阶跃，只是一个简单的平坦图形！但是如果激活函数 $s(z)$ 满足这些性质，基于这样一个激活函数的神经元可普遍用于计算。 问题 在本书前面我们遇到过其它类型的称为修正线性单元的神经元。解释为什么这样的神经元不满足刚刚给出的普遍性的条件。找到一个普遍性的证明，证明修正线性单元可普遍用于计算。 假设我们考虑线性神经元，即具有激活函数 $s(z) = z$ 的神经元。解释为什么线性神经元不满足刚刚给出的普遍性的条件。证明这样的神经元不能用于通用计算。 修补阶跃函数目前为止，我们假定神经元可以准确生成阶跃函数。这是一个非常好的近似，但也仅仅是近似。实际上，会有一个很窄的故障窗口，如下图说明，在这里函数会表现得和阶跃函数非常不同。 在这些故障窗口中我给出的普遍性的解释会失败。 现在，它不是一个很严重的故障。通过使得输入到神经元的权重为一个足够大的值，我们能把这些故障窗口变得任意小。当然，我们可以把故障窗口窄过我在上面显示的~——~窄得我们的眼睛都看不到。所以也许我们可以不用过于担心这个问题。 尽管如此，有一些方法解决问题是很好的。 实际上，这个问题很容易解决。让我们看看只有一个输入和一个输出的神经网络如何修补其计算函数。同样的想法也可以解决有更多输入和输出的问题。 特别地，假设我们想要我们的网络计算函数 $f$。和以前一样，我们试着设计我们的网络，使得隐藏神经元的加权输出是 $\sigma^{-1} \circ f(x)$： 如果我们要使用前面描述的技术做到这一点，我们会使用隐藏神经元产生一系列的凹凸函数： 再说一下，我夸大了图上的故障窗口大小，好让它们更容易看到。很明显如果我们把所有这些凹凸函数加起来，我们最终会得到一个合理的 $\sigma^{-1} \circ f(x)$ 的近似，除了那些故障窗口。 假设我们使用一系列隐藏神经元来计算我们最初的目标函数的一半，即 $\sigma^{-1} \circ f(x) / 2$，而不是使用刚刚描述的近似。当然，这看上去就像上一个图像的缩小的版本： 并且假设我们使用另一系列隐藏神经元来计算一个 $\sigma^{-1} \circ f(x) / 2$ 的近似，但是用将凹凸图形偏移一半宽度： 现在我们有两个不同的 $\sigma^{-1} \circ f(x) / 2$ 的近似。如果我们把这两个近似图形加起来，我们会得到一个 $\sigma^{-1} \circ f(x)$ 的整体近似。这个整体的近似仍然在一些小窗口的地方有故障。但是问题比以前要小很多。原因是在一个近似中的故障窗口的点，不会在另一个的故障窗口中。所以在这些窗口中，近似会有 $2$ 倍的因素更好。 我们甚至能通过加入大量的，用 $M$ 表示，重叠的近似 $\sigma^{-1} \circ f(x) / M$来做得更好。假设故障窗口已经足够窄了，其中的点只会在一个故障窗口中。并且假设我们使用一个 $M$ 足够大的重叠近似，结果会是一个非常好的整体近似。 结论我们已经讨论的对于普遍性的解释当然不是如何使用神经网络计算的切实可行的用法！其更像是 NAND 门或者其它类似的普遍性证明。因为这个原因，我主要专注于让解释更清晰和易于理解，而不是过于挖掘细节。然而，你可以发现如果你能改进这个解释是个很有趣和有教益的练习。 尽管这个结果并不能直接用于解释网络，它还是是很重要的，因为它解开了是否使用一个神经网络可以计算任意特定函数的问题。对这个问题的答案总是“是”。所以需要问的正确问题，并不是是否任意函数可计算，而是计算函数的好的方法是什么。 我们建立的对于普遍性的解释只使用了两个隐藏层来计算一个任意的函数。而且，正如我们已经讨论过的，只使用单个的隐藏层来取得相同的结果是可能的。鉴于此，你可能想知道为什么我们会对深度网络感兴趣，即具有很多隐藏层的网络。我们不能简单地把这些网络用浅层的、单个隐藏层的网络替换吗？ 尽管在原理上这是可能的，使用深度网络仍然有实际的原因。正如在第一章中表明过，深度网络有一个分级结构，使其尤其适用于学习分级的知识，这看上去可用于解决现实世界的问题。但是更具体地，当攻克诸如图像识别的问题，使用一个不仅能理解单独的像素，还能理解越来越复杂的概念的系统是有帮助的，这里说的复杂的概念，可以从图像的边缘信息到简单的几何形状，以及所有复杂的、多物体场景的方式。在后面的章节中，我们将看到在学习这样的分级知识时，深度网络要比浅层网络做得更好。总结一下： 普遍性告诉我们神经网络能计算任何函数；而实际经验依据提示深度网络最能适用于学习能够解决许多现实世界问题的函数。 参考文献[1] Michael Nielsen. CHAPTER 4 A visual proof that neural nets can compute any function[DB/OL]. http://neuralnetworksanddeeplearning.com/chap4.html, 2018-06-29. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap4.tex, 2018-06-29.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 4 A visual proof that neural nets can compute any function</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——如何选择神经网络的超参数]]></title>
    <url>%2F2018%2F06%2F28%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%B6%85%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 为什么选择神经网络的超参数是个难题直到现在，我们还没有解释对诸如学习率 $\eta$，正则化参数 $\lambda$ 等等超参数选择的方法。我只是给出那些效果很好的值而已。实践中，当你使用神经网络解决问题时，寻找好的超参数其实是很困难的一件事。例如，我们要解决 MNIST 问题，开始时对于选择什么样的超参数一无所知。假设，刚开始的实验中选择前面章节的参数都是运气较好。但在使用学习率 $\eta=10.0$ 而\正则化参数 $\lambda=1000.0$。下面是我们的一个尝试： 1234567891011121314151617181920212223242526&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10])&gt;&gt;&gt; net.SGD(training_data, 30, 10, 10.0, lmbda = 1000.0,... evaluation_data=validation_data, monitor_evaluation_accuracy=True)Epoch 0 training completeAccuracy on evaluation data: 1030 / 10000Epoch 1 training completeAccuracy on evaluation data: 990 / 10000Epoch 2 training completeAccuracy on evaluation data: 1009 / 10000...Epoch 27 training completeAccuracy on evaluation data: 1009 / 10000Epoch 28 training completeAccuracy on evaluation data: 983 / 10000Epoch 29 training completeAccuracy on evaluation data: 967 / 10000 我们分类准确率并不比随机选择更好。网络就像随机噪声产生器一样。 你可能会说，“这好办，降低学习率和正则化参数就好了。”不幸的是，你并不先验地知道这些就是需要调整的超参数。可能真正的问题出在 $30$ 个隐藏元中，本身就不能很有效，不管我们如何调整其他的超参数都没有作用的？可能我们真的需要至少 $100$ 个隐藏神经元？或者是 $300$ 个隐藏神经元？或者更多层的网络？或者不同输出编码方式？可能我们的网络一直在学习，只是学习的回合还不够？可能 minibatch 设的太小了？可能我们需要切换成二次代价函数？可能我们需要尝试不同的权重初始化方法？等等。很容易就在超参数的选择中迷失了方向。如果你的网络规模很大，或者使用了很多的训练数据，这种情况就很令人失望了，因为一次训练可能就要几个小时甚至几天乃至几周，最终什么都没有获得。如果这种情况一直发生，就会打击你的自信心。可能你会怀疑神经网络是不是适合你所遇到的问题？可能就应该放弃这种尝试了？ 本节，我会给出一些用于设定超参数的启发式想法。目的是帮你发展出一套工作流来确保很好地设置超参数。当然，我不会覆盖超参数优化的每个方法。那是太繁重的问题，而且也不会是一个能够完全解决的问题，也不存在一种通用的关于正确策略的共同认知。总是会有一些新的技巧可以帮助你提高一点性能。但是本节的启发式想法能帮你开个好头。 宽泛策略（Broad strategy）在使用神经网络来解决新的问题时，一个挑战就是获得任何一种非寻常的学习，也就是说，达到比随机的情况更好的结果。这个实际上会很困难，尤其是遇到一种新类型的问题时。让我们看看有哪些策略可以在面临这类困难时候尝试。 假设，我们第一次遇到 MNIST 分类问题。刚开始，你很有激情，但是当第一个神经网络完全失效时，你会觉得有些沮丧。此时就可以将问题简化。丢开训练和验证集合中的那些除了$0$ 和 $1$ 的那些图像。然后试着训练一个网络来区分 $0$ 和 $1$。不仅仅问题比 $10$个分类的情况简化了，同样也会减少 80\% 的训练数据，这样就给出了 $5$ 倍的加速。这样可以保证更快的实验，也能给予你关于如何构建好的网络更快的洞察。 你通过简化网络来加速实验进行更有意义的学习。如果你相信 $[784, 10]$ 的网络更可能比随机更加好的分类效果，那么就从这个网络开始实验。这会比训练一个 $[784, 30 ,10]$的网络更快，你可以进一步尝试后一个。 你可以通过提高监控的频率来在试验中获得另一个加速了。 我们可以继续，逐个调整每个超参数，慢慢提升性能。一旦我们找到一种提升性能的 $\eta$ 值，我们就可以尝试寻找好的值。然后按照一个更加复杂的网络架构进行实验，假设是一个有 $10$ 个隐藏元的网络。然后继续调整 $\eta$ 和 $\lambda$。接着调整成 $20$ 个隐藏元。然后将其他的超参数调整再调整。如此进行，在每一步使用我们 hold out 验证数据集来评价性能，使用这些度量来找到越来越好的超参数。当我们这么做的时候，一般都需要花费更多时间来发现由于超参数改变带来的影响，这样就可以一步步减少监控的频率。 所有这些作为一种宽泛的策略看起来很有前途。然而，我想要回到寻找超参数的原点。实际上，即使是上面的讨论也传达出过于乐观的观点。实际上，很容易会遇到神经网络学习不到任何知识的情况。你可能要花费若干天在调整参数上，仍然没有进展。 所以我想要再重申一下在前期你应该从实验中尽可能早的获得快速反馈。 直觉上看，这看起来简化问题和架构仅仅会降低你的效率。实际上，这样能够将进度加快，因为你能够更快地找到传达出有意义的信号的网络。一旦你获得这些信号，你可以尝尝通过微调超参数获得快速的性能提升。这和人生中很多情况一样，万事开头难。 好了，上面就是宽泛的策略。现在我们看看一些具体的设置超参数的推荐。我会聚焦在学习率 $\eta$，L2 regularization参数 $\lambda$，和小批量数据大小。然而，很多的观点同样可以应用在其他的超参数的选择上，包括一些关于网络架构的、其他类型的regularization和一些本书后面遇到的如 momentum co-efficient 这样的超参数。 学习速率（Learning rate）假设我们运行了三个不同学习速率（$\eta=0.025$、$\eta=0.25$、$\eta=2.5$）的 MNIST 网络。我们会像前面介绍的实验那样设置其他的超参数，进行 $30$ 回合，minibatch 大小为 $10$，然后 $\lambda = 5.0$。我们同样会使用整个 $50,000$ 幅训练图像。下面是一副展示了训练代价的变化情况的图： 使用 $\eta=0.025$，代价函数平滑下降到最后的回合。使用 $\eta=0.25$，代价刚开始下降，在大约 $20$ 回合后接近饱和状态，后面就是微小的震荡和随机抖动。最终使用 $\eta=2.5$ 代价从始至终都震荡得非常明显。为了理解震荡的原因，回想一下随机梯度下降其实是期望我们能够逐渐地抵达代价函数的谷底的， 然而，如果 $\eta$ 太大的话，步长也会变大可能会使得算法在接近最小值时候又越过了谷底。这在 $\eta=2.5$ 时非常可能发生。当我们选择 $\eta=0.25$ 时，初始几步将我们带到了谷底附近，但一旦到达了谷底，又很容易跨越过去。而在我们选择 $\eta=0.025$ 时，在前 $30$ 回合的训练中不再受到这个情况的影响。当然，选择太小的%学习速率，也会带来另一个题,\gls*{sgd}算法变慢了。一种更加好的策略其实是，在开始时使用 $\eta=0.25$，随着越来越接近谷底，就换成$\eta=0.025$。这种可变学习速率的方法我们后面会介绍。现在，我们就聚焦在找出一个单独的好的学习速率的选择，$\eta$。 所以，有了这样的想法，我们可以如下设置 $\eta$。 首先，我们选择在训练数据上的代价立即开始下降而非震荡或者增加时作为 $\eta$ 的阈值的估计。这个估计并不需要太过精确。你可以估计这个值的量级，比如说从 $\eta=0.01$ 开始。如果代价在训练的前面若干回合开始下降，你就可以逐步地尝试 $\eta=0.1, 1.0,…$，直到你找到一个 $\eta$ 的值使得在开始若干回合代价就开始震荡或者增加。 相反，如果代价在 $\eta=0.01$ 时就开始震荡或者增加，那就尝试 $\eta=0.001, 0.0001,…$ 直到你找到代价在开始回合就下降的设定。按照这样的方法，我们可以掌握学习速率的阈值的量级的估计。你可以选择性地优化估计，选择那些最大的 $\eta$，比方说 $\eta=0.5$ 或者 $\eta=0.2$（这里也不需要过于精确）。 显然，$\eta$ 实际值不应该比阈值大。实际上，如果 $\eta$ 的值重复使用很多回合的话，你更应该使用稍微小一点的值，例如，阈值的一半这样的选择。这样的选择能够允许你训练更多的回合，不会减慢学习的速度。 在 MNIST 数据中，使用这样的策略会给出一个关于学习速率 $\eta$ 的一个量级的估计，大概是 $0.1$。在一些改良后，我们得到了阈值 $\eta=0.5$。所以，我们按照刚刚的取一半的策略就确定了学习速率为 $\eta=0.25$。实际上，我发现使用 $\eta=0.5$ 在 $30$ 回合内表现是很好的，所以选择更低的学习速率，也没有什么问题。 为什么使用代价函数来选择学习速率？而不是验证集这看起来相当直接。然而，使用训练代价函数来选择 $\eta$ 看起来和我们之前提到的通过验证集来确定超参数的观点有点矛盾。实际上，我们会使用验证准确率来选择\正则化超参数，minibatch 大小，和层数及隐藏元个数这些网络参数，等等。为何对学习速率要用不同的方法呢？坦白地说，这些选择其实是我个人美学偏好，个人习惯罢了。原因就是其他的超参数倾向于提升最终的测试集上的分类准确率，所以将他们通过验证准确率来选择更合理一些。然而，学习速率仅仅是偶然地影响最终的分类准确率的。学习率主要的目的是控制梯度下降的步长，监控训练代价是最好的检测步长过大的方法。 所以，这其实就是个人的偏好。在学习的前期，如果验证准确率提升，训练代价通常都在下降。所以在实践中使用那种衡量方式并不会对判断的影响太大。 学习速率调整我们一直都将学习速率设置为常量。但是，通常采用可变的学习速率更加有效。在学习的前期，权重可能非常糟糕。所以最好是使用一个较的学习速率让权重变化得更快。越往后，我们可以降低学习速率，这样可以作出更加精良的调整。 我们要如何设置学习速率呢？其实有很多方法。一种自然的观点是使用提前终止的想法。就是保持学习速率为一个常量直到验证准确率开始变差。然后按照某个量下降学习速率，比如说按照$10$ 或者 $2$。我们重复此过程若干次，直到%学习速率是初始值的 $1/1024$（或者$1/1000$）。那时就终止。 可变学习速率可以提升性能，但是也会产生大量可能的选择。这些选择会让人头疼~——~你可能需要花费很多精力才能优化学习规则。对刚开始实验，我建议使用单一的常量作为学习速率的选择。这会给你一个比较好的近似。后面，如果你想获得更好的性能，值得按照某种规则进行实验，根据我已经给出的资料。 提前停止 来确定训练的周期的数量正如我们在本章前面讨论的那样，提前停止表示在每个回合的最后，我们都要计算验证集上的分类准确率。当准确率不再提升，就终止它。这让选择回合数变得很简单。特别地，也意味着我们不再需要担心显式地掌握回合数和其他超参数的关联。而且，这个过程还是自动的。另外，提前停止也能够帮助我们避免过拟合。尽管在实验前期不采用提前停止，这样可以看到任何过匹配的信号，使用这些来选择正则化方法，但提前停止仍然是一件很棒的事。 我们需要再明确一下什么叫做分类准确率不再提升，这样方可实现提前停止。正如我们已经看到的，分类准确率在整体趋势下降的时候仍旧会抖动或者震荡。如果我们在准确度刚开始下降的时候就停止，那么肯定会错过更好的选择。一种不错的解决方案是如果分类准确率在一段时间内不再提升的时候终止。例如，我们要解决 MNIST 问题。如果分类准确度在近$10$ 个回合都没有提升的时候，我们将其终止。这样不仅可以确保我们不会终止得过快，也能够使我们不要一直干等直到出现提升。 这种 $10$ 回合不提升就终止的规则很适合 MNIST 问题的一开始的探索。然而，网络有时候会在很长时间内于一个特定的分类准确率附近形成平缓的局面，然后才会有提升。如果你尝试获得相当好的性能，这个规则可能就会太过激进了~——~停止得太草率。所以，我建议在你更加深入地理解网络训练的方式时，仅仅在初始阶段使用 $10$ 回合不提升规则，然后逐步地选择更久的回合，比如说：$20$ 回合不提升就终止，$50$ 回合不提升就终止，以此类推。当然，这就引入了一种新的需要优化的超参数！实践中，其实比较容易设置这个超参数来获得相当好的结果。类似地，对不同于 MNIST 的问题，$10$ 回合不提升就终止的规则会太过激进或者太过保守，这都取决于问题本身的特质。然而，进行一些小的实验，发现好的提前终止的策略还是非常简单的。 我们还没有在我们的 MNIST 实验中使用提前终止。原因是我们已经比较了不同的学习观点。这样的比较其实比较适合使用同样的训练回合。但是，在 network2.py 中实现提前终止还是很有价值的： 小批量数据的大小我们应该如何设置小批量数据的大小？为了回答这个问题，让我们先假设正在进行在线学习，也就是说使用大小为 $1$ 的%小批量数据。 一个关于在线学习的担忧是使用只有一个样本的小批量数据会带来关于梯度的错误估计。实际上，误差并不会真的产生这个问题。原因在于单一的梯度估计不需要绝对精确。我们需要的是确保代价函数保持下降的足够精确的估计。就像你现在要去北极点，但是只有一个不大精确的（差个 $10-20$ 度）指南针。如果你不断频繁地检查指南针，指南针会在平均状况下给出正确的方向，所以最后你也能抵达北极点。 基于这个观点，这看起来好像我们需要使用在线学习。实际上，情况会变得更加复杂。在上一章的问题中我指出我们可以使用矩阵技术来对所有在小批量数据中的样本同时计算梯度更新，而不是进行循环。所以，取决于硬件和线性代数库的实现细节，这会比循环方式进行梯度更新快好多。也许是 $50$ 和 $100$ 倍的差别。 现在，看起来这对我们帮助不大。我们使用 $100$ 的小批量数据的学习规则如下; w \rightarrow w' = w-\eta \frac{1}{100} \sum_x \nabla C_x这里是对小批量数据中所有训练样本求和。而在线学习是 w \rightarrow w' = w-\eta \nabla C_x即使它仅仅是 $50$ 倍的时间，结果仍然比直接在线学习更好，因为我们在线学习更新得太过频繁了。假设，在小批量数据下，我们将学习率扩大了 $100$ 倍，更新规则就是 w \rightarrow w' = w-\eta \sum_x \nabla C_x这看起来像做了 $100$ 次独立的在线学习。但是仅仅花费了 $50$ 次在线学习的时间。当然，其实不是同样的 100 次在线学习，因为小批量数据中 $\nabla C_x$ 是都对同样的权重进行衡量的，而在线学习中是累加的学习。使用更大的小批量数据看起来还是显著地能够进行训练加速的。 所以，选择最好的小批量数据大小也是一种折衷。太小了，你不会用上很好的矩阵库的快速计算。太大，你是不能够足够频繁地更新权重的。你所需要的是选择一个折衷的值，可以最大化学习的速度。 幸运的是，小批量数据大小的选择其实是相对独立的一个超参数（网络整体架构外的参数），所以你不需要优化那些参数来寻找好的%小批量数据大小。因此，可以选择的方式就是使用某些可以接受的值（不需要是最优的）作为其他参数的选择，然后进行不同小批量数据大小的尝试，像上面那样调整 $\eta$。画出验证准确率的值随时间（非回合）变化的图，选择哪个得到最快性能的提升的小批量数据大小。得到了小批量数据大小，也就可以对其他的超参数进行优化了。 当然，你也发现了，我这里并没有做到这么多。实际上，我们的实现并没有使用到%小批量数据更新快速方法。就是简单使用了小批量数据大小为 $10$。所以，我们其实可以通过降低小批量数据大小来进行提速。我也没有这样做，因为我希望展示小批量数据大于$1$ 的使用，也因为我实践经验表示提升效果其实不明显。在实践中，我们大多数情况肯定是要实现更快的小批量数据更新策略，然后花费时间精力来优化小批量数据大小，来达到总体的速度提升。 自动技术：我已经给出很多在手动进行超参数优化时的启发式规则。手动选择当然是种理解网络行为的方法。不过，现实是，很多工作已经使用自动化过程进行。通常的技术就是网格搜索，可以系统化地对超参数的参数空间的网格进行搜索。网格搜索的成就和限制（易于实现的变体）在 James Bergstra 和 YoshuaBengio $2012$年的论文作者为 James Bergstra 和 Yoshua Bengio（2012）。中已经给出了综述。很多更加精细的方法也被大家提出来了。 总结跟随上面的经验并不能帮助你的网络给出绝对最优的结果。但是很可能给你一个好的开始和一个改进的基础。特别地，我已经非常独立地讨论了超参数的选择。实践中，超参数之间存在着很多关系。你可能使用 $\eta$ 进行试验，发现效果不错，然后去优化 $\lambda$，发现这里又和 $\eta$ 混在一起了。在实践中，一般是来回往复进行的，最终逐步地选择到好的值。总之，启发式规则其实都是经验，不是金规玉律。你应该注意那些没有效果的尝试的信号，然后乐于尝试更多试验。特别地，这意味着需要更加细致地监控神经网络的行为，特别是验证集上的准确率。 选择超参数的难度在于如何选择超参数的方法太分散, 这些方法分散在许多的研究论文，软件程序甚至仅仅在一些研究人员的大脑中, 因而变得更加困难。很多很多的论文给出了（有时候矛盾的）建议。 设定超参数的挑战让一些人抱怨神经网络相比较其他的机器学习算法需要大量的工作进行参数选择。我也听到很多不同的版本：“的确，参数完美的神经网络可能会在这问题上获得最优的性能。但是，我可以尝试一下随机森林（或者 SVM 或者……这里脑补自己偏爱的技术）也能够工作的。我没有时间搞清楚那个最好的神经网络。” 当然，从一个实践者角度，肯定是应用更加容易的技术。这在你刚开始处理某个问题时尤其如此，因为那时候，你都不确定一个机器学习算法能够解决那个问题。但是，如果获得最优的性能是最重要的目标的话，你就可能需要尝试更加复杂精妙的知识的方法了。如果机器学习总是简单的话那是太好不过了，但也没有什么理由说机器学习非得这么简单。 参考文献[1] Michael Nielsen. CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-28. [2] Zhu Xiaohu. Zhang Freeman. Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-28.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>超参数</tag>
        <tag>Broad strategy</tag>
        <tag>Learning rate</tag>
        <tag>Early stopping</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——代码实现]]></title>
    <url>%2F2018%2F06%2F27%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 再看手写识别问题：代码让我们实现本章讨论过的这些想法（交叉熵、正则化、初始化权重等等）。我们将写出一个新的程序，network2.py，这是一个对中开发的 network.py 的改进版本。如果你没有仔细看过 network.py，那你可能会需要重读前面关于这段代码的讨论。仅仅 $74$ 行代码，也很易懂。对 network.py 的详细讲解见第一章的 使用神经网络识别手写数字——代码实现 和 network.py 一样，主要部分就是 Network 类了，我们用这个来表示神经网络。使用一个 sizes 的列表来对每个对应层进行初始化，默认使用交叉熵作为代价 cost 参数： 1234567class Network(object): def __init__(self, sizes, cost=CrossEntropyCost): self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost 最开始几行里的 方法的和 network.py 中一样，可以轻易弄懂。但是下面两行是新的，我们需要知道他们到底做了什么。12345678910### 权重初始化我们先看看 default_weight_initializer 方法，使用了我们新式改进后的初始权重方法。如我们已经看到的，使用了均值为 $0$ 而标准差为 $1/\sqrt&#123;n&#125;$，$n$ 为对应的输入连接个数。我们使用均值为 $0$ 而标准差为 $1$ 的高斯分布来初始化偏置。下面是代码：```Pythondef default_weight_initializer(self): self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] 为了理解这段代码，需要知道 np 就是进行线性代数运算的 Numpy 库。我们在程序的开头会 import Numpy。同样我们没有对第一层的神经元的偏置进行初始化。因为第一层其实是输入层，所以不需要引入任何的偏置。我们在 network.py 中做了完全一样的事情。 作为 default_weight_initializer 的补充，我们同样包含了一个 large_weight_initializer 方法。这个方法使用了第一章中的观点初始化了 权重和偏置。代码也就仅仅是和 default_weight_initializer 差了一点点了： 1234def large_weight_initializer(self): self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] 我将 larger_weight_initializer 方法包含进来的原因也就是使得跟第一章的结果更容易比较。我并没有考虑太多的推荐使用这个方法的实际情景。 交叉熵初始化方法 ``` 中的第二个新的东西就是我们初始化了 cost 属性。为了理解这个工作的原理，让我们看一下用来表示交叉熵代价的类1234567891011```Pythonclass CrossEntropyCost(object): @staticmethod def fn(a, y): return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): return (a-y) 让我们分解一下。第一个看到的是：即使使用的是交叉熵，数学上看，就是一个函数，这里我们用 Python 的类而不是 Python 函数实现了它。为什么这样做呢？答案就是代价函数在我们的网络中扮演了两种不同的角色。明显的角色就是代价是输出激活值 $a$ 和目标输出 $y$ 差距优劣的度量。这个角色通过 CrossEntropyCost.fn 方法来扮演。（注意，np.nan_to_num 调用确保了 Numpy 正确处理接近 $0$ 的对数值）但是代价函数其实还有另一个角色。回想第二章中运行反向传播算法时，我们需要计算网络输出误差，$\delta^L$。这种形式的输出误差依赖于 代价函数的选择：不同的代价函数，输出误差的形式就不同。对于交叉熵函数，输出误差就如公式所示： \delta^L = a^L-y类似地，network2.py 还包含了一个表示二次代价函数的类。这个是用来和第一章的结果进行对比的，因为后面我们几乎都在使用交叉函数。代码如下。QuadraticCost.fn 方法是关于网络输出 $a$ 和目标输出 $y$ 的二次代价函数的直接计算结果。由 QuadraticCost.delta! 返回的值基于二次代价函数的误差表达式，我们在第二章中得到它。 123456789class QuadraticCost(object): @staticmethod def fn(a, y): return 0.5*np.linalg.norm(a-y)**2 @staticmethod def delta(z, a, y): return (a-y) * sigmoid_prime(z) Network2.py 完整代码现在，我们理解了 network2.py 和 network.py 两个实现之间的主要差别。都是很简单的东西。还有一些更小的变动，下面我们会进行介绍，包含 L2 正则化的实现。在讲述正则化之前，我们看看 network2.py 完整的实现代码。你不需要太仔细地读遍这些代码，但是对整个结构尤其是文档中的内容的理解是非常重要的，这样，你就可以理解每段程序所做的工作。当然，你也可以随自己意愿去深入研究！如果你迷失了理解，那么请读读下面的讲解，然后再回到代码中。不多说了，给代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315"""network2.py~~~~~~~~~~~~~~An improved version of network.py, implementing the stochasticgradient descent learning algorithm for a feedforward neural network.Improvements include the addition of the cross-entropy cost function,regularization, and better initialization of network weights. Notethat I have focused on making the code simple, easily readable, andeasily modifiable. It is not optimized, and omits many desirablefeatures."""#### Libraries# Standard libraryimport jsonimport randomimport sys# Third-party librariesimport numpy as np#### Define the quadratic and cross-entropy cost functionsclass QuadraticCost(object): @staticmethod def fn(a, y): """Return the cost associated with an output ``a`` and desired output ``y``. """ return 0.5*np.linalg.norm(a-y)**2 @staticmethod def delta(z, a, y): """Return the error delta from the output layer.""" return (a-y) * sigmoid_prime(z)class CrossEntropyCost(object): @staticmethod def fn(a, y): """Return the cost associated with an output ``a`` and desired output ``y``. Note that np.nan_to_num is used to ensure numerical stability. In particular, if both ``a`` and ``y`` have a 1.0 in the same slot, then the expression (1-y)*np.log(1-a) returns nan. The np.nan_to_num ensures that that is converted to the correct value (0.0). """ return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a))) @staticmethod def delta(z, a, y): """Return the error delta from the output layer. Note that the parameter ``z`` is not used by the method. It is included in the method's parameters in order to make the interface consistent with the delta method for other cost classes. """ return (a-y)#### Main Network classclass Network(object): def __init__(self, sizes, cost=CrossEntropyCost): """The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using ``self.default_weight_initializer`` (see docstring for that method). """ self.num_layers = len(sizes) self.sizes = sizes self.default_weight_initializer() self.cost=cost def default_weight_initializer(self): """Initialize each weight using a Gaussian distribution with mean 0 and standard deviation 1 over the square root of the number of weights connecting to the same neuron. Initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers. """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def large_weight_initializer(self): """Initialize the weights using a Gaussian distribution with mean 0 and standard deviation 1. Initialize the biases using a Gaussian distribution with mean 0 and standard deviation 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers. This weight and bias initializer uses the same approach as in Chapter 1, and is included for purposes of comparison. It will usually be better to use the default weight initializer instead. """ self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] def feedforward(self, a): """Return the output of the network if ``a`` is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0, evaluation_data=None, monitor_evaluation_cost=False, monitor_evaluation_accuracy=False, monitor_training_cost=False, monitor_training_accuracy=False): """Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory, as is the regularization parameter ``lmbda``. The method also accepts ``evaluation_data``, usually either the validation or test data. We can monitor the cost and accuracy on either the evaluation data or the training data, by setting the appropriate flags. The method returns a tuple containing four lists: the (per-epoch) costs on the evaluation data, the accuracies on the evaluation data, the costs on the training data, and the accuracies on the training data. All values are evaluated at the end of each training epoch. So, for example, if we train for 30 epochs, then the first element of the tuple will be a 30-element list containing the cost on the evaluation data at the end of each epoch. Note that the lists are empty if the corresponding flag is not set. """ if evaluation_data: n_data = len(evaluation_data) n = len(training_data) evaluation_cost, evaluation_accuracy = [], [] training_cost, training_accuracy = [], [] for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch( mini_batch, eta, lmbda, len(training_data)) print "Epoch %s training complete" % j if monitor_training_cost: cost = self.total_cost(training_data, lmbda) training_cost.append(cost) print "Cost on training data: &#123;&#125;".format(cost) if monitor_training_accuracy: accuracy = self.accuracy(training_data, convert=True) training_accuracy.append(accuracy) print "Accuracy on training data: &#123;&#125; / &#123;&#125;".format( accuracy, n) if monitor_evaluation_cost: cost = self.total_cost(evaluation_data, lmbda, convert=True) evaluation_cost.append(cost) print "Cost on evaluation data: &#123;&#125;".format(cost) if monitor_evaluation_accuracy: accuracy = self.accuracy(evaluation_data) evaluation_accuracy.append(accuracy) print "Accuracy on evaluation data: &#123;&#125; / &#123;&#125;".format( self.accuracy(evaluation_data), n_data) print return evaluation_cost, evaluation_accuracy, \ training_cost, training_accuracy def update_mini_batch(self, mini_batch, eta, lmbda, n): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the learning rate, ``lmbda`` is the regularization parameter, and ``n`` is the total size of the training data set. """ nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = (self.cost).delta(zs[-1], activations[-1], y) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def accuracy(self, data, convert=False): """Return the number of inputs in ``data`` for which the neural network outputs the correct result. The neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation. The flag ``convert`` should be set to False if the data set is validation or test data (the usual case), and to True if the data set is the training data. The need for this flag arises due to differences in the way the results ``y`` are represented in the different data sets. In particular, it flags whether we need to convert between the different representations. It may seem strange to use different representations for the different data sets. Why not use the same representation for all three data sets? It's done for efficiency reasons -- the program usually evaluates the cost on the training data and the accuracy on other data sets. These are different types of computations, and using different representations speeds things up. More details on the representations can be found in mnist_loader.load_data_wrapper. """ if convert: results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data] else: results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data] return sum(int(x == y) for (x, y) in results) def total_cost(self, data, lmbda, convert=False): """Return the total cost for the data set ``data``. The flag ``convert`` should be set to False if the data set is the training data (the usual case), and to True if the data set is the validation or test data. See comments on the similar (but reversed) convention for the ``accuracy`` method, above. """ cost = 0.0 for x, y in data: a = self.feedforward(x) if convert: y = vectorized_result(y) cost += self.cost.fn(a, y)/len(data) cost += 0.5*(lmbda/len(data))*sum( np.linalg.norm(w)**2 for w in self.weights) return cost def save(self, filename): """Save the neural network to the file ``filename``.""" data = &#123;"sizes": self.sizes, "weights": [w.tolist() for w in self.weights], "biases": [b.tolist() for b in self.biases], "cost": str(self.cost.__name__)&#125; f = open(filename, "w") json.dump(data, f) f.close()#### Loading a Networkdef load(filename): """Load a neural network from the file ``filename``. Returns an instance of Network. """ f = open(filename, "r") data = json.load(f) f.close() cost = getattr(sys.modules[__name__], data["cost"]) net = Network(data["sizes"], cost=cost) net.weights = [np.array(w) for w in data["weights"]] net.biases = [np.array(b) for b in data["biases"]] return net#### Miscellaneous functionsdef vectorized_result(j): """Return a 10-dimensional unit vector with a 1.0 in the j'th position and zeroes elsewhere. This is used to convert a digit (0...9) into a corresponding desired output from the neural network. """ e = np.zeros((10, 1)) e[j] = 1.0 return edef sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) L1 、L2 正则化有个更加有趣的变动就是在代码中增加了 L2 正则化。尽管这是一个主要的概念上的变动，在实现中其实相当简单。对大部分情况，仅仅需要传递参数 lmbda 到不同的方法中，主要是 Network.SGD 方法。实际上的工作就是一行代码的事在 Network.update_mini_batch 的倒数第四行。这就是我们改动梯度下降规则来进行权重下降的地方。尽管改动很小，但其对结果影响却很大！ 12self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] 上面代码仅仅是下面 L2 正则化计算式子的简单代码翻译， w = \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial C_0}{\partial w}同样的道理根据 L1 正则化的计算式子可以直接翻译成代码， w =w-\frac{\eta \lambda}{n} sgn(w) - \eta \frac{\partial C_0}{\partial w}代码就是 12self.weights = [w-eta*(lmbda/n)*np.sign(w)-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] 其实这种情况在神经网络中实现一些新技术的常见现象。我们花费了近千字的篇幅来讨论正则化。概念的理解非常微妙困难。但是添加到程序中的时候却如此简单。精妙复杂的技术可以通过微小的代码改动就可以实现了。 Network2.py 构建神经网络的其它注意事项另一个微小却重要的改动是随机梯度下降方法的几个标志位的增加。这些标志位让我们可以对在代价和准确率的监控变得可能。这些标志位默认是 False! 的，但是在我们例子中，已经被置为 True! 来监控 Network 的性能。另外，network2.py 中的 Network.SGD 方法返回了一个四元组来表示监控的结果。我们可以这样使用： 12345678&gt;&gt;&gt; evaluation_cost, evaluation_accuracy,... training_cost, training_accuracy = net.SGD(training_data, 30, 10, 0.5,... lmbda = 5.0,... evaluation_data=validation_data,... monitor_evaluation_accuracy=True,... monitor_evaluation_cost=True,... monitor_training_accuracy=True,... monitor_training_cost=True) 所以，比如 evaluation_cost 将会是一个 $30$ 个元素的列表其中包含了每个周期在验证集合上的代价函数值。这种类型的信息在理解网络行为的过程中特别有用。比如，它可以用来画出展示网络随时间学习的状态。其实，这也是我在前面的章节中展示性能的方式。然而要注意的是如果任何标志位都没有设置的话，对应的元组中的元素就是空列表。 另一个增加项就是在 Network.save 方法中的代码，用来将 Network 对象保存在磁盘上，还有一个载回内存的函数。这两个方法都是使用 JSON 进行的，而非 Python 的 pickle 或者 cPickle 块。这些通常是 Python 中常见的保存和装载对象的方法。使用 JSON 的原因是，假设在未来某天，我们想改变 Network 类来允许非 sigmoid 的神经元。对这个改变的实现，我们最可能是改变在 Network.__init__ 方法中定义的属性。如果我们简单地 pickle 对象，会导致 load 函数出错。使用 JSON 进行序列化可以显式地让老的 Network 仍然能够 load。 其他也还有一些微小的变动。但是那些只是 network.py 的微调。结果就是把程序从 $74$ 行增长到了 $152$ 行。 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-27. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-27. [4] skylook. neural-networks-and-deep-learning, network2.py[DB/OL]. https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py, 2018-06-27.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>手写数字识别</tag>
        <tag>代码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络的反向传播算法]]></title>
    <url>%2F2018%2F06%2F26%2F%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[阅读本篇前，最好先阅读多层感知机的反向传播算法。反向传播的基础知识和符号定义都在该文章中。 卷积神经网络：前向传播算法CNN输入层前向传播到卷积层输入层的前向传播是CNN前向传播算法的第一步。一般输入层对应的都是卷积层。 我们这里还是以图像识别为例。先考虑最简单的，样本都是二维的黑白图片。这样输入层$X$就是一个矩阵，矩阵的值等于图片的各个像素位置的值。这时和卷积层相连的卷积核$W$就也是矩阵如果样本都是有 RGB 的彩色图片，这样输入 $X$ 就是 3 个矩阵，即分别对应 R，G 和 B 的矩阵，或者说是一个张量。这时和卷积层相连的卷积核$W$就也是张量，对应的最后一维的维度为 3 .即每个卷积核都是 3 个子矩阵组成。同样的方法，对于 3D 的彩色图片之类的样本，我们的输入 $X$ 可以是 4 维，5 维的张量，那么对应的卷积核 $W$ 也是个高维的张量。 不管维度多高，对于我们的输入，前向传播的过程可以表示为：a^2= \sigma(z^2) = \sigma(a^1*W^2 +b^2) 其中，上标代表层数，星号代表卷积，而b代表我们的偏倚, $\sigma$为激活函数，这里一般都是ReLU。和多层感知机的前向传播比较一下，其实形式非常的像，只是我们这儿是张量的卷积，而不是矩阵的乘法。同时由于$W$是张量，那么同样的位置，$W$参数的个数就比多层感知机多很多了。 为了简化我们的描述，本文后面如果没有特殊说明，我们都默认输入是3维的张量，即用RBG可以表示的彩色图片。 这里需要我们自己定义的CNN模型参数是： 一般我们的卷积核不止一个，比如有K个，那么我们输入层的输出，或者说第二层卷积层的对应的输入就K个。 卷积核中每个子矩阵的的大小，一般我们都用子矩阵为方阵的卷积核，比如FxF的子矩阵。 填充padding（以下简称P），我们卷积的时候，为了可以更好的识别边缘，一般都会在输入矩阵在周围加上若干圈的0再进行卷积，加多少圈则P为多少。 步幅stride（以下简称S），即在卷积过程中每次移动的像素距离大小。 隐藏层前向传播到卷积层现在我们再来看普通隐藏层前向传播到卷积层时的前向传播算法。 假设隐藏层的输出是M个矩阵对应的三维张量，则输出到卷积层的卷积核也是M个子矩阵对应的三维张量。这时表达式和输入层的很像，也是 a^l= \sigma(z^l) = \sigma(a^{l-1}*W^l +b^l)其中，上标代表层数，星号代表卷积，而 $b$ 代表我们的偏倚, $\sigma$ 为激活函数，这里一般都是ReLU。 也可以写成M个子矩阵子矩阵卷积后对应位置相加的形式，即： a^l= \sigma(z^l) = \sigma(\sum\limits_{k=1}^{M}z_k^l) = \sigma(\sum\limits_{k=1}^{M}a_k^{l-1}*W_k^l +b^l)和CNN输入层前向传播到卷积层的区别仅仅在于，这里的输入是隐藏层来的，而不是我们输入的原始图片样本形成的矩阵。 隐藏层前向传播到池化层池化层的处理逻辑是比较简单的，我们的目的就是对输入的矩阵进行缩小概括。比如输入的若干矩阵是NxN维的，而我们的池化大小是 $k \times k$的区域，则输出的矩阵都是 $\frac{N}{k} \times \frac{N}{k}$ 维的。 这里需要需要我们定义的CNN模型参数是： !. 池化区域的大小 k 池化的标准，一般是 MAX 或者 Average。 隐藏层前向传播到全连接层由于全连接层就是普通的 多层感知机 模型结构，因此我们可以直接使用 多层感知机 的前向传播算法逻辑，即： a^l = \sigma(z^l) = \sigma(W^la^{l-1} + b^l)这里的激活函数一般是 sigmoid 或者 tanh。 经过了若干全连接层之后，最后的一层为Softmax输出层。此时输出层和普通的全连接层唯一的区别是，激活函数是softmax函数。 这里需要需要我们定义的CNN模型参数是： 全连接层的激活函数 全连接层各层神经元的个数 CNN前向传播算法小结有了上面的基础，我们现在总结下CNN的前向传播算法。 输入：1 个图片样本，CNN 模型的层数L和所有隐藏层的类型，对于卷积层，要定义卷积核的大小 K，卷积核子矩阵的维度 F，填充大小 P，步幅 S。对于池化层，要定义池化区域大小 k 和池化标准（MAX或Average），对于全连接层，要定义全连接层的激活函数（输出层除外）和各层的神经元个数。 输出：CNN模型的输出$a^L$ 根据输入层的填充大小P，填充原始图片的边缘，得到输入张量$a^1$。 初始化所有隐藏层的参数$W,b$ for $l$=2 to $L-1$:3.1 如果第$l$层是卷积层,则输出为 a^l= ReLU(z^l) = ReLU(a^{l-1}*W^l +b^l)3.2 如果第$l$层是池化层,则输出为$ a^l= pool(a^{l-1})$, 这里的pool指按照池化区域大小k和池化标准将输入张量缩小的过程。3.3 如果第$l$层是全连接层,则输出为 a^l= \sigma(z^l) = \sigma(W^la^{l-1} +b^l) 对于输出层第L层: a^L= softmax(z^L) = softmax(W^La^{L-1} +b^L) 以上就是 CNN 前向传播算法的过程总结。有了 CNN 前向传播算法的基础，我们后面再来理解CNN的反向传播算法就简单多了。 卷积神经网络：反向传播算法计算池化层的误差 $\delta^l$ {\delta ^l} = {upsample}\left( \delta ^{l + 1} \right) \odot \sigma '\left( \right)池化层用于削减数据量，在这一层上前向传播的数据会有损失，则在反向传播时，传播来的梯度也会有所损失。一般来说，池化层没有参数，于是仅需要计算梯度反向传播的结果。 池化层的反向传播的方法是upsample，先将矩阵还原成原大小，之后： 对于最大值池化，将梯度放置于每个池化区域取得最大值的位置，其他位置为0 对于平均值池化，则把的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置(在池化的正向计算过程中需记录每个小区域内最大值的位置) 下面是一个upsample的例子： 例如对于矩阵： \left( \begin{array}{} 1& 2 \\ 3& 4 \end{array} \right)假设经过2*2的池化，还原为原来大小： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&1& 2&0 \\ 0&3&4&0 \\ 0&0&0&0 \end{array} \right)若是最大值池化，假设每个窗口的最大值位置都是左上，则传播结果为： \left( \begin{array}{ccc} 1&0&2&0 \\ 0&0& 0&0 \\ 3&0&4&0 \\ 0&0&0&0 \end{array} \right)若是经过平均值池化，则传播结果为： \left( \begin{array}{ccc} 0.25&0.25&0.5&0.5 \\ 0.25&0.25& 0.5&0.5 \\ 0.75&0.75&1&1 \\ 0.75&0.75&1&1 \end{array} \right)卷积层的误差 $\delta^l$\delta^{l-1} = \delta^{l}*rot180(w^{l}) \odot \sigma'(z^{l-1})其中 $rot180(w^{l})$ 为卷积核旋转 180 度的函数，* 为卷积符号。 举一个例子来说，对于以下卷积等式： \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&w_{12}\\ w_{21}&w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)对于 $a_11$，有 $z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}$ ，仅 $z_11$ 与其有关，则有 $\nabla a_{11} = \delta_{11}w_{11}$。对于 $a_22$，所有 $z$ 项都和该数有关，有 $\nabla a_{22} = \delta_{11}w_{22} + \delta_{12}w_{21} + \delta_{21}w_{12} + \delta_{22}w_{11}$。 依次类推，可得： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&\delta_{11}& \delta_{12}&0 \\ 0&\delta_{21}&\delta_{22}&0 \\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&w_{21}\\ w_{12}&w_{11} \end{array} \right) = \left( \begin{array}{ccc} \nabla a_{11}&\nabla a_{12}&\nabla a_{13} \\ \nabla a_{21}&\nabla a_{22}&\nabla a_{23}\\ \nabla a_{31}&\nabla a_{32}&\nabla a_{33} \end{array} \right)卷积层的权重梯度 $\partial C / \partial w$对权重的梯度为： \frac{\partial C}{\partial w^{l}} = rot180(\delta^l*a^{l-1})同样对于前向传播： \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23}\ a_{31}&a_{32}&a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&w_{12}\\ w_{21}&w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)对于z，有以下： $z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}$ $z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22}$ $z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22}$ $z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22}$ 有梯度： $\nabla w_{11} = \delta_{11}a_{11} + \delta_{12}a_{12} + \delta_{21}a_{21} + \delta_{22}a_{22}$ $\nabla w_{12} = \delta_{11}a_{12} + \delta_{12}a_{13} + \delta_{21}a_{22} + \delta_{22}a_{12}$ $\nabla w_{21} = \delta_{11}a_{21} + \delta_{12}a_{22} + \delta_{21}a_{31} + \delta_{22}a_{32}$ $\nabla w_{22} = \delta_{11}a_{22} + \delta_{12}a_{23} + \delta_{32}a_{21} + \delta_{33}a_{22}$ 反向传播为： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&\delta_{11}& \delta_{12}&0 \\ 0&\delta_{21}&\delta_{22}&0 \\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13}\\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{array} \right) = \left( \begin{array}{ccc} \nabla w_{22}&\nabla w_{21}\\ \nabla w_{12}&\nabla w_{11} \end{array} \right)卷积层的权重梯度 $\partial C / \partial b$偏置梯度较为简单，为上一层梯度和： \frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v}卷积神经网络：反向传播算法思想卷积神经网络相比于多层感知机，增加了两种新的层次——卷积层与池化层。由现在我们想把同样的思想用到CNN中，很明显，CNN有些不同的地方，不能直接去套用多层感知机的反向传播算法的公式。 多层感知机反向传播的四个基本方程： 要套用多层感知机的反向传播算法到CNN，有几个问题需要解决： 池化层没有激活函数，这个问题倒比较好解决，我们可以令池化层的激活函数为 $\sigma(z)=z$，即激活后就是自己本身。这样池化层激活函数的导数为 1. 池化层在前向传播的时候，对输入进行了压缩，那么我们现在需要向前反向推导 $\delta^{l-1}$，这个推导方法和多层感知机完全不同。 卷积层是通过张量卷积，或者说若干个矩阵卷积求和而得的当前层的输出，这和多层感知机很不相同，多层感知机的全连接层是直接进行矩阵乘法得到当前层的输出。这样在卷积层反向传播的时候，上一层的 $\delta^{l-1}$ 递推计算方法肯定有所不同。 对于卷积层，由于 $w$ 使用的运算是卷积，那么从 $\delta$ 推导出该层的所有卷积核的 $w, b$ 的方式也不同。 从上面可以看出，问题1比较好解决，但是问题 2, 3, 4 就需要好好的动一番脑筋了，而问题 2, 3, 4也是解决 CNN 反向传播算法的关键所在。另外大家要注意到的是，多层感知机中的 $a^l, z^l$ 都只是一个向量，而我们 CNN 中的 $a^l, z^l$ 都是一个张量，这个张量是三维的，即由若干个输入的子矩阵组成。 下面我们就针对问题 2, 3, 4 来一步步研究CNN的反向传播算法。 在研究过程中，需要注意的是，由于卷积层可以有多个卷积核，各个卷积核的处理方法是完全相同且独立的，为了简化算法公式的复杂度，我们下面提到卷积核都是卷积层中若干卷积核中的一个。 卷积神经网络：反向传播计算公式的证明 {\delta ^l} = {upsample}\left( \delta ^{l + 1} \right) \odot \sigma '\left( {z^l} \right)\delta^{l-1} = \delta^{l}*rot180(w^{l}) \odot \sigma'(z^{l-1})\frac{\partial C}{\partial w^{l}} = rot180(\delta^l*a^{l-1})\frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v}证明 ${\delta ^l} = {upsample}\left( \delta ^{l + 1} \right) \odot \sigma ‘ \left( {z^l} \right)$我们首先解决上面的问题2，如果已知池化层的$\delta^l$，推导出上一隐藏层的$\delta^{l-1}$。 在前向传播算法时，池化层一般我们会用 MAX 或者 Average 对输入进行池化，池化的区域大小已知。现在我们反过来，要从缩小后的误差 $\delta^l$ ，还原前一次较大区域对应的误差。 在反向传播时，我们首先会把 $\delta^l$ 的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是 MAX，则把 $\delta^l$ 的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是 Average，则把 $\delta^l$ 的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做 upsample。 这样我们就得到了上一层 $\frac{\partial C}{\partial a_k^{l-1}}$的值，要得到 $\delta_k^{l-1}$： \delta_k^{l-1} = \frac{\partial C}{\partial a_k^{l-1}} \frac{\partial a_k^{l-1}}{\partial z_k^{l-1}} = upsample(\delta_k^l) \odot \sigma'(z_k^{l-1})其中，upsample函数完成了池化误差矩阵放大与误差重新分配的逻辑。 我们概括下，对于张量$\delta^{l-1}$，我们有： \delta^{l-1} = upsample(\delta^l) \odot \sigma'(z^{l-1})证明 $\delta^{l-1} = \delta^{l}*rot180(w^{l}) \odot \sigma’(z^{l-1})$对于卷积网络，前向传播公式为： a^l= \sigma(z^l) = \sigma(a^{l-1}*w^l +b^l)注意到 $ \delta^{l-1}$ 和 $\delta^l$ 的递推关系为： \delta^{l} = \frac{\partial C}{\partial z^l} = \frac{\partial C}{\partial z^{l+1}}\frac{\partial z^{l+1}}{\partial z^{l}} = \delta^{l+1}\frac{\partial z^{l+1}}{\partial z^{l}}因此要推导出 $\delta^{l-1}$ 和 $\delta^{l}$ 的递推关系，必须计算 $\frac{\partial z^{l}}{\partial z^{l-1}}$的梯度表达式。 注意到 $z^{l}$和$z^{l-1}$ 的关系为：z^l = a^{l-1}*w^l +b^l =\sigma(z^{l-1})*w^l +b^l 因此我们有： \delta^{l-1} = \delta^{l}\frac{\partial z^{l}}{\partial z^{l-1}} = \delta^{l}*rot180(W^{l}) \odot \sigma'(z^{l-1})这里的式子其实和 DNN 的类似，区别在于对于含有卷积的式子求导时，卷积核被旋转了 180 度。即式子中的 $rot180()$，翻转 180 度的意思是上下翻转一次，接着左右翻转一次。在 DNN 中这里只是矩阵的转置。那么为什么呢？由于这里都是张量，直接推演参数太多了。我们以一个简单的例子说明为啥这里求导后卷积核要翻转。 假设我们 $l-1$ 层的输出 $a^{l-1}$ 是一个 3x3 矩阵，第 $l$ 层的卷积核 $W^l$ 是一个2x2矩阵，采用 1 像素的步幅，则输出 $z^{l}$ 是一个 2x2 的矩阵。我们简化 $b^l$ 都是 $0$,则有 a^{l-1} * w^l = z^{l}我们列出 $a,W,z$ 的矩阵表达式如下： \left( \begin{array}{ccc} a_{11}&a_{12}&a_{13} \\ a_{21}&a_{22}&a_{23}\\ a_{31}&a_{32}&a_{33} \end{array} \right) * \left( \begin{array}{ccc} w_{11}&w_{12}\\ w_{21}&w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)利用卷积的定义，很容易得出： $z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22}$ $z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22}$ $z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22}$ $z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22}$ 接着我们模拟反向求导： \nabla a^{l-1} = \frac{\partial C}{\partial a^{l-1}} = \frac{\partial C}{\partial z^{l}} \frac{\partial z^{l}}{\partial a^{l-1}} = \delta^{l} \frac{\partial z^{l}}{\partial a^{l-1}}从上式可以看出，对于 $a^{l-1}$ 的梯度误差 $\nabla a^{l-1}$，等于第 $l$ 层的梯度误差乘以 $\frac{\partial z^{l}}{\partial a^{l-1}}$，而 $\frac{\partial z^{l}}{\partial a^{l-1}}$ 对应上面的例子中相关联的 $w$ 的值。假设我们的 $z$ 矩阵对应的反向传播误差是 $\delta_{11}, \delta_{12}, \delta_{21}, \delta_{22}$ 组成的 2x2 矩阵，则利用上面梯度的式子和 4 个等式，我们可以分别写出 $\nabla a^{l-1}$ 的 9 个标量的梯度。 比如对于 $a_{11}$ 的梯度，由于在 4 个等式中 $a_{11}$ 只和 $z_{11}$ 有乘积关系，从而我们有： \nabla a_{11} = \delta_{11}w_{11}对于 $a_{12}$ 的梯度，由于在 4 个等式中 $a_{12}$ 和 $z_{12}, z_{11}$ 有乘积关系，从而我们有： \nabla a_{12} = \delta_{11}w_{12} + \delta_{12}w_{11}同样的道理我们得到： $ \nabla a_{13} = \delta_{12}w_{12} $ $\nabla a_{21} = \delta_{11}w_{21} + \delta_{21}w_{11}$ $\nabla a_{22} = \delta_{11}w_{22} + \delta_{12}w_{21} + \delta_{21}w_{12} + \delta_{22}w_{11}$ $\nabla a_{23} = \delta_{12}w_{22} + \delta_{22}w_{12}$ $\nabla a_{31} = \delta_{21}w_{21}$ $\nabla a_{32} = \delta_{21}w_{22} + \delta_{22}w_{21}$ $\nabla a_{33} = \delta_{22}w_{22}$ 这上面9个式子其实可以用一个矩阵卷积的形式表示，即： \left( \begin{array}{ccc} 0&0&0&0 \\ 0&\delta_{11}& \delta_{12}&0 \\ 0&\delta_{21}&\delta_{22}&0 \\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&w_{21}\\ w_{12}&w_{11} \end{array} \right) = \left( \begin{array}{ccc} \nabla a_{11}&\nabla a_{12}&\nabla a_{13} \\ \nabla a_{21}&\nabla a_{22}&\nabla a_{23}\\ \nabla a_{31}&\nabla a_{32}&\nabla a_{33} \end{array} \right)为了符合梯度计算，我们在误差矩阵周围填充了一圈0，此时我们将卷积核翻转后和反向传播的梯度误差进行卷积，就得到了前一次的梯度误差。这个例子直观的介绍了为什么对含有卷积的式子求导时，卷积核要翻转180度的原因。 以上就是卷积层的误差反向传播过程。 证明 $\frac{\partial C}{\partial w^{l}} = rot180(\delta^l*a^{l-1})$ 和 $\frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v}$我们现在已经可以递推出每一层的梯度误差 $\delta^l$ 了，对于全连接层，可以按 DNN 的反向传播算法求该层 $W,b$ 的梯度，而池化层并没有 $W,b$,也不用求 $W,b$ 的梯度。只有卷积层的 $W,b$ 需要求出。 对于卷积网络，前向传播公式为： a^l= \sigma(z^l) = \sigma(a^{l-1}*w^l +b^l)又因为 \delta^l \equiv \frac{\partial C}{\partial z^l}.z^l = a^{l-1}*w^l +b^l那么 \frac{\partial C}{\partial w^l} = \frac{\partial C}{\partial z^l}\frac{\partial z^l}{\partial w^l}=\delta^l \frac{\partial z^l}{\partial w^l}= \delta^l * rot180(a^{l-1})即 \frac{\partial C}{\partial w^{l}} = \delta^l*rot180(a^{l-1})而对于b,则稍微有些特殊，因为 $\delta^l$ 是张量，而 $b$ 只是一个向量，不能像 DNN 那样直接和 $\delta^l$ 相等。通常的做法是将 $\delta^l$ 的各个子矩阵的项分别求和，得到一个误差向量，即为 $b$ 的梯度： \frac{\partial C}{\partial b^{l}} = \sum\limits_{u,v}(\delta^l)_{u,v} 参考文献[1] 月见樽. CNN的反向传播[DB/OL]. https://qiankun214.github.io/2018/02/21/CNN的反向传播/, 2018-06-26. [2] oio328Loio. 神经网络学习（三）反向（BP）传播算法（1）[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79537246, 2018-06-26. [3] oio328Loio. 神经网络学习（十二）卷积神经网络与BP算法[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79705332, 2018-06-26. [4] 刘建平Pinard. 卷积神经网络(CNN)反向传播算法[DB/OL]. http://www.cnblogs.com/pinard/p/6494810.html, 2018-06-26. [5] 刘建平Pinard. 卷积神经网络(CNN)前向传播算法[DB/OL]. http://www.cnblogs.com/pinard/p/6489633.html, 2018-06-26.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>反向传播算法</tag>
        <tag>卷积神经网络</tag>
        <tag>前向传播算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——正则化]]></title>
    <url>%2F2018%2F06%2F24%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 产生过度拟合和正则化的原因拥有大量的自由参数的模型能够描述特别神奇的现象。即使这样的模型能够很好的拟合已有的数据，但并不表示是一个好模型。因为这可能只是因为模型中足够的自由度使得它可以描述几乎所有给定大小的数据集，而不需要真正洞察现象的本质。所以发生这种情形时，模型对已有的数据会表现的很好，但是对新的数据很难泛化。对一个模型真正的测验就是它对没有见过的场景的预测能力。 我们用来对 MNIST 数字分类的 30 个隐藏神经元神经网络拥有将近 24,000 个参数！当然很多。我们有 100 个隐藏元的网络拥有将近 80,000 个参数，而目前最先进的深度神经网络包含百万级或者十亿级的参数。我们应当信赖这些结果么？ 让我们通过构造一个网络泛化能力很差的例子使这个问题更清晰。我们的网络有 30 个隐藏神经元，共 23,860 个参数。但是我们不会使用所有 50,000 幅 MNIST 训练图像。相反，我们只使用前 1,000 幅图像。使用这个受限的集合，会让泛化的问题突显。我们按照之前同样的方式，使用交叉熵损失函数，学习率设置为 $\eta = 0.5$ 而小批量大小设置为 $10$。不过这里我们要训练 400 个周期。我们现在使用 network2 来研究损失函数改变的情况： 12345678&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,... monitor_evaluation_accuracy=True, monitor_training_cost=True) 使用上面的结果，我们可以画出当网络学习时代价变化的情况： 这看起来令人振奋，因为损失函数有一个光滑的下降，跟我们预期一致。注意，我只是展示了 $200$ 到 $399$ 损失函数的情况。这给出了很好的近距离理解训练后期的情况，也是出现有趣现象的地方。 让我们看看分类准确率在测试集上的表现： 这里我还是聚焦到了后面的过程。 在前 200 迭代期（图中没有显示）中准确率提升到了 82%。然后学习逐渐变缓。最终，在 280 迭代期左右分类准确率就停止了增长。后面的的迭代期，仅仅看到了在 280 迭代期准确率周围随机的小波动。将这幅图和前面的图进行对比，前面的图中和训练数据相关的代价持续平滑下降。如果我们只看那个代价，会发现我们模型的表现变得“更好”。但是测试准确率展示了提升只是一种假象。就像费米不大喜欢的那个模型一样，我们的网络在 280 迭代期后就不再能够推广到测试数据上。所以这不是有用的学习。我们说网络在 280 迭代期后就过度拟合或者过度训练了。 这里引出过度拟合的概念，过度拟合即过拟合。为了得到一致假设而使假设变得过度严格称为过拟合。过拟合的一种定义：给定一个假设空间 $H$，一个假设 $h$ 属于 $H$，如果存在其他的假设 $h’$ 属于 $H$,使得在训练样例上h的错误率比 $h’$ 小，但在整个实例分布上 $h’$ 比 $h$ 的错误率小，那么就说假设 $h$ 过度拟合训练数据。 你可能想知道这里的问题是不是由于我们看的是训练数据的代价，而对比的却是测试数据上的分类准确率导致的。 换言之，可能我们这里在进行苹果和橙子的对比。如果我们比较训练数据上的代价和测试数据上的代价，会发生什么，我们是在比较类似的度量吗？或者可能我们可以比较在两个数据集上的分类准确率啊？实际上，不管我们使用什么度量的方式，尽管细节会变化，但本质上都是一样的。让我们来看看测试数据集上的代价变化情况： 我们可以看到测试集上的代价在 15 迭代期前一直在提升，随后越来越差，尽管训练数据集上的代价表现是越来越好的。这其实是另一种模型过度拟合的迹象。尽管，这里带来了关于我们应当将 15 还是 280 迭代期当作是过度拟合开始影响学习的时间点的困扰。从一个实践角度，我们真的关心的是提升测试数据集上的分类准确率，而测试集合上的代价不过是分类准确率的一个反应。所以更加合理的选择就是将 280 迭代期看成是过度拟合开始影响学习的时间点。 另一个过度拟合的迹象在训练数据上的分类准确率上也能看出来： 准确率一直在提升接近 100%。也就是说，我们的网络能够正确地对所有 $1000$ 幅图像进行分类！而在同时，我们的测试准确率仅仅能够达到 82.27%。所以我们的网络实际上在学习训练数据集的特例，而不是能够一般地进行识别。我们的网络几乎是在单纯记忆训练集合，而没有对数字本质进行理解能够泛化到测试数据集上。 过拟合是神经网络的一个主要问题。这在现代网络中特别正常，因为网络权重 $W$ 和偏置 $b$ 数量巨大。为了高效地训练，我们需要一种检测过拟合是不是发生的技术，这样我们不会过度训练。并且我们也想要找到一些技术来降低过拟合的影响。 检测过拟合的明显方法是使用上面的方法跟踪测试数据集合上的准确率随训练变化情况。如果我们看到测试数据上的准确率不再提升，那么我们就停止训练。当然，严格地说，这其实并非是过拟合的一个必要现象，因为测试集和训练集上的准确率可能会同时停止提升。当然，采用这样的策略是可以阻止过拟合的。 实际上，我们会使用这种策略的变化形式来试验。记得之前我们载入 MNIST 数据时用了三个数据集： 123&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper() 到现在我们一直在使用 training_data 和 test_data，没有用过 validation_data。validation_data 中包含了 $10,000$ 幅数字图像，这些图像和 MNIST 训练数据集中的 $50,000$ 幅图像以及测试数据集中的$10,000$ 幅都不相同。我们会使用 validation_data 而不是 test_data 来防止过拟合。我们会为 test_data 使用和上面提到的相同的策略。我们在每个迭代期·的最后都计算在 validation_data 上的分类准确率。一旦分类准确率已经饱和，就停止训练。这个策略被称为提前停止。当然，实际应用中，我们不会立即知道什么时候准确率会饱和。相反，我们会一直训练直到我们确信准确率已经饱和（这里需要一些判定标准来确定什么时候停止）。 为何要使用 validation_data 来替代 test_data 防止过拟合问题？实际上，这是一个更为一般的策略的一部分，这个一般的策略就是使用 validation_data 来衡量不同的超参数（如迭代期}，学习率，最好的网络架构等等）的选择的效果。我们使用这样方法来找到超参数的合适值。因此，尽管到现在我并没有提及这点，但其实本书前面已经稍微介绍了一些超参数选择的方法。 当然，这仍然没有回答为什么我们用 validation_data 而不是 test_data 来防止过拟合的问题。实际上，有一个更加一般的问题，就是 为何用 validation_data 取代 test_data 来设置更好的超参数？ 为了理解这点，想想当设置超参数时，我们想要尝试许多不同的超参数选择。如果我们设置超参数是基于 test_data 的话，可能最终我们就会得到 过拟合 于 test_data 的超参数。也就是说，我们可能会找到那些符合 test_data 特点的超参数，但是网络的性能并不能够泛化到其他数据集合上。我们借助 validation_data 来克服这个问题。然后一旦获得了想要的超参数，最终我们就使用 test_data 进行准确率测量。这给了我们在 test_data 上的结果是一个网络泛化能力真正的度量方式的信心。换言之，你可以将验证集看成是一种特殊的训练数据集能够帮助我们学习好的超参数。这种寻找好的超参数的方法有时候被称为 hold out 方法，因为 validation_data 是从 traning_data 训练集中留出或者“拿出”的一部分。 在实际应用中，甚至在衡量了 test_data 的性能后，我们可能也会改变想法并去尝试另外的方法，也许是一种不同的网络架构，这将会引入寻找新的超参数的过程。如果我们这样做，难道不会产生 过拟合 于 test_data 的困境么？我们是不是需要一种数据集的潜在无限回归，这样才能够确信模型能够泛化？去除这样的疑惑其实是一个深刻而困难的问题。但是对我们实际应用的目标，我们不会担心太多。相反，我们会继续采用基于 training_data，validation_data，和 test_data 的基本 Hold-Out 方法。 我们已经研究了只使用 $1,000$ 幅训练图像时的 过拟合 问题。那么如果我们使用所有的 50,000 幅图像的训练数据会发生什么？我们会保留所有其它的参数都一样（$30$ 个隐藏元，learning-rate $0.5$ mini-batch 规模为 $10$），但是 epoch为 30 次。下图展示了分类准确率在训练和测试集上的变化情况。注意我们使用的测试数据，而不是验证集合，为了让结果看起来和前面的图更方便比较。 如你所见，测试集和训练集上的准确率相比我们使用 $1,000$ 个训练数据时相差更小。特别地，在训练数据上的最佳的分类准确率 97.86% 只比测试集上的 95.33% 准确率高了1.53%。而之前的例子中，这个差距是 17.73%！过拟合仍然发生了，但是已经减轻了不少。我们的网络从训练数据上更好地泛化到了测试数据上。一般来说，最好的降低 过拟合 的方式之一就是增加训练样本的量。有了足够的训练数据，就算是一个规模非常大的网络也不大容易 过拟合。不幸的是，训练数据其实是很难或者很昂贵的资源，所以这不是一种太切实际的选择。 正则化增加训练样本的数量是一种减轻过拟合的方法。还有其他的方法能够减轻过拟合的程度吗？一种可行的方式就是降低网络的规模。然而，大的网络拥有一种比小网络更强的潜力，所以这里存在一种应用冗余性的选项。 幸运的是，还有其他的技术能够缓解过拟合，即使我们只有一个固定的网络和固定的训练集合。这种技术就是正则化。本节，我会给出一种最为常用的正则化手段，有时候被称为权重-decay 或者 L2正则化。L2正则化的想法是增加一个额外的项到损失函数上，这个项叫做正则化-term。下面是正则化的交叉熵： C = -\frac{1}{n} \sum_{xj} \left[ y_j \ln a^L_j+(1-y_j) \ln(1-a^L_j)\right] + \frac{\lambda}{2n} \sum_w w^2其中第一个项就是常规的交叉熵的表达式。第二个现在加入的就是所有权重的平方的和。然后使用一个因子 $\lambda / 2n$ 进行量化调整，其中 $\lambda &gt; 0$ 可以称为正则化参数，而 $n$ 就是训练集合的大小。我们会在后面讨论 $\lambda$ 的选择策略。需要注意的是，正则化项里面并不包偏置。这点我们后面也会再讲述。 当然，对其他的损失函数也可以进行正则化，例如二次 损失函数。类似的正则化的形式如下： C = \frac{1}{2n} \sum_x \|y-a^L\|^2 + \frac{\lambda}{2n} \sum_w w^2两者都可以写成这样： C = C_0 + \frac{\lambda}{2n} \sum_w w^2其中 $C_0$ 是原始的 损失函数。 直觉地看，正则化的效果是让网络倾向于学习小一点的权重，其他的东西都一样的。大的权重只有能够给出损失函数第一项足够的提升时才被允许。换言之，正则化可以当做一种寻找小的权重和最小化原始的损失函数之间的折中。这两部分之间相对的重要性就由 $\lambda$ 的值来控制了：$\lambda$ 越小，就偏向于最小化原始 损失函数，反之，倾向于小的权重。 现在，对于这样的折中为何能够减轻过拟合还不是很清楚！但是，实际表现表明了这点。我们会在下一节来回答这个问题。但现在，我们来看看一个正则化的确减轻过拟合的例子。 为了构造这个例子，我们首先需要弄清楚如何将随机梯度下降算法应用在一个正则化的神经网络上。特别地，我们需要知道如何计算对网络中所有权重和偏置的偏导数 $\partial C/\partial w$ 和 $\partial C/\partial b$。对方程$C = C_0 + \frac{\lambda}{2n} \sum_w w^2$进行求偏导数得： \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} w \frac{\partial C}{\partial b} = \frac{\partial C_0}{\partial b}$\partial C_0/\partial w$ 和 $\partial C_0/\partial b$ 可以通过反向传播算法进行计算，正如《反向传播算法》中描述的那样。所以我们看到其实计算正则化的损失函数的梯度是很简单的：仅仅需要反向传播，然后加上$\frac{\lambda}{n} w$ 得到所有权重的偏导数。而偏置的偏导数就不要变化，所以偏置的梯度下降学习规则不会发生变化： b \rightarrow b -\eta \frac{\partial C_0}{\partial b}weight 的学习规则就变成： w \rightarrow w-\eta \frac{\partial C_0}{\partial w}-\frac{\eta \lambda}{n} w w = \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial C_0}{\partial w}这正和通常的梯度下降学习规则相同，除了通过一个因子 $1-\frac{\eta\lambda}{n}$ 重新调整了权重$w$。这种调整有时被称为权重衰减，因为它使得权重变小。粗看，这样会导致权重会不断下降到 $0$。但是实际不是这样的，因为如果在原始损失函数中造成下降的话其他的项（比如 $\eta \frac{\partial C_0}{\partial w}$）可能会让权重增加。 好的，这就是梯度下降工作的原理。那么随机梯度下降呢？正如在没有正则化的随机梯度下降中，我们可以通过平均 $m$ 个训练样本的 mini-batch 来估计 $\partial C_0/\partialw$。因此，为了随机梯度下降的正则化学习规则就变成 w \rightarrow \left(1-\frac{\eta \lambda}{n}\right) w -\frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial w}其中后面一项是在训练样本的 mini-batch $x$ 上进行的，而 $C_x$ 是对每个训练样本的（无正则化的）代价。这其实和之前通常的随机梯度下降的规则是一样的，除了有一个权重下降的因子 $1-\frac{\eta \lambda}{n}$。最后，为了完整，我给出偏置的正则化的学习规则。这当然是和我们之前的非正则化的情形一致了， b \rightarrow b - \frac{\eta}{m} \sum_x \frac{\partial C_x}{\partial b}这里求和也是在训练样本的 mini-batch $x$ 上进行的。 让我们看看正则化给网络带来的性能提升吧。这里还会使用有 $30$ 个隐藏神经元、 mini-batch大小为 $10$， learning-rate为 $0.5$，使用交叉熵的神经网络。然而，这次我们会使用正则化参数为 $\lambda = 0.1$。注意在代码中，我们使用的变量名字为 lmbda!，这是因为在 Python 中 lambda是关键字，有着不相关的含义。我也会再次使用 test_data，而不是 validation_data。不过严格地讲，我们应当使用 validation_data 的，因为前面已经讲过了。这里我这样做，是因为这会让结果和非正则化的结果对比起来效果更加直接。你可以轻松地调整为 validation_data，你会发现有相似的结果。 12345678910&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper()&gt;&gt;&gt; import network2&gt;&gt;&gt; net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data[:1000], 400, 10, 0.5,... evaluation_data=test_data, lmbda = 0.1,... monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,... monitor_training_cost=True, monitor_training_accuracy=True) 训练集上的损失函数持续下降，和前面无正则化的情况一样的规律： 但是这次测试集上的准确率在整个 400 迭代期内持续增加： 显然，正则化的使用能够解决过拟合的问题。而且，准确率相当高了，最高处达到了87.1%，相较于之前的 82.27%。因此，我们几乎可以确信在 400 迭代期之后持续训练会有更加好的结果。看起来，经实践检验，正则化让网络具有更好的泛化能力，显著地减轻了过拟合的影响。 如果我们摆脱人为的仅用 1,000 个训练图像的环境，转而用所有 50,000 图像的训练集，会发生什么？当然，我们之前已经看到过拟合在大规模的数据上其实不是那么明显了。那正则化能不能起到相应的作用呢？保持超参数和之前一样，30 epoch, learning-rate 为 0.5, mini-batch 大小为 10。不过我们这里需要改变正则化参数。原因在于训练数据的大小已经从 $n=1,000$ 改成了 $n=50,000$，这个会改变权重衰减因子 $1-\frac{\eta\lambda}{n}$。如果我们持续使用 $\lambda = 0.1$ 就会产生很小的权重衰减，因此就将正则化的效果降低很多。我们通过修改为 $\lambda = 5.0$ 来补偿这种下降。 好了，来训练网络，重新初始化权重： 1234&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data, 30, 10, 0.5,... evaluation_data=test_data, lmbda = 5.0,... monitor_evaluation_accuracy=True, monitor_training_accuracy=True) 我们得到： 这个结果很不错。第一，我们在测试集上的分类准确率在使用正则化后有了提升，从95.49% 到 96.49%。这是个很大的进步。第二，我们可以看到在训练数据和测试数据上的结果之间的差距也更小了。这仍然是一个大的差距，不过我们已经显著得到了本质上的降低过拟合的进步。 最后，我们看看在使用 100 个隐藏神经元和正则化参数为 $\lambda = 5.0$ 相应的测试分类准确率。我不会给出详细分析，纯粹为了好玩，来看看我们使用一些技巧（交叉熵函数和 L2正则化）能够达到多高的准确率。 12345&gt;&gt;&gt; net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)&gt;&gt;&gt; net.large_weight_initializer()&gt;&gt;&gt; net.SGD(training_data, 30, 10, 0.5, lmbda=5.0,... evaluation_data=validation_data,... monitor_evaluation_accuracy=True) 最终在验证集上的准确率达到了 97.92%。这是比 30 个隐藏元的较大飞跃。实际上，稍微改变一点，60 epoch $\eta=0.1$ 和$\lambda = 5.0$。我们就突破了 98%，达到了98.04% 的分类准确率 98%。对于 152 行代码这个效果还真不错！ 我已经把正则化描述为一种减轻过拟合和提高分类准确率的方法。实际上，这不是仅有的好处。实践表明，在使用不同的（随机）权重初始化进行多次 MNIST 网络训练的时候，我发现无正则化的网络会偶然被限制住，明显困在了损失函数的局部最优值处。结果就是不同的运行会给出相差很大的结果。对比看来，正则化的网络能够提供更容易复制的结果。 为何会这样子？从经验上看，如果损失函数是无正则化的，那么权重向量的范数可能会增长，而其他的东西都保持一样。随着时间的推移，这会导致权重向量变得非常大。所以会使得权重向量卡在朝着更多还是更少的方向上变化，因为当范数很大的时候梯度下降带来的变化仅仅会引起在那个方向发生微小的变化。我相信这个现象让我们的学习算法更难有效地探索权重空间，最终导致很难找到损失函数的最优值。 为什么正则化可以帮助减轻过度拟合我们已经看到了正则化在实践中能够减少过拟合了。这是令人振奋的，不过，这背后的原因还不得而知！通常的说法是：小的权重在某种程度上，意味着更低的复杂性，也就对数据给出了一种更简单却更强大解释，因此应该优先选择。这虽然很简短，不过暗藏了一些可能看起来会令人困惑的因素。让我们将这个解释细化，认真地研究一下。现在给一个简单的数据集，我们为其建立模型： 这里我们其实在研究某种真实的现象，$x$ 和 $y$ 表示真实的数据。我们的目标是训练一个模型来预测 $y$ 关于 $x$ 的函数。我们可以使用神经网络来构建这个模型，但是我们先来个简单的：用一个多项式来拟合数据。这样做的原因其实是多项式相比神经网络能够让事情变得更加清楚。一旦我们理解了多项式的场景，对于神经网络可以如法炮制。现在，图中有十个点，我们就可以找到唯一的 $9$ 阶多项式 $y=a_0x^9 + a_1x^8 + … + a_9$ 来完全拟合数据。下面是多项式的图像，这里我不明确列出这些系数。 这给出了一个准确的拟合。但是我们同样也能够使用线性模型 $y=2x$ 得到一个好的拟合效果： 哪个是更好的模型？哪个更可能是真的？还有哪个模型更可能泛化到其他的拥有同样现象的样本上？ 这些都是很难回答的问题。实际上，我们如果没有更多关于真实现象背后的信息的话，并不能确定给出上面任何一个问题的答案。但是让我们考虑两种可能的情况：（1）9 阶多项式实际上是完全描述了真实情况的模型，最终它能够很好地泛化；（2）正确的模型是 $y=2x$，但是存在着由于测量误差导致的额外的噪声，使得模型不能够准确拟合。 先验假设无法说出哪个是正确的（或者，如果还有其他的情况出现）。逻辑上讲，这些都可能出现。并且这不是微不足道的差异。在给出的数据上，两个模型的表现其实是差不多的。但是假设我们想要预测对应于某个超过了图中所有的 $x$ 的 $y$ 的值，在两个模型给出的结果之间肯定有一个极大的差距，因为 9 阶多项式模型肯定会被 $x^9$ 主导，而线性模型只是线性的增长。 在科学中，一种观点是我们除非不得已应该追随更简单的解释。当我们找到一个简单模型似乎能够解释很多数据样本的时候，我们都会激动地认为发现了规律！我们怀疑模型必须表达出某些关于现象的内在的真理。如上面的例子，线性模型加噪声肯定比多项式更加可能。我们会认为线性模型加噪声表达出了一些潜在的真理。从这个角度看，多项式模型仅仅是学习到了局部噪声的影响效果。所以尽管多项式对于这些特定的数据点表现得很好。模型最终会在未知数据上的泛化上出现问题，所以噪声线性模型具有更强大的预测能力。 让我们从这个观点来看神经网络。假设神经网络大多数有很小的权重，这最可能出现在正则化的网络中。更小的权重意味着网络的行为不会因为我们随便改变了一个输入而改变太大。这会让正则化网络学习局部噪声的影响更加困难。将它看做是一种让单个的证据不会影响网络输出太多的方式。相对的， 正则化网络学习去对整个训练集中经常出现的证据进行反应。对比看，大权重的网络可能会因为输入的微小改变而产生比较大的行为改变。所以一个无正则化的网络可以使用大的权重来学习包含训练数据中的噪声的大量信息的复杂模型。简言之，正则化网络受限于根据训练数据中常见的模式来构造相对简单的模型，而能够抵抗训练数据中的噪声的特性影响。我们的想法就是这可以让我们的网络对看到的现象进行真实的学习，并能够根据已经学到的知识更好地进行泛化。 所以，倾向于更简单的解释的想法其实会让我们觉得紧张。人们有时候将这个想法称为“奥卡姆剃刀原则”，然后就会热情地将其当成某种科学原理来应用这个法则。但是，这就不是一个一般的科学原理。也没有任何先验的逻辑原因来说明简单的解释就比更为复杂的解释要好。实际上，有时候更加复杂的解释其实是正确的。 下面有三点提示：第一，确定两种解释中哪个“更加简单”其实是一件相当微妙的工作。第二，即使我们可以做出这样一个判断，简单性也是一个使用时需要相当小心的指导！第三，对模型真正的测试不是简单性，而是它在新场景中对新的活动中的预测能力。 所以，我们应当时时记住这一点，正则化的神经网络常常能够比非正则化的泛化能力更强，这只是一种实验事实（empirical fact）。 我已经在上面讲过了为何现在还没有一个人能够发展出一整套具有说服力的关于正则化可以帮助网络泛化的理论解释。实际上，研究者们不断地在写自己尝试不同的正则化方法，然后看看哪种表现更好，尝试理解为何不同的观点表现的更好。所以你可以将正则化看做某种任意整合的技术。尽管其效果不错，但我们并没有一套完整的关于所发生情况的理解，仅仅是一些不完备的启发式规则或者经验。 这里也有更深的问题，这个问题也是有关科学的关键问题：我们如何泛化。正则化能够给我们一种计算上的魔力帮助神经网络更好地泛化，但是并不会带来原理上理解的指导，甚至不会告诉我们什么样的观点才是最好的。 这实在是令人困扰，因为在日常生活中，我们人类在泛化上表现很好。给一个儿童几幅大象的图片，他就能快速地学会认识其他的大象。当然，他们偶尔也会搞错，很可能将一只犀牛误认为大象，但是一般说来，这个过程会相当准确。所以我们有个系统人的大脑拥有超大量的自由变量。在受到仅仅少量的训练图像后，系统学会了在其他图像上的推广。某种程度上，我们的大脑的正则化做得特别好！怎么做的？现在还不得而知。我期望若干年后，我们能够发展出更加强大的技术来正则化神经网络，最终这些技术会让神经网络甚至在小的训练集上也能够学到强大的泛化能力。 实际上，我们的网络已经比我们预先期望的要好一些了。拥有 100 个隐藏元的网络会有接近 80,000 个参数。我们的训练集仅仅有 50,000 幅图像。这好像是用一个 80,000 阶的多项式来拟合 50,000 个数据点。我们的网络肯定会过拟合得很严重。但是，这样的网络实际上却泛化得很好。为什么？这一点并没有很好地理解。这里有个猜想：梯度下降学习的动态有一种自正则化的效应。这真是一个意料之外的巧合，但也带来了对于这种现象本质无知的不安。不过，我们还是会在后面依照这种实践的观点来应用正则化技术的。神经网络也是由于这点表现才更好一些。 现在我们回到前面留下来的一个细节：L2 正则化没有限制偏置，以此作为本节的结论。当然了，对正则化的过程稍作调整就可以对偏置进行规范了。实践看来，做出这样的调整并不会对结果改变太多，所以，在某种程度上，对不对偏置进行正则化其实就是一种习惯了。然而，需要注意的是，有一个大的偏置并不会像大的权重那样会让神经元对输入太过敏感。所以我们不需要对大的偏置所带来的学习训练数据的噪声太过担心。同时，允许大的偏置能够让网络更加灵活。因为，大的偏置让神经元更加容易饱和，这有时候是我们所要达到的效果。所以，我们通常不会对偏置进行正则化。 L1 正则化L1正则化：这个方法是在未正则化 的损失函数上加上一个权重绝对值的和： C = C_0 + \frac{\lambda}{n} \sum_w |w|凭直觉地看，这和 L2正则化相似，惩罚大的权重，倾向于让网络优先选择小的权重。当然，L1正则化和 L2正则化并不相同，所以我们不应该期望从 L1正则化得到完全同样的行为。让我们来试着理解使用 L1正则化训练的网络和 L2正则化训练的网络所不同的行为。 首先，我们会研究一下 cost-func 的偏导数。对$C = C_0 + \frac{\lambda}{n} \sum_w |w|$求导我们有 \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w}+ \frac{\lambda}{n} {\rm sgn}(w)其中 ${\rm sgn}(w)$ 就是 $w$ 的正负号，即 $w$ 是正数时为 $+1$，而 $w$ 为负数时为 $-1$。使用这个表达式，我们可以轻易地对反向传播算法进行修改从而使用基于 L1正则化的随机梯度下降进行学习。对 L1正则化的网络进行更新的规则就是 w \rightarrow w' =w-\frac{\eta \lambda}{n} sgn(w) - \eta \frac{\partial C_0}{\partial w}其中和往常一样，我们可以用一个小批量数据的均值来估计 $\partial C_0/\partial w$。对比 L2 正则化的更新规则， w \rightarrow w' = w\left(1 - \frac{\eta \lambda}{n} \right)- \eta \frac{\partial C_0}{\partial w}在两种情形下正则化的效果就是缩小权重。这符合我们的直觉，两种正则化都惩罚大的权重。但权重 缩小的方式不同。在 L1正则化中，权重 通过一个常量向 $0$ 进行缩小。在 L2正则化中，权重 通过一个和 $w$ 成比例的量进行缩小的。所以，当一个特定的权重 绝对值 $|w|$ 很大时，L1正则化的权重 缩小得远比 L2正则化要小得多。相反，当一个特定的权重绝对值 $|w|$ 很小时，L1正则化的权重 缩小得要比 L2正则化大得多。最终的结果就是：L1正则化倾向于聚集网络的权重 在相对少量的高重要度连接上，而其他权重 就会被驱使向 $0$ 接近。 我在上面的讨论中其实忽略了一个问题~——~在 $w=0$ 的时候，偏导数 $\partial C/\partial w$ 未定义。原因在于函数 $|w|$ 在 $w=0$ 时有个“直角”，事实上，导数是不存在的。不过也没有关系。我们下面要做的就是应用通常的（无正则化的）随机梯度下降的规则在 $w=0$ 处。这应该不会有什么问题，凭直觉地看，正则化的效果就是缩小权重，显然，不能对一个已经是 $0$ 的权重进行缩小了。更准确地说，我们将会使用上述方程并约定 $ sgn(0) = 0$。这样就给出了一种细致又紧凑的规则来进行采用 L1 正则化的随机梯度下降学习。 L1、L2 正则化与参数估计的关系 通过上面的推导我们可以发现，最大后验估计与最大似然估计最大的不同在于p(参数)项，所以可以说最大后验估计是正好可以解决机器学习缺乏先验知识的缺点，将先验知识加入后，优化损失函数。 其实p(参数)项正好起到了正则化的作用。如：如果假设p(参数)服从高斯分布，则相当于加了一个L2 正则化；如果假设p(参数)服从拉普拉斯分布，则相当于加了一个L1 正则化。(内容引自《贝叶斯估计、最大似然估计、最大后验估计三者的区别》) 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-24. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-24. [3] oio328Loio. 神经网络学习（九）优化方法：正则化[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79597995. 2018-06-24.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>正则化</tag>
        <tag>奥卡姆剃刀原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——弃权]]></title>
    <url>%2F2018%2F06%2F24%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E5%BC%83%E6%9D%83%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 2 Improving the way neural networks learn 弃权（Dropout）的过程弃权是一种相当激进的技术。和规范化不同，弃权技术并不依赖于代价函数的修改。而是，在弃权中，我们改变了网络本身。在介绍它为什么能工作，以及所得到的结果前，让我描述一下弃权（Dropout）基本的工作机制。 特别地，假设我们有一个训练数据 $x$ 和 对应的目标输出 $y$。通常我们会通过在网络中前向传播 $x$ ，然后进行反向传播来确定对梯度的贡献。使用弃权（Dropout）技术，这个过程就改了。我们会从随机（临时）地删除网络中的一半的隐藏神经元开始，同时让输入层和输出层的神经元保持不变。在此之后，我们会得到最终如下线条所示的网络。注意那些被弃权（Dropout）的神经元，即那些临时被删除的神经元，用虚圈表示在图中： 我们前向传播输入 $x$，通过修改后的网络，然后反向传播结果，同样通过这个修改后的网络。在一个小批量数据的若干样本上进行这些步骤后，我们对有关的权重和偏置进行更新。然后重复这个过程，首先重置弃权（Dropout）的神经元，然后选择一个新的随机的隐藏神经元的子集进行删除，估计对一个不同的小批量数据的梯度，然后更新权重和偏置。通过不断地重复，我们的网络会学到一个权重和偏置的集合。当然，这些权重和偏置也是在一半的隐藏神经元被弃权（Dropout）的情形下学到的。当我们实际运行整个网络时，是指两倍的隐藏神经元将会被激活。为了补偿这个，我们将从隐藏神经元出去的权重减半。 为什么弃权（Dropout）有效？这个弃权（Dropout）过程可能看起来奇怪，像是临时安排的。为什么我们会指望这样的方法能够行正则化呢？为了解释所发生的事，我希望你停下来想一下标准没有弃权（Dropout））的训练方式。特别地，想象一下我们训练几个不同的神经网络，都使用同一个训练数据。当然，网络可能不是从同一初始状态开始的，最终的结果也会有一些差异。出现这种情况时，我们可以使用一些平均或者投票的方式来确定接受哪个输出。例如，如果我们训练了五个网络，其中三个把一个数字分类成 “3”，那很可能它就是“3”。另外两个可能就犯了错误。这种平均的方式通常是一种强大（尽管代价昂贵）的方式来减轻过拟合。原因在于不同的网络可能会以不同的方式过拟合，平均法可能会帮助我们消除那样的过拟合。 那么这和弃权（Dropout）有什么关系呢？启发式地看，当我们弃权（Dropout）掉不同的神经元集合时，有点像我们在训练不同的神经网络。所以，弃权（Dropout）过程就如同大量不同网络的效果的平均那样。不同的网络会以不同的方式过拟合了，所以，弃权（Dropout）过的网络的效果会减轻过拟合。 一个相关的启发式解释在早期使用这项技术的论文中曾经给出：“因为神经元不能依赖其他神经元特定的存在，这个技术其实减少了复杂的互适应的神经元。所以，强制要学习那些在神经元的不同随机子集中更加健壮的特征。” 换言之，如果我们将我们的神经网络看做一个进行预测的模型的话，我们就可以将弃权（Dropout）看做是一种确保模型对于一部分证据丢失健壮的方式。这样看来，弃权（Dropout）和 L1、L2 regularization也是有相似之处的，这也倾向于更小的权重，最后让网络对丢失个体连接的场景更加健壮。 对于弃权的理解CSDN: 神经网络之dropout层 参考文献[1] Michael Nielsen.CHAPTER 2 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html. 2018-06-26. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-26. [3] 鹿往森处走. 神经网络之dropout层[DB/OL]. https://www.cnblogs.com/zyber/p/6824980.html, 2018-06-26.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>弃权（Dropout）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——Softmax]]></title>
    <url>%2F2018%2F06%2F22%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94Softmax%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn Softmax 前言我们大多数情况会使用交叉熵来解决学习缓慢的问题。但是，我希望简要介绍一下另一种解决这个问题的方法，基于 softmax 神经元层。在人工神经网络（ANN）中，softmax 通常被用作输出层的激活函数。这不仅是因为它的效果好，而且因为它使得 ANN 的输出值更易于理解。同时，softmax 配合 log 似然代价函数，其训练效果也要比采用二次代价函数的方式好。 Softmax 函数性质 softmax的函数公式如下： a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},在公式中的指数确保了所有的输出激活值是正数。然后方程中分母的求和又保证了 softmax 的输出和为 $1$。这个特定的形式确保输出激活值形成一个概率分布的自然的方式。你可以将其想象成一种重新调节 $z^L_j$ 的方法，然后将这个结果整合起来构成一个概率分布。 softmax函数最明显的特点在于：它把每个神经元的输入占当前层所有神经元输入之和的比值，当作该神经元的输出。这使得输出更容易被解释：神经元的输出值越大，则该神经元对应的类别是真实类别的可能性更高。 softmax 的单调性 证明如果 $j=k$ 则 $\partial a^L_j / \partial z^L_k$ 为正，$j \neq k$ 时为负。结果是，增加 $z^L_j$ 会提高对应的输出激活值 $a^L_j$ 并降低其他所有输出激活值。单调性证明见后文。 softmax的非局部性 softmax 层的一个好处是输出 $a^L_j$ 是对应带权输入 $a^L_j = \sigma(z^L_j)$ 的函数。由于分母求和所有的 $e^{z^L_k}$ 所以计算式子中计算每一个 $a_j^L$ 都与其他 $a_j^L$ 紧密相关。深入理解就是对于 softmax 层来说：任何特定的输出激活值 $a^L_j$ 依赖所有的带权输入。 逆转softmax层 假设我们有一个使用 softmax 输出层的神经网络，然后激活值 $a^L_j$ 已知。容易证明对应带权输入的形式为 $z^L_j = \ln a^L_j + C$，其中常量 $C$ 是独立于 $j$ 的。 Softmax 解决学习缓慢问题我们现在已经对柔性最大值神经元层有了一定的认识。但是我们还没有看到一个柔性最大值层会怎么样解决学习缓慢问题。为了理解这点，让我们先定义一个对数似然函数。我们使用 $x$ 表示网络的训练输入，$y$ 表示对应的目标输出。然后关联这个训练输入的代价函数就是 C \equiv -\ln a^L_y所以，如果我们训练的是 MNIST 图像，输入为 $7$ 的图像，那么对应的对数似然代价就是 $-\ln a_7^L$。看看这个直觉上的含义，想想当网络表现很好的时候，也就是确认输入为 $7$ 的时候。这时，他会估计一个对应的概率 $a_7^L$ 跟$1$ 非常接近，所以代价 $-\ln a_7^L$ 就会很小。反之，如果网络的表现糟糕时，概率$a_7^L$ 就变得很小，代价 $-\ln a_7^L$ 随之增大。所以对数似然代价函数也是满足我们期待的代价函数的条件的。 那关于学习缓慢问题呢？为了分析它，回想一下学习缓慢的关键就是量 $\partial C /\partial w^L_{jk}$ 和 $\partial C / \partial b^L_j$ 的变化情况。这里我不会显式地给出详细的推导，但是通过一点代数运算你会得 \frac{\partial C}{\partial b^L_j} = a^L_j-y_j \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k (a^L_j-y_j)这些方程其实和我们前面对交叉熵得到的类似。而且，正如前面的分析，这些表达式确保我们不会遇到学习缓慢的问题。事实上，把一个具有对数似然代价的 softmax 输出层，看作与一个具有交叉熵代价的 S 型输出层非常相似，这是很有用的。 有了这样的相似性，你应该使用一个具有交叉熵代价的 S 型输出层，还是一个具有对数似然代价的柔性最大值输出层呢？实际上，在很多应用场景中，这两种方式的效果都不错。作为一种通用的视角，柔性最大值加上对数似然的组合更加适用于那些需要将输出激活值解释为概率的场景。那并不总是一个需要关注的问题，但是在诸如 MNIST 这种有着不重叠的分类问题上确实很有用。 数学形式证明 Softmax 有效性softmax的函数公式如下： a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},softmax在的求导结果比较特别，分为两种情况。 上文讲到，二次代价函数在训练ANN时可能会导致训练速度变慢的问题。那就是，初始的输出值离真实值越远，训练速度就越慢。这个问题可以通过采用交叉熵代价函数来解决。其实，这个问题也可以采用另外一种方法解决，那就是采用 softmax 激活函数，并采用log似然代价函数（log-likelihood cost function）来解决。 log似然代价函数的公式为： C = - \sum_i y_i log a_i注意这种情况：其中，表示第 $a_k$ 个神经元的输出值，$y_k$ 表示第 k 个神经元对应的真实值，取值为 0 或 1 。由于 $y_k$ 取值为 0 或 1 ，对于每一个样本， $y_1,y_2,..,y_k$ 只会有一个取 1 其余的都取值为0， 所以对数似然函数求和符号可以去掉，化简为 C \equiv -\ln a^L_j 为了检验 softmax 和这个代价函数也可以解决上述所说的训练速度变慢问题，接下来的重点就是推导ANN的权重 w 和偏置 b 的梯度公式。 先求损失函数对偏置b的偏导数： 当 $i=j$ 时，带入 上面的结果$\frac{\partial a^L_j}{\partial z^L_i}=a_j^L(1-a_j^L)$ \begin{aligned} \frac{\partial C}{\partial b_{j}^L} &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_i} \\ &= - \frac{1}{a^L_j} [a_j^L(1-a_j^L)] \\ &= a_j^L -1 \end{aligned} 当 $i\not= j$ 时，带入 上面的结果$\frac{\partial a^L_j}{\partial z^L_i}=-a_j^La_i^L$ \begin{aligned} \frac{\partial C}{\partial b_{j}^L} &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_i} \\ &= - \frac{1}{a^L_j} (-a_j^La_i^L) \\ &= a_i^L \end{aligned}根据反向传播的四个方程，具体分析见《反向传播算法》 可以知道，$\frac{\partial C}{\partial b^l_j} =\delta^l_j$ 和 $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ 所以,当 $i=j$ 时， \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k (a^L_j-1)当 $i\not= j$ 时， \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k a_i举个例子通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 2, 3, 4 ],那么经过softmax函数作用后概率分别就是=[e^2/(e^2+e^3+e^4),e^3/(e^2+e^3+e^4),e^4/(e^2+e^3+e^4)] = [0.0903,0.2447,0.665],如果这个样本正确的分类是第二个的话，那么计算出来的偏导（实际上这个偏导就是 $\delta^L$ 或者说是 $\partial C/\partial b^L$ ）就是[0.0903,0.2447-1,0.665]=[0.0903,-0.7553,0.665]，是不是非常简单！然后再根据这个进行back propagation就可以了 注意！当 $y_j$ 取值不为 0 或 1，而是区间 [0,1] 的一个实数值时，上面的式子只需稍稍做点修改，只需把下面式子中 $\frac{\partial C}{\partial a^L_j}$ 的结果从 $\frac{1}{a^L_j}$ 改为 $\frac{y_i}{a^L_j}$ 即可， \begin{aligned} \frac{\partial C}{\partial b_{j}^L} &= \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_i} \\ &= - \frac{1}{a^L_j} [a_j^L(1-a_j^L)] \\ &= a_j^L -1 \end{aligned}其它的求导过程也要做相应调整。所以在有的地方会看到这样的公式， \frac{\partial C}{\partial b^L_j} = a^L_j-y_j \frac{\partial C}{\partial w^L_{jk}} = a^{L-1}_k (a^L_j-y_j)两者都是正确的，只是因为前提不一样，所以结论也有差异。 交叉熵与对数似然的关系结论：交叉熵和最大似然的loss函数是一致的，在样本所属分类是唯一的情况下。 两者能够和谐统一的关键点是： 样本所属类别是唯一的，样本一定是某一类的，似然的思想是抽样样本的概率最大化，所以每一个样本只能处于一个固定的状态。这就使得每个样本的概率形式可以写成一个综合的形式，而综合的形式呢刚好可以在log下拆分成交叉熵的样子。在多类下，若样本所属类别是唯一的，最大似然的loss与交叉熵的loss仍然是一致的。 论证： 二项分布 二项分布也叫 0-1 分布，如随机变量 x 服从二项分布，关于参数 μ（0≤μ≤1），其值取 1 和取 0 的概率如下： p(x=1|\mu)=\mu p(x=0|\mu)=1-\mu则在 x 上的概率分布为： \text{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x}服从二项分布的样本集的对数似然函数 给定样本集 D={x1,x2,…,xB} 是对随机变量 x 的观测值，假定样本集从二项分布 p(x|μ) 中独立（p(x1,x2,…,xN)=∏ip(xi)）采样得来，则当前样本集关于 μ 的似然函数为： p(\mathcal D|\mu)=\prod_{n=1}^Np(x_n|\mu)=\prod_{n=1}^N\mu^{x_n}\left(1-\mu\right)^{1-x_n}从频率学派的观点来说，通过最大似然函数的取值，可以估计参数 μ，最大化似然函数，等价于最大化其对数形式： 求其关于 μ 的导数，解得 μ 的最大似然解为： \mu_{ML}=\frac1N\sum_{n=1}^Nx_n这里我们仅关注： \ln P(\mathcal D|\mu)=\sum_{n=1}^Nx_n\ln \mu+(1-x_n)\ln(1-\mu)交叉熵损失函数 L_H(\mathbf x,\mathbf z)=-\sum_{n=1}^Nx_n\log z_n+(1-x_n)\log(1-z_n)x 表示原始信号，z 表示重构信号。（损失函数的目标是最小化，似然函数则是最大化，二者仅相差一个符号）。 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-22. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-22. [3] __鸿. softmax的log似然代价函数（公式求导）[DB/OL]. https://blog.csdn.net/u014313009/article/details/51045303. 2018-06-22. [4] 忆臻HIT_NLP. 手打例子一步一步带你看懂softmax函数以及相关求导过程[DB/OL]. https://www.jianshu.com/p/ffa51250ba2e. 2018-06-22.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>Softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[改进神经网络的学习方法——交叉熵]]></title>
    <url>%2F2018%2F06%2F21%2F%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%86%B5%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 3 Improving the way neural networks learn 为什么需要交叉熵代价函数人类却能够根据明显的犯错快速地学习到正确的东西。相反，在我们的错误不是很好地定义的时候，学习的过程会变得更加缓慢。但神经网络却不一定如此，这种行为看起来和人类学习行为差异很大。人工神经元在其犯错较大的情况下其实学习很有难度。 为了理解这个问题的源头，想想我们的神经元是通过改变权重和偏置，并以一个代价函数的偏导数 $\partial C/\partial w$ 和 $\partial C/\partial b$ 决定的速度学习。所以，我们在说“学习缓慢”时，实际上就是说这些偏导数很小。 用上一章《反向传播算法》的符号定义，对于二次代价函数,输出层的权重的偏导数为 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k (a^L_j-y_j) \sigma'(z^L_j)项 $\sigma’(z^L_j)$ 会在一个输出神经元困在错误值时导致学习速度的下降。 我们可以从这幅图看出，当神经元的输出接近 $1$ 的时候，曲线变得相当平，所以$\sigma’(z)$ 就很小了。上面式子中的 $\frac{\partial C}{\partial w^L_{jk}}$ 也会非常小。这其实就是学习缓慢的原因所在。而且，我们后面也会提到，这种学习速度下降的原因实际上也是更加一般的神经网络学习缓慢的原因，并不仅仅是在这个特例中特有的。 注意，在输出层使用线性神经元时使用二次代价函数。假设我们有一个多层多神经元网络，最终输出层的神经元都是线性神经元，输出不再是$S$型函数作用的结果，而是 $a^L_j = z^L_j$。如果我们使用二次代价函数，那么对单个训练样本 $x$ 的输出误差就是 \delta^L = a^L-y这表明如果输出神经元是线性的那么二次代价不再会导致学习速度下降的问题。在此情形下，二次代价函数就是一种合适的选择。 但是如果输出神经元是$S$型函数作用的结果，我们就最好考虑其他的代价函数。 交叉熵代价函数的定义那么我们如何解决这个问题呢？研究表明，我们可以通过使用交叉熵函数来替换二次代价函数。为了理解什么是交叉熵，我们稍微改变一下之前的简单例子。假设，我们现在要训练一个包含若干输入变量的的神经元，$x_1, x_2, \ldots$ 对应的权重为 $w_1,w_2, \ldots$ 和偏置 $b$： 神经元的输出就是 $a = \sigma(z)$，其中 $z = \sum_j w_j x_j+b$ 是输入的带权和。我们如下定义这个神经元的交叉熵代价函数： C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right]其中 $n$ 是训练数据的总数，求和是在所有的训练输入 $x$ 上进行的，$y$ 是对应的目标输出。 对于交叉熵代价函数，针对一个训练样本 $x$ 的输出误差 $\delta^L$为 \delta^L = a^L-y关于输出层的权重的偏导数为 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k (a^L_j-y_j)这里 $\sigma’(z^L_j)$ 就消失了，所以交叉熵避免了学习缓慢的问题。 那么我们应该在什么时候用交叉熵来替换二次代价函数？实际上，如果在输出神经元是$S$时，交叉熵一般都是更好的选择。为什么？考虑一下我们初始化网络的权重和偏置的时候通常使用某种随机方法。可能会发生这样的情况，这些初始选择会对某些训练输入误差相当明显，比如说，目标输出是 $1$，而实际值是$0$，或者完全反过来。如果我们使用二次代价函数，那么这就会导致学习速度的下降。它并不会完全终止学习的过程，因为这些权重会持续从其他的样本中进行学习，但是显然这不是我们想要的效果。 交叉熵在分类问题中的应用交叉熵损失函数应用在分类问题中时，不管是单分类还是多分类，类别的标签都只能是 0 或者 1。 交叉熵在单分类问题中的应用这里的单类别是指，每一张图像样本只能有一个类别，比如只能是狗或只能是猫。交叉熵在单分类问题上基本是标配的方法 loss = -\sum_{i=1}^ny_i log(\hat{y}_i)上式为一张样本的 $loss$ 计算方法。式中 $n$ 代表着 $n$ 种类别。举例如下， 交叉熵在多标签问题中的应用这里的多类别是指，每一张图像样本可以有多个类别，比如同时包含一只猫和一只狗和单分类问题的标签不同，多分类的标签是n-hot。 值得注意的是，这里的Pred采用的是sigmoid函数计算。将每一个节点的输出归一化到[0,1]之间。所有Pred值的和也不再为1。换句话说，就是每一个Label都是独立分布的，相互之间没有影响。所以交叉熵在这里是单独对每一个节点进行计算，每一个节点只有两种可能值，所以是一个二项分布。前面说过对于二项分布这种特殊的分布，熵的计算可以进行简化。 同样的，交叉熵的计算也可以简化，即 loss =-ylog(\hat{y})-(1-y)log(1-\hat{y})注意，上式只是针对一个节点的计算公式。这一点一定要和单分类loss区分开来。 交叉熵代价函数对权重求导的证明交叉熵代价函数的定义： C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right]代价函数 $C$ 对 $w^L_{jk}$ 求偏导 \begin{aligned} \frac{\partial C}{\partial w_{jk}^L} &= -\frac{1}{n} \sum_x \left( \frac{y^L_j }{\sigma(z^L_j)} -\frac{(1-y^L_j)}{1-\sigma(z^L_j)} \right) \frac{\partial \sigma(z^L_j)}{\partial w_{jk}^L} \\ &= -\frac{1}{n} \sum_x \left( \frac{y^L_j }{\sigma(z^L_j)} -\frac{(1-y^L_j)}{1-\sigma(z^L_j)} \right)\sigma'(z^L_j) a_k^{L-1} \\ &=\frac{1}{n} \sum_x \frac{\sigma'(z^L_j) a_k^{L-1}}{\sigma(z^L_j) (1-\sigma(z^L_j))} (\sigma(z^L_j)-y_j) \end{aligned}其中$\frac{\partial z^l_j}{\partial w^l_{jk}} = a_k^{l-1}$ 来自，根据 $z_j^l$ 定义 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j所以 \frac{\partial z^l_j}{\partial w^l_{jk}} = a_k^{l-1}根据 $\sigma(z) = 1/(1+e^{-z})$ 的定义， \begin{aligned} \sigma'(z) &= (\frac{1}{1+e^{-z}})' \\ &= \frac{e^{-z}}{(1+e^{-z})^{2}} \\ &= \frac{1+e^{-z}-1}{(1+e^{-z})^{2}} \\ &= \frac{1}{(1+e^{-z})}(1-\frac{1}{(1+e^{-z})}) \\ &= \sigma(z)(1-\sigma(z)) \\ \end{aligned}把 $\sigma’(z)$ 带入 $\frac{\partial C}{\partial w_j}$ 可得 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_xa^{L-1}_k (a^L_j-y_j)其向量形式是 \frac{\partial C}{\partial w^L} = \frac{1}{n} \sum_x a_j^{L-1}(\sigma(z^L)-y)对偏置用同样的方法可得 \frac{\partial C}{\partial b^L_{j}} = \frac{1}{n} \sum_x (a^L_j-y_j)交叉熵的含义和来源我们对于交叉熵的讨论聚焦在代数分析和代码实现。这虽然很有用，但是也留下了一个未能回答的更加宽泛的概念上的问题，如：交叉熵究竟表示什么？存在一些直觉上的思考交叉熵的方法吗？我们如何想到这个概念？ 让我们从最后一个问题开始回答：什么能够激发我们想到交叉熵？假设我们发现学习速度下降了，并理解其原因是因为对于二次代价函数,输出层的权重的偏导数为 \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k (a^L_j-y_j) \sigma'(z^L_j)项 $\sigma’(z^L_j)$ 会在一个输出神经元困在错误值时导致学习速度的下降。在研究了这些公式后，我们可能就会想到选择一个不包含 $\sigma’(z)$ 的代价函数。所以，这时候对一个训练样本 $x$，其代价 $C = C_x$ 满足： \frac{\partial C}{\partial w_j} = a_j^{L-1}(a^L_j-y) \frac{\partial C}{\partial b } = (a-y)如果我们选择的损失函数满足这些条件，那么它们就能以简单的方式呈现这样的特性：初始误差越大，神经元学习得越快。这也能够解决学习速度下降的问题。实际上，从这些公式开始，现在我们就看看凭着我们数学的直觉推导出交叉熵的形式是可行的。我们来推一下，由链式法则，我们有 \frac{\partial C}{\partial b} = \frac{\partial C}{\partial a} \sigma'(z)使用 $\sigma’(z) = \sigma(z)(1-\sigma(z)) = a(1-a)$，上个等式就变成 \frac{\partial C}{\partial b} = \frac{\partial C}{\partial a}a(1-a)对比等式，我们有 \frac{\partial C}{\partial a} = \frac{a-y}{a(1-a)}对此方程关于 $a$ 进行积分，得到 C = -[y \ln a + (1-y) \ln (1-a)]+ {\rm constant}其中 constant 是积分常量。这是一个单独的训练样本 $x$ 对损失函数的贡献。为了得到整个的损失函数，我们需要对所有的训练样本进行平均，得到了 C = -\frac{1}{n} \sum_x [y \ln a +(1-y) \ln(1-a)] + {\rm constant}而这里的常量就是所有单独的常量的平均。所以我们看到方程 \frac{\partial C}{\partial w_j} = a_j^{L-1}(a^L_j-y) \frac{\partial C}{\partial b } = (a-y)唯一确定了交叉熵的形式，并加上了一个常量的项。这个交叉熵并不是凭空产生的。而是一种我们以自然和简单的方法获得的结果。 那么交叉熵直觉含义又是什么？我们如何看待它？深入解释这一点会将我们带到一个不大愿意讨论的领域。然而，还是值得提一下，有一种源自信息论的解释交叉熵的标准方式。粗略地说，交叉熵是“不确定性”的一种度量。特别地，我们的神经元想要计算函数 $x \rightarrow y = y(x)$。但是，它用函数$x\rightarrow a = a(x)$ 进行了替换。假设我们将 $a$ 想象成我们神经元估计为 $y = 1$ 的概率，而 $1-a$ 则是 $y=0$ 的概率。那么交叉熵衡量我们学习到 $y$ 的正确值的平均起来的不确定性。 如果输出我们期望的结果，不确定性就会小一点；反之，不确定性就大一些。当然，我这里没有严格地给出“不确定性”到底意味着什么，所以看起来像在夸夸其谈。但是实际上，在信息论中有一种准确的方式来定义不确定性究竟是什么。详细内容请看交叉熵（cross-entropy）的数学历史。 交叉熵（cross-entropy）的数学历史通用的说，熵(Entropy)被用于描述一个系统中的不确定性(the uncertainty of a system)。在不同领域熵有不同的解释，比如热力学的定义和信息论也不大相同。 先给出一个”不严谨”的概念表述： 熵：可以表示一个事件A的自信息量，也就是A包含多少信息。 KL散度：可以用来表示从事件A的角度来看，事件B有多大不同。 交叉熵：可以用来表示从事件A的角度来看，如何描述事件B。 一句话总结的话：KL散度可以被用于计算代价，而在特定情况下最小化KL散度等价于最小化交叉熵。而交叉熵的运算更简单，所以用交叉熵来当做代价。 信息量 首先是信息量。假设我们听到了两件事，分别如下：事件A：巴西队进入了2018世界杯决赛圈。事件B：中国队进入了2018世界杯决赛圈。仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。 假设 $X$ 是一个离散型随机变量，其取值集合为 $\chi$ ,概率分布函数 $p(x)=Pr(X=x),x\in\chi$ ,则定义事件 $X=x_0$ 的信息量为： I(x_0)=-log(p(x_0))由于是概率所以$p(x_0)$的取值范围是[0,1], 绘制为图形如下： 什么是熵(Entropy)？ 放在信息论的语境里面来说，就是一个事件所包含的信息量。我们现在有了信息量的定义，而熵用来表示所有信息量的期望，即： 因此熵被定义为 $S(x)=-\sum_{i}P(x_{i})log_{b}P(x_{i})$ 如何衡量两个事件/分布之间的不同：KL散度 我们上面说的是对于一个随机变量x的事件A的自信息量，如果我们有另一个独立的随机变量x相关的事件B，该怎么计算它们之间的区别？ 此处我们介绍默认的计算方法：KL散度，有时候也叫KL距离，一般被用于计算两个分布之间的不同。看名字似乎跟计算两个点之间的距离也很像，但实则不然，因为KL散度不具备有对称性。在距离上的对称性指的是A到B的距离等于B到A的距离。 KL散度的数学定义： 相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异 维基百科对相对熵的定义 In the context of machine learning, DKL(P‖Q) is often called the information gain achieved if P is used instead of Q. 对于离散事件我们可以定义事件A和B的差别为： D_{KL}(A||B) = \sum_{i}P_{A}(x_i) log\bigg(\frac{P_{A}(x_i)}{P_{B}(x_i)} \bigg) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- P_{A}(x_i)log(P_{B}(x_i))对于连续事件，那么我们只是把求和改为求积分而已。 D_{KL}(A||B) = \int a(x) log\bigg(\frac{a(x)}{b(x)} \bigg)从公式中可以看出： 如果 $P_A=P_B$，即两个事件分布完全相同，那么KL散度等于0。 观察公式，可以发现减号左边的就是事件A的熵，请记住这个发现。 如果颠倒一下顺序求 $D_{KL}(B||A)$，那么就需要使用B的熵，答案就不一样了。所以KL散度来计算两个分布A与B的时候是不是对称的，有“坐标系”的问题**，$D_{KL}(A||B)\ne D_{KL}(B||A)$ 换句话说，KL散度由A自己的熵与B在A上的期望共同决定。当使用KL散度来衡量两个事件(连续或离散)，上面的公式意义就是求 A与B之间的对数差 在 A上的期望值。 KL散度 = 交叉熵 - 熵？ 如果我们默认了用KL散度来计算两个分布间的不同，那还要交叉熵做什么？ 事实上交叉熵和KL散度的公式非常相近，其实就是KL散度的后半部分(公式2.1)：A和B的交叉熵 = A与B的KL散度 - A的熵。 $D_{KL}(A||B) = -S(A)+H(A,B) $ 对比一下这是KL散度的公式： $D_{KL}(A||B) = \sum_{i}P_{A}(x_i) log\bigg(\frac{P_{A}(x_i)}{P_{B}(x_i)} \bigg) = \sum_{i}P_{A}(x_i)log(P_{A}(x_i ))- P_{A}(x_i)log(P_{B}(x_i)) $ 这是熵的公式： $S(A) = -\sum_{i}P_A(x_{i})logP_A(x_{i})$ 这是交叉熵公式： $H(A,B)= -\sum_{i}P_{A}(x_i)log(P_{B}(x_i)) $ 此处最重要的观察是，如果 $S(A)$是一个常量，那么$D_{KL}(A||B) = H(A,B) $ ，也就是说KL散度和交叉熵在特定条件下等价。 为什么交叉熵可以用作代价？ 接着上一点说，最小化模型分布 $P(model)$ 与 训练数据上的分布 $P(training)$ 的差异 等价于 最小化这两个分布间的KL散度，也就是最小化 $KL(P(training)||P(model))$。 比照第四部分的公式： 此处的A就是数据的真实分布： $P(training)$ 此处的B就是模型从训练数据上学到的分布： $P(model)$ 巧的是，训练数据的分布A是给定的。那么根据我们在第四部分说的，因为A固定不变，那么求 $D_{KL}(A||B)$等价于求 $H(A,B)$ ，也就是A与B的交叉熵。得证，交叉熵可以用于计算“学习模型的分布”与“训练数据分布”之间的不同。当交叉熵最低时(等于训练数据分布的熵)，我们学到了“最好的模型”。 但是，完美的学到了训练数据分布往往意味着过拟合，因为训练数据不等于真实数据，我们只是假设它们是相似的，而一般还要假设存在一个高斯分布的误差，是模型的泛化误差下线。 因此在评价机器学习模型时，我们往往不能只看训练数据上的误分率和交叉熵，还是要关注测试数据上的表现。如果在测试集上的表现也不错，才能保证这不是一个过拟合或者欠拟合的模型。交叉熵比照误分率还有更多的优势，因为它可以和很多概率模型完美的结合。 所以逻辑思路是，为了让学到的模型分布更贴近真实数据分布，我们最小化 模型数据分布 与 训练数据之间的KL散度，而因为训练数据的分布是固定的，因此最小化KL散度等价于最小化交叉熵。 因为等价，而且交叉熵更简单更好计算，当然用它。 参考文献[1] Michael Nielsen.CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-21. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL].https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-21. [3] 微调. 为什么交叉熵（cross-entropy）可以用于计算代价？[DB/OL]. https://www.zhihu.com/question/65288314/answer/244557337. 2018-06-21. [4] 史丹利复合田. 一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉[DB/OL]. https://blog.csdn.net/tsyccnh/article/details/79163834. 2018-06-22.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 3 Improving the way neural networks learn</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>交叉熵</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反向传播算法]]></title>
    <url>%2F2018%2F06%2F21%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 2 How the backpropagation algorithm works 反向传播概述反向传播算法最初在 1970 年代被提及，但是人们直到 David Rumelhart、Geoffrey Hinton 和 Ronald Williams 的著名的 1986 年的论文中才认识到这个算法的重要性。 反向传播的核心是一个对代价函数 $C$ 关于任何权重 $w$ 和 偏置 $b$ 的偏导数 $\partial{C}/\partial{w}$ 的表达式。 这个表达式告诉我们在改变权重和偏置时，代价函数变化的快慢。 关于代价函数的两个假设 代价函数可以被写成在每一个训练样本 $x$ 上的代价函数 $C_x$ 的均值 $C=\frac{1}{n}\sum_{x}C_x$。 代价函数可以写成神经网络输出的函数。 需要假设1的原因是，反向传播实际上是对一个独立的训练样本计算了 $\partial{C_x}/\partial{w}$ 和 $\partial{C_x}/\partial{b}$。然后通过在所有训练样本上进行平均化获得 $\partial{C}/\partial{w}$ 和 $\partial{C}/\partial{w}$ 。 需要假设2的原因是，要把代价函数与神经网络输出联系起来，进而与神经网络的参数联系起来。 符号定义 $W_{jk}^{l}$ 是从 $l-1$ 层的第 $k$ 个神经元到 $l$ 层的第 $j$ 个神经元的权重。 $b_j^l$ 是第 $l$ 层的第 $j$ 个神经元的偏置。 $a_j^l$ 是第 $l$ 层的第 $j$ 个神经元的激活值。 $\sigma$ 是激活函数。 把上面的符号向量化 $W^{l}$ 是权重矩阵，第 $j$ 行 $k$ 列的元素是 $W_{jk}^{l}$。 例如第二层与第三层之间的权重矩阵是 w^3 = { \left[ \begin{matrix} w_{11}^3 & w_{12}^3 & w_{13}^3 & w_{14}^3\\ w_{21}^3 & w_{22}^3 & w_{23}^3 & w_{24}^3 \end{matrix} \right] } $b^l$ 是偏置向量。第 $j$ 行的元素是 $b_j^l$。 例如第二层的偏置向量是 b^2 = { \left[ \begin{matrix} b_{1}^2 \\ \\ b_{2}^2\\ \\ b_{3}^2\\ \\ b_{4}^2 \end{matrix} \right] }有了这些表示 $l$ 层的第 $j$ 个神经元的激活值 $a_j^l$ 就和 $l-1$ 层的激活值通过方程关联起来了 a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right)把上面式子向量化 a^{l} = \sigma(w^l a^{l-1}+b^l)例如第三层的激活向量是 { \left[ \begin{matrix} a_{1}^3 \\ \\ a_{2}^3 \\ \end{matrix} \right] } =\sigma\left( { \left[ \begin{matrix} w_{11}^3 & w_{12}^3 & w_{13}^3 & w_{14}^3\\ w_{21}^3 & w_{22}^3 & w_{23}^3 & w_{24}^3 \end{matrix} \right] } { \left[ \begin{matrix} a_{1}^2 \\ \\ a_{2}^2 \\ \\ a_{3}^2 \\ \\ a_{4}^2 \\ \end{matrix} \right] }+ { \left[ \begin{matrix} b_{1}^3 \\ \\ b_{2}^3 \\ \end{matrix} \right] }\right) $a^{l}$ 是激活向量。第 $j$ 行的元素是 $a_j^l$。 定义 z^l \equiv w^l a^{l-1}+b^l则 $a^l =\sigma(z^l)$ $z^l$ 表示第第 $l$ 层的带权输入。第 $j$ 个元素是 $z_j^l$。 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j $z_j^l$ 是第 $l$ 层的第 $j$ 个神经元的带权输入。 反向传播的核心是一个对代价函数 $C$ 关于任何权重 $w$ 和 偏置 $b$ 的偏导数 $\partial{C}/\partial{w}$ 的表达式。为了计算这些值，引入一个中间量 $\delta_j^l$ ,表示在 $l$ 层的第 $j$ 个神经元的误差。 定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.$\delta^l$ 是误差向量，$\delta^l$ 的第 $j$ 个元素是 $\delta_j^l$。 反向传播的四个基本方程 $\nabla_a$ 是求梯度运算符，$\nabla_a C$ 结果是一个向量，其元素是偏导数 $\partial C / \partial a^L_j$。 $\odot$ 是按元素乘积的运算符，$ {(s \odot t)}_j = s_j t_j $ ，例如 \left[\begin{array}{c} 1 \\ 2 \end{array}\right] \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right] = \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right] = \left[ \begin{array}{c} 3 \\ 8 \end{array} \right]. 计算公式 维度变化 $\delta^L =\nabla_a C\odot \dfrac{\partial f^{(L)}}{\partial z^L} $ $(d^L, 1) = (d^L, 1) * (d^L, 1)$ $\delta^{(l)} =({(w^{(l+1)})}^T\delta^{(l+1)})\odot \sigma’(z^l) $ $(d^l, 1) = {({d}^{l+1}, {d}^{l})}^T (d^{l+1}, 1) * (d^l, 1)$ $\dfrac{\partial C}{\partial b^{(l)}} =\delta^{(l)}$ $(d^l, 1) = (d^l, 1)$ $\dfrac{\partial C}{\partial w^l} = \delta^l (a^{l-1})^T $ $(d^l, d^{l-1}) = (d^l,1) {(d^{l-1}, 1)}^T$ 反向传播算法 正如我们上面所讲的，反向传播算法对一个训练样本计算代价函数的梯度，$C=C_x$。在实践中，通常将反向传播算法和诸如随机梯度下降这样的学习算法进行组合使用，我们会对许多训练样本计算对应的梯度。特别地，给定一个大小为 m 的小批量数据，下面的算法在这个小批量数据的基础上应用梯度下降学习算法： 反向传播算法与小批量随机梯度下降算法结合的一个示意代码，完整代码参看 network.py 12345678910111213141516171819202122232425262728293031323334def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in range(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) 12345678def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y)def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) 四个基本方程的证明 我们现在证明这四个基本的方程（BP）-（BP4）。所有的这些都是多元微积分的链式法则的推论。 证明$\delta^L = \nabla_a C \odot \sigma’(z^L)$从方程（BP1）开始，它给出了误差 $\delta^l$ 的表达式。根据定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.根据关于代价函数的两个假设2 “代价函数可以写成神经网络输出的函数”，应用链式法测可知可先对神经网络输出求偏导${\partial C}/{\partial a^L_k}$再对带权输出求偏导${\partial a^L_k}/{\partial z^L_j}$。 \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j},看起来上面式子很复杂，但是由于第 $k$ 个神经元的输出激活值 $a_k^l$ 只依赖于 当下标 $k=j$ 时第 $j$ 个神经元的输入权重 $z_j^l$。所有当 $k\neq {j}$ 时 $\partial a^L_k / \partial z^L_j$ 消失了。结果我们可以简化上一个式子为 \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.又因为 $a^L_j = \sigma(z^L_j)$ 所以 $\frac{\partial a^L_j}{\partial z^L_j}$ 可以写成 $\sigma’(z^L_j)$，方程变为 \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j)这就是分量形式的（BP1），再根据$\nabla_a$ 是求梯度运算符，$\nabla_a C$ 结果是一个向量，其元素是偏导数 $\partial C / \partial a^L_j$。方程可以写成向量形式 \delta^L ={\nabla_a {C}} \odot {\sigma'(z^L_j)}（BP1） 得到证明。 证明 $ \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^l)$证明（BP2），它个给出以下一层误差 $\delta^{l+1}$ 的形式表示误差 $\delta^l$。为此，要以 $\delta^l_j = \partial C / \partial z^l_j$的形式重写 $\delta^{l+1}_k = \partial C / \partial z^{l+1}_k$,$\delta^{l+1}$ 和 $\delta^l$ 通过 $z_k^{l+1}$ 和 $z_j^l$ 联系起来，应用链式法测 根据 $z_k^{l+1}$ 的定义有 z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k$z_k^{l+1}$ 对 $z_j^{l}$ 做偏微分，得到 \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j)注意虽然$z^{l+1}$ 和 $z^{l}$ 所在的两层神经元连接错综复杂，但两层之间任意一对神经元（同一层内不连接）只有一条连接，即 $z_k^{l+1}$ 和 $z_j^{l}$ 之间只通过 $w_{kj}^{l+1}$ 连接。所以$z_k^{l+1}$ 对 $z_j^{l}$ 做偏微分的结果很简单，只是 $ w^{l+1}_{kj} \sigma’(z^l_j)$。把这个结果带入 $\delta_j^l$ 中 \delta^l_j = \sum_k w^{l+1}_{kj} \delta^{l+1}_k \sigma'(z^l_j)这正是以分量形式写的(BP2)。 写成向量形式 \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)举例 { \left[ \begin{matrix} \delta_{1}^l \\ \\ \delta_{2}^l \\ \\ ... \\ \delta_{j}^l \end{matrix} \right] } = { \left[ \begin{matrix} w_{11}^{l+1} & w_{21}^{l+1} & w_{31}^{l+1} & ... &w_{k1}^{l+1} \\ \\ w_{12}^{l+1} & w_{22}^{l+1} & w_{32}^{l+1} & ... & w_{k2}^{l+1} \\ \\ ... \\ w_{j1}^{l+1} & w_{j2}^{l+1} & w_{j1}^{l+1} & ... & w_{kj}^{l+1} \end{matrix} \right] } { \left[ \begin{matrix} \delta_{1}^{l+1} \\ \\ \delta_{2}^{l+1} \\ \\ \delta_{3}^{l+1} \\ \\ ... \\ \delta_{k}^{l+1} \end{matrix} \right] } \odot { \left[ \begin{matrix} \sigma'(z_1^l) \\ \\ \sigma'(z_2^l) \\ \\ \sigma'(z_3^l) \\ \\ ... \\ \sigma'(z_k^l) \end{matrix} \right] }（BP2） 得到证明。 证明 $\frac{\partial C}{\partial b^l_j} =\delta^l_j.$根据 $z_j^l$ 定义 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j和 $\delta_j^l$ 定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.因此 \frac{\partial C}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j}\frac{\partial z^l_j}{\partial b^l_j}又因为 \frac{\partial z^l_j}{\partial b^l_j} = 1所以 \frac{\partial C}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j}\frac{\partial z^l_j}{\partial b^l_j} = \frac{\partial C}{\partial z^l_j}\cdot 1= \frac{\partial C}{\partial z^l_j}=\delta^l_j即 \frac{\partial C}{\partial b^l_j} =\delta^l_j写成向量形式 \frac{\partial C}{\partial b^l} =\delta^l（BP3） 得到证明。 证明 $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$根据 $z_j^l$ 定义 z^l_j=\sum_k w^l_{jk} a^{l-1}_k+b^l_j和 $\delta_j^l$ 定义 \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.又因为 \frac{\partial z^l_j}{\partial w^l_{jk}} = a_k^{l-1}所以 \frac{\partial C}{\partial w^l_{jk}} = \frac{\partial C}{\partial z^l_j}\frac{\partial z^l_j}{\partial w^l_{jk}} = \delta^l_j a_k^{l-1}把式子向量化 \frac{\partial C}{\partial w^l} = \delta^l (a^{l-1})^T举例 \frac{\partial C}{\partial w^l} = { \left[ \begin{matrix} \delta_{1}^l \\ \\ \delta_{2}^l \\ \\ ... \\ \delta_{j}^l \end{matrix} \right] }{ \left[ \begin{matrix} a_{1}^{l-1} & a_{2}^{l-1} & ... &a_{k}^{l-1} \end{matrix} \right] }（BP4） 得到证明。 一个直观的图： 到此关于反向传播的四个方程已经全部证明完毕。 其他学者反向传播四个方程的证明（他写的更简明扼要些）：CSDN: oio328Loio 矩阵形式反向传播算法正如我们上面所讲的，反向传播算法对一个训练样本计算代价函数的梯度，$C=C_x$。在实践中，通常将反向传播算法和诸如随机梯度下降这样的学习算法进行组合使用，我们会对许多训练样本计算对应的梯度。特别地，给定一个大小为 m 的小批量数据，下面的算法在这个小批量数据的基础上应用梯度下降学习算法： 根据上面的小批量数据的符号定义，为了方便用矩阵表示，下面新增了一些符号。 $z^{v,l}$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的带权输入向量，用矩阵 $Z^l$ 来表示就是 Z^l = { \left[ \begin{matrix} {(z^{1,l})}^T \\ \\ {(z^{2,l})}^T\\ \\ ...\\ \\ {(z^{m,l})}^T \end{matrix} \right] }$a^{v,l}$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的激活向量，用矩阵 $A^l$ 来表示就是 A^l = { \left[ \begin{matrix} {(a^{1,l})}^T \\ \\ {(a^{2,l})}^T\\ \\ ...\\ \\ {(a^{m,l})}^T \end{matrix} \right] }$\delta^{v,l}$ 表示神经网络第 $l$ 层的小批量样本中的第 $v$ 个样本的误差向量，用矩阵 $\Delta^l$ 来表示就是 \delta^l = { \left[ \begin{matrix} {(\delta^{1,l})}^T \\ \\ {(\delta^{2,l})}^T\\ \\ ...\\ \\ {(\delta^{m,l})}^T \end{matrix} \right] } \delta^L =\nabla_a C\odot \frac{\partial f^{(L)}}{\partial z^L}的矩阵形式是， \Delta^L =\nabla_{A^L} C\odot \frac{\partial f^{(L)}}{\partial Z^L} \delta^{l}=({(w^{l+1})}^T\delta^{l+1})\odot \sigma'(z^l)的矩阵形式是， \Delta^{l}=(\Delta^{l+1}w^{l+1})\odot \sigma'(Z^l) \frac{\partial C}{\partial b^{(l)}}=\delta^{(l)}的矩阵形式是， \frac{\partial C}{\partial b^{(l)}}=\frac{1}{m}{(sum(\Delta^l, axis=0))}^T \frac{\partial C}{\partial w^l} = \delta^l (a^{l-1})^T的矩阵形式是， \frac{\partial C}{\partial w^{(l)}}=\frac{1}{m}{(\Delta^l)}^TA^{l-1}归纳一下，可以得到矩阵形式的反向传播算法： 计算公式 维度变化 $\Delta^L =\nabla_{A^L} C\odot \dfrac{\partial f^{(L)}}{\partial Z^L}$ $(m, d^L) = (m, d^L) * (m, d^L)$ $\Delta^{l} =(\Delta^{l+1}W^{l+1})\odot \sigma’(Z^l)$ $(m, {d}^l) = (m, {d}^{l+1}) ({d}^{l+1}, {d}^l)$ $\dfrac{\partial C}{\partial b^{(l)}} =\dfrac{1}{m}{(sum(\Delta^l, axis=0))}^T$ $(d^l, 1) = {(1, d^l)}^T$ $\dfrac{\partial C}{\partial w^{(l)}} =\dfrac{1}{m}{(\Delta^l)}^TA^{l-1} $ $(d^{l}, d^{l-1}) = {(m, d^l)}^T(m, d^{l-1})$ 反向传播：全局观 如上图所示，假设我们对 $w_{jk}^l$ 做一点微小的扰动 $\Delta w_{jk}^l$, 这个扰动会沿着神经网络最终影响到代价函数 $C$, 代价函数的 $\Delta C$ 改变和 $\Delta w_{jk}^l$ 按照下面公式联系起来 \Delta C \approx \frac{\partial C}{\partial w^l_{jk}} \Delta w^l_{jk}可以想象影响代价函数的一条路径是 \Delta C \approx \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}为了计算 $C$ 的全部改变，我们需要对所有可能的路径进行求和，即 \Delta C \approx \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}因为 \frac{\partial C}{\partial w^l_{jk}}=\frac{\Delta C}{\Delta w^l_{jk}}根据上面的三个式子可知 \frac{\partial C}{\partial w^l_{jk}} = \sum_{mnp\ldots q} \frac{\partial C}{\partial a^L_m} \frac{\partial a^L_m}{\partial a^{L-1}_n} \frac{\partial a^{L-1}_n}{\partial a^{L-2}_p} \ldots \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}}上面的公式看起来复杂，这里有一个相当好的直觉上的解释。我们用这个公式计算 $C$ 关于网络中一个权重的变化率。而这个公式告诉我们的是：两个神经元之间的连接其实是关联于一个变化率因子，这仅仅是一个神经元的激活值相对于其他神经元的激活值的偏导数。路径的变化率因子就是这条路径上众多因子的乘积。整个变化率 $\partial C / \partial w^l_{jk}$ 就是对于所有可能从初始权重到最终输出的代价函数的路径的变化率因子的和。针对某一路径，这个过程解释如下， 如果用矩阵运算对上面式子所有的情况求和，然后尽可能化简，最后你会发现，自己就是在做反向传播！可以将反向传播想象成一种计算所有可能路径变化率求和的方式。或者，换句话说，反向传播就是一种巧妙地追踪权重和偏置微小变化的传播，抵达输出层影响代价函数的技术。 如果你尝试用上面的思路来证明反向传播，会比本文的反向传播四个方程证明复杂许多，因为按上面的思路来证明有许多可以简化的地方。其中可以添加一个巧妙的步骤，上面方程的偏导对象是类似 $a_q^{l+1}$ 的激活值。巧妙之处是改用加权输入，例如 $z_q^{l+1}$ ，作为中间变量。如果没想到这个主意，而是继续使用激活值 $a_q^{l+1}$ ，你得到的证明最后会比前文给出的证明稍稍复杂些。 其实最早的证明的出现也不是太过神秘的事情。因为那只是对简化证明的艰辛工作的积累！ 参考文献[1] Michael Nielsen. CHAPTER 2 How the backpropagation algorithm works[DB/OL]. http://neuralnetworksanddeeplearning.com/chap2.html, 2018-06-21. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap2.tex, 2018-06-21. [3] oio328Loio. 神经网络学习（三）反向（BP）传播算法（1）[DB/OL]. https://blog.csdn.net/hoho1151191150/article/details/79537246, 2018-06-25.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 2 How the backpropagation algorithm works</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>反向传播算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贝叶斯估计、最大似然估计、最大后验估计三者的区别]]></title>
    <url>%2F2018%2F06%2F20%2F%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E3%80%81%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[实例分析即使学过机器学习的人，对机器学习中的 MLE(极大似然估计)、MAP(最大后验估计)以及贝叶斯估计(Bayesian) 仍有可能一知半解。对于一个基础模型，通常都可以从这三个角度去建模，比如对于逻辑回归（Logistics Regression）来说： MLE: Logistics Regression MAP: Regularized Logistics RegressionBayesian: Bayesian Logistic Regression 本文结合实际例子，以通俗易懂的方式去讲解这三者之间的本质区别，希望帮助读者扫清理解中的障碍。 先导知识点： 假设空间（Hypothesis Space） 什么叫假设空间呢？我们可以这样理解。机器学习包含很多种算法，比如线性回归、支持向量机、神经网络、决策树、GDBT等等。我们在建模的时候，第一步就是要选择一个特定的算法比如“支持向量机”。一旦选择了一个算法，就相当于我们选择了一个假设空间。在一个假设空间里，我们通常会有无数种不同的解（或者可以理解成模型），一个优化算法（比如梯度下降法）做的事情就是从中选择最好的一个解或者多个解/模型，当然优化过程要依赖于样本数据。举个例子，如果我们选择用支持向量机，那相当于我们可选的解/模型集中在上半部分（蓝色点）。 一个具体“toy”问题 “ 张三遇到了一个数学难题，想寻求别人帮助。通过一番思考之后发现自己的朋友在清华计算机系当老师。于是，他决定找清华计算机系学生帮忙。那张三用什么样的策略去寻求帮助呢？ 在这里，“清华计算机系”是一个假设空间。在这个假设空间里，每一位学生可以看做是一个模型（的实例化）。 对于张三来说，他有三种不同的策略可以选择。 第一种策略 : MLE 第一种策略就是从系里选出过往成绩最好的学生，并让他去解答这个难题。比如我们可以选择过去三次考试中成绩最优秀的学生。 一般的学习流程分为“学习过程”和“预测过程”。第一种策略的方案可以用下面的图来表示。在这里，学习过程相当于从所有系的学生中挑选出成绩最好的学生。所以，这里的“学生过往成绩单”就是我们已知的训练数据 D， 选出成绩最好的学生（计算历史平均分数，并选出最高的），这个过程就是MLE。一旦我们找到了成绩最好的学生，就可以进入预测环节。在预测环节中，我们就可以让他回答张三手里的难题 x’, 之后就可以得到他给出的解答 y’。 第二种策略：MAP 跟第一种策略的不同点在于，第二种策略中我们听取了老师的建议，老师就是张三的朋友。这位老师给出了自己的观点：_“小明和小花的成绩中可能存在一些水分”。_当我们按照成绩的高低给学生排序，假设前两名依次为小明和小花，如果我们不考虑这位老师的评价，则我们肯定把小明作为目标对象。然而，既然老师已经对小明和小花做了一些负面的评价，那这个时候，我们很有可能最后选择的是班级里的第三名，而不是小明或者小花。 我们把第二种策略的过程也用一个图来描述。与上面的图相比，唯一的区别在于这里多出了老师的评价，我们称之为 Prior。 也就是说我们根据学生以往的成绩并结合老师评价，选择了一位我们认为最优秀的学生（可以看成是模型）。之后就可以让他去回答张老师的难题 x’，并得到他的解答 y’。整个过程类似于MAP的估计以及预测。 到这里，有些读者可能会有一些疑惑：“_老师的评价(Prior)跟学生过往的成绩（Observation）是怎么结合在一起的？”_。 为了回答这个问题，我们不得不引出一个非常著名的定理，叫做贝叶斯定理，如下图所示。左边的项是MAP需要优化的部分，通过贝叶斯定理这个项可以分解成MLE（第一种策略）和Prior，也就是老师的评价。在这里，分母是常数项（Constant），所以不用考虑。 第三种策略 - Bayesian 最后，我们来介绍第三种策略。这种策略应该很多人也可以想象得到，其实就是让所有人都去参与回答张三的难题，但最后我们通过一些加权平均的方式获得最终的答案。比如有三个学生，而且我们对这三个学生情况没有任何了解。通过提问，第一个学生回答的答案是A，第二个学生回答的答案也是A，但第三个学生回答的是B。在这种情况下，我们基本可以把A作为标准答案。接着再考虑一个稍微复杂的情况，假设我们通过以往他们的表现得知第三个学生曾经多次获得过全国奥赛的金牌，那这个时候该怎么办？ 很显然，在这种情况下，我们给予第三个学生的话语权肯定要高于其他两位学生。 我们把上面的这种思路应用到张三的问题上，其实相当于我们让所有计算机系的学生参与回答这个问题，之后把他们的答案进行汇总并得出最终的答案。如果我们知道每一位学生的话语权（权重），这个汇总的过程是确定性（deterministic)。 但每一位学生的话语权（权重）怎么得到呢？ 这就是贝叶斯估计做的事情！ 我们用下面的一幅图来讲述贝叶斯估计和预测的整个过程。跟MAP类似，我们已知每一位学生过去三次考试考试成绩（D）以及老师的评价（Prior）。 但跟MAP不同的是，我们这里的目标不再是- “选出最优秀的学生”，而是通过观测数据（D）去获得每一位学生的发言权（权重），而且这些权重全部加起来要等于1， 相当于是一个valid分布(distribution)。 总结起来，在第三种策略之下，给定过去考试成绩(D)和老师的评价（Prior）, 我们的目标是估计学生权重的分布，也称之为Posterior Distribution。 那这个分布具体怎么去估计呢？ 这部分就是贝叶斯估计做的事情，有很多种方法可以做这件事情，比如MCMC, Variational Method等等，但这并不是本文章的重点，所以不在这里进一步解释，有兴趣的读者可以关注之后关于贝叶斯的专栏文章。从直观的角度思考，因为我们知道每一位学生过往的成绩，所以我们很容易了解到他们的能力水平进而估计出每一位学生的话语权（权重）。 一旦我们获得了这个分布（也就是每一位学生的权重），接下来就可以通过类似于加权平均的方式做预测了，那些权重高的学生话语权自然就越大。 以上是对MLE, MAP以及贝叶斯估计的基本讲解。下面我们试图去回答两个常见的问题。 Q: 随着我们观测到越来越多的数据，MAP估计逐步逼近MLE，这句话怎么理解？ 我们接着使用之前MAP（第二种策略）的例子。在这里，我们对原来的问题稍作改变。在之前的例子里我们假设能够得到每一位学生过去三次考试中的成绩。但在这里，我们进一步假定可以获得每一位学生过去100次考试中的成绩。 那这样的修改会带来什么样的变化呢？ 如果仔细想一想，其实也很容易想得到。我们设想一下这样的两种场景。假设我们知道某一位学生过去三次的考试成绩比较优异，但老师却告诉我们这位学生能力其实不怎么样，那这时候我们很可能就去相信老师了，毕竟仅仅通过三次考试的成绩很难对一个学生有全面的了解。但相反，假设我们了解到这位学生在过去100次考试中全部获得了班里第一名，但同时老师又告诉我们这位学生的能力其实不怎么样，那这时候我们会有什么样的反应？ 两三次考试或许可以算做是运气，但连续100次都是第一名这件事情很难再跟运气画等号吧？ 我们甚至可能会去怀疑老师的品德，是不是故意污蔑人家？ 这就是说，当我们观测到的数据越来越多的时候，我们从数据中获取的信息的置信度是越高的，相反老师提供的反馈（Prior）的重要性就会逐渐降低。理想情况下，当我们拥有无穷多的数据样本时，MAP会逼近MLE估计，道理都是一样的。 Q: 为什么贝叶斯估计会比MLE, MAP难？ 回顾一下，MLE 和MAP都是在寻找一个最优秀的学生。贝叶斯估计则是在估计每一位学生的权重。第一种情况下，为了寻找最优秀的学生，我们只需知道学生之间的“相对”优秀程度。这个怎么理解呢？ 比如一个班里有三个学生A,B,C，我们知道学生A比B优秀，同时知道B比C优秀，那这时候就可以推断出学生A是最优秀的，我们并不需要明确知道A的成绩是多少，B的成绩是多少….. 但在贝叶斯估计模式下，我们必须要知道每一个学生的绝对权重，因为最后我们获得的答案是所有学生给出的答案的加权平均，而且所有学生的权重加起来要保证等于1(任何一个分布的积分和必须要等于1）。 假设我们知道每一位学生的能力值，a1, a2,…. an，这个能作为权重吗？ 显然不能。为了获得权重，有一种最简单的方法就是先求和，然后再求权重。比如先计算 a1+…+an = S, 再用a1/S 作为权重。这貌似看起来也不难啊，只不过多做了一个加法操作？ 我们很容易看出这个加法操作的时间复杂度是O(n)，依赖于总体学生的数量。如果我们的假设空间只有几百名学生，这个是不成问题的。 但实际中，比如我们假设我们的模型用的是支持向量机，然后把假设空间里的每一个可行解比喻成学生，那这个假设空间里有多少个学生呢？ 是无数个！！， 也就是说需要对无穷多个数做这种加法操作。 当然，这种加法操作会以积分(integeral)的方式存在，但问题是这种积分通常没有一个closed-form的解，你必须要去近似地估计才可以，这就是MCMC或者Variational方法做的事情，不在这里多做解释。 本文几点重要的Take-aways： 每一个模型定义了一个假设空间，一般假设空间都包含无穷的可行解； MLE不考虑先验（prior)，MAP和贝叶斯估计则考虑先验（prior）； MLE、MAP是选择相对最好的一个模型（point estimation）， 贝叶斯方法则是通过观测数据来估计后验分布(posterior distribution)，并通过后验分布做群体决策，所以后者的目标并不是在去选择某一个最好的模型； 当样本个数无穷多的时候，MAP理论上会逼近MLE； 贝叶斯估计复杂度大，通常用MCMC等近似算法来近似； 最后贴一张总结的图: 理论分析一。机器学习 核心思想是从past experience中学习出规则，从而对新的事物进行预测。对于监督学习来说，有用的样本数目越多，训练越准确。 用下图来表示机器学习的过程及包含的知识： 简单来说就是： 首先要定义我们的假设空间（Model assumption）：如线性分类，线性回归，逻辑回归，SVM，深度学习网络等。 如何衡量我们学出来的模型的好坏？定义损失函数（目标函数），lost function，如square loss 如何对假设的模型做优化，及optimization过程。简单说，就是选择一种算法（如：梯度下降，牛顿法等），对目标函数进行优化，最终得到最优解； 不同的模型使用不同的算法，如逻辑回归通常用梯度下降法解决，神经网络用反向推导解决，贝叶斯模型则用MCMC来解决。 机器学习 = 模型 + 优化（不同算法） 还有一个问题，模型的复杂度怎么衡量？因为复杂的模型容易出现过拟合（overfitting）。解决过拟合的方就是加入正则项（regularization） 以上问题都解决之后，我们怎么判断这个解就是真的好的呢？用交叉验证（cross-validation）来验证一下。 二。ML vs MAP vs Bayesian ML（最大似然估计）：就是给定一个模型的参数，然后试着最大化p(D|参数)。即给定参数的情况下，看到样本集的概率。目标是找到使前面概率最大的参数。 逻辑回归都是基于ML做的； 缺点：不会把我们的先验知识加入模型中。 MAP（最大后验估计）：最大化p(参数|D)。 Bayesian：我们的预测是考虑了所有可能的参数，即所有的参数空间（参数的分布）。 ML和MAP都属于同一个范畴，称为（freqentist），最后的目标都是一样的：找到一个最优解，然后用最优解做预测。 三。ML 我们需要去最大化p(D|参数)，这部分优化我们通常可以用把导数设置为0的方式去得到。然而，ML估计不会把先验知识考虑进去，而且很容易造成过拟合现象。 举个例子，比如对癌症的估计，一个医生一天可能接到了100名患者，但最终被诊断出癌症的患者为5个人，在ML估计的模式下我们得到的得到癌症的概率为0.05。 这显然是不太切合实际的，因为我们根据已有的经验，我们知道这种概率会低很多。然而ML估计并没有把这种知识融入到模型里。 四。MAP 通过上面的推导我们可以发现，MAP与ML最大的不同在于p(参数)项，所以可以说MAP是正好可以解决ML缺乏先验知识的缺点，将先验知识加入后，优化损失函数。 其实p(参数)项正好起到了正则化的作用。如：如果假设p(参数)服从高斯分布，则相当于加了一个L2 norm；如果假设p(参数)服从拉普拉斯分布，则相当于加了一个L1 norm。 五。Bayesian 再次强调一下： ML和MAP只会给出一个最优的解， 然而贝叶斯模型会给出对参数的一个分布，比如对模型的参数, 假定参数空间里有参数1,参数2, 参数3,…参数N，贝叶斯模型学出来的就是这些参数的重要性（也就是分布），然后当我们对新的样本预测的时候，就会让所有的模型一起去预测，但每个模型会有自己的权重（权重就是学出来的分布）。最终的决策由所有的估计根据其权重做出决策。 模型的ensemble的却大的优点为它可以reduce variance, 这根投资很类似，比如我们投资多种不同类型的股票，总比投资某一个股票时的风险会低。 六。上面提到了frequentist和bayesian，两者之间的区别是什么？ 用一个简答的例子来再总结一下。 比如你是班里的班长，你有个问题想知道答案，你可以问所有的班里的学生。 一种方案是，问一个学习最好的同学。 另一种方案是，问所有的同学，然后把答案综合起来，但综合的时候，会按照每个同学的成绩好坏来做个权重。 第一种方案的思想类似于ML,MAP，第二种方案类似于贝叶斯模型。 七。Bayesian的难点 所以整个贝叶斯领域的核心技术就是要近似的计算 p(\\theta|D），我们称之为bayesian inference,说白了，这里的核心问题就是要近似这个复杂的积分（integral), 一种解决方案就是使用蒙特卡洛算法。比如我想计算一个公司所有员工的平均身高，这个时候最简答粗暴的方法就是让行政去一个一个去测量，然后计算平均值。但想计算所有中国人的平均身高，怎么做？（显然一个个测量是不可能的） 即采样。我们随机的选取一些人测量他们的身高，然后根据他们的身高来估计全国人民的审稿。当然采样的数目越多越准确，采样的数据越有代表性越准确。这就是蒙特卡洛算法的管家思想。 再例： 假设我们不知道π，但是想计算圆的面积。也可以通过采样的方法近似得到。随机再下图所示的正方形中撒入一些点，记落入红色区域的点的个数为n1,落入白色区域的个数为n2，则四分之一圆的面积就为n1/(n1+n2).——蒙特卡洛思想 那么，如何对连续函数估计呢？采样n多个数据，逼近最后的积分值。 假设我们要计算 f(x)的期望值， 我们也有p(x)这种分布，这个时候我们就可以不断的从p(x)这个分布里做一些采样，比如 x1,x2,…xn, 然后用这些采样的值去算f(x), 所以最后得到的结果就是 (f(x1) + f(x2),, + f(xn))/ n 然鹅，上面例子中提到的采样都是独立的。也就是每个样本跟其他的样本都是独立的，不影响彼此之间的采样。然而，在现实问题上，有些时候我们想加快有效样本的采样速度。这个问题讨论的就是怎么优化采样过程了，也是机器学习里一个比较大的话题了。 重申一下，用上面提到的采样方式我们可以去近似估计复杂的积分，也可以计算圆的面积，也可以计算全国人口的平均身高。但这个采样方式是独立的，有些时候，我们希望我们用更少的样本去更准确的近似某一个目标，所以就出现了sampling这种一个领域的研究，就是在研究以什么样的方式优化整个采样过程，使得过程更加高效。 MCMC这种采样方法，全称为markov chain monte carlo采样方法，就是每个采样的样本都是互相有关联性的。 但是MCMC算法需要在整个数据集上计算。也就是说为了得到一个样本，需要用所有的数据做迭代。这样当N很大时，显然不适用。而且限制了贝叶斯方法发展的主要原因就是计算复杂度太高。因此现在贝爷第领域人们最关心的问题是：怎么去优化采样，让它能够在大数据环境下学习出贝叶斯模型？ 降低迭代复杂度的一个实例： 对于逻辑回归，使用梯度下降法更新参数时，有批量梯度下降法（即使用整个数据集去更新参数），为了降低计算复杂度，人们使用了随机梯度下降法，即随机从数据集中选取样本来更新参数。 所以，能否将此思想用于MCMC采样中呢？ Yes！langevin dynamic（MCMC算法中的一种），和stochastic optimizaiton(比如随机梯度下降法)可以结合在一起用。这样，我们就可以通过少量的样本去做采样，这个时候采样的效率就不在依赖于N了，而是依赖于m, m是远远小于N。 参考文献[1] 贪心科技. 机器学习中的MLE、MAP和贝叶斯估计[DB/OL]. https://zhuanlan.zhihu.com/p/37543542, 2018-06-20. [2] 江湖小妞. 贝叶斯思想以及与最大似然估计、最大后验估计的区别[DB/OL].http://www.cnblogs.com/little-YTMM/p/5399532.html, 2018-06-20.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率论</tag>
        <tag>贝叶斯估计</tag>
        <tag>最大似然估计</tag>
        <tag>最大后验估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用神经网络识别手写数字——代码实现]]></title>
    <url>%2F2018%2F06%2F19%2F%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[部分代码解读在之前描述 MNIST 数据时，我说它分成了 60,000 个训练图像和 10,000 个测试图像。这是官方的 MNIST 的描述。实际上，我们将用稍微不同的方法对数据进行划分。我们将测试集保持原样，但是将 60,000 个图像的 MNIST 训练集分成两个部分：一部分 50,000 个图像，我们将用来训练我们的神经网络，和一个单独的 10,000 个图像的验证集。在本章中我们不使用验证数据，但是在本书的后面我们将会发现它对于解决如何去设置某些神经网络中的超参数是很有用的，例如学习率等，这些参数不被我们的学习算法直接选择。尽管验证数据不是原始 MNIST 规范的一部分，然而许多人以这种方式使用 MNIST，并且在神经网络中使用验证数据是很普遍的。从现在起当我提到“MNIST 训练数据”时，我指的是我们的 50,000 个图像数据集，而不是原始的 60,000图像数据集 除了 MNIST 数据，我们还需要一个叫做 Numpy 的 Python 库，用来做快速线性代数。如果你没有安装过 Numpy。在列出一个完整的代码清单之前，让我解释一下神经网络代码的核心特性。核心片段是一个 Network 类，我们用来表示一个神经网络。这是我们用来初始化一个 Network 对象的代码： 神经网络架构12345678class Network(object): def __init__(self, sizes): self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] 在这段代码中，列表 sizes 包含各层神经元的数量。例如，如果我们想创建一个在第一层有 2 个神经元，第二层有 3 个神经元，最后层有 1 个神经元的 Network 对象，我们应这样写代码： 1net = Network([2, 3, 1]) Network 对象中的权重和偏置都是被随机初始化的，使用 Numpy 的 np.random.randn! 函数来生成均值为 0，标准差为 1 的高斯分布。这样的随机初始化给了我们的随机梯度下降算法一个起点。 激活函数我们从定义 S型函数开始： 12def sigmoid(z): return 1.0/(1.0+np.exp(-z)) 前馈神经网络然后对 Network 类添加一个 feedforward 方法，对于网络给定一个输入 $a$，返回对应的输出,这个方法所做的是对每一层应用方程。 12345def feedforward(self, a): """Return the output of the network if "a" is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a 随机梯度下降算法当然，我们想要 Network 对象做的主要事情是学习。为此我们给它们一个实现随机梯度下降算法的 SGD 方法。代码如下。其中一些地方看似有一点神秘，我会在代码后面逐个分析。 123456789101112131415161718192021222324def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """Train the neural network using mini-batch stochastic gradient descent. The "training_data" is a list of tuples "(x, y)" representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If "test_data" is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.""" if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print "Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format( j, self.evaluate(test_data), n_test) else: print "Epoch &#123;0&#125; complete".format(j) training_data 是一个 (x, y) 元组的列表，表示训练输入和其对应的期望输出。变量 epochs 和 mini_batch_size 正如你预料的迭代期数量，和采样时的小批量数据的大小。eta 是学习率，$\eta$。如果给出了可选参数 test_data，那么程序会在每个训练器后评估网络，并打印出部分进展。这对于追踪进度很有用，但相当拖慢执行速度。 代码如下工作。在每个周期，它首先随机地将训练数据打乱，然后将它分成多个适当大小的小批量数据。这是一个简单的从训练数据的随机采样方法。然后对于每一个小批量数据我们应用一次梯度下降。这是通过代码 self.update_mini_batch(mini_batch, eta) 完成的，它仅仅使用 mini_batch 中的训练数据，根据单次梯度下降的迭代更新网络的权重和偏置。这是 update_mini_batch 方法的代码： 123456789101112131415def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The "mini_batch" is a list of tuples "(x, y)", and "eta" is the learning rate.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] 大部分工作由这行代码完成： 1delta_nabla_b, delta_nabla_w = self.backprop(x, y) 这行调用了一个称为反向传播的算法，一种快速计算代价函数的梯度的方法。因此 update_mini_batch 的工作仅仅是对 mini_batch 中的每一个训练样本计算梯度，然后适当地更新 self.weights 和 self.biases。 我现在不会列出 self.backprop 的代码。我们将在下章中学习反向传播是怎样工作的，包括 self.backprop 的代码。现在，就假设它按照我们要求的工作，返回与训练样本 $x$ 相关代价的适当梯度。 Network.py 完整代码让我们看一下完整的程序，包括我之前忽略的文档注释。除了 self.backprop，程序已经有了足够的文档注释，所有的繁重工作由 self.SGD 和 self.update_mini_batch 完成，对此我们已经有讨论过。self.backprop 方法利用一些额外的函数来帮助计算梯度，即 sigmoid_prime，它计算 $\sigma$ 函数的导数，以及 self.cost_derivative，这里我不会对它过多描述。你能够通过查看代码或文档注释来获得这些的要点（或者细节）。我们将在下章详细地看它们。注意，虽然程序显得很长，但是很多代码是用来使代码更容易理解的文档注释。实际上，程序只包含 74 行非空、非注释的代码。所有的代码可以在 GitHub 上找到。 注意作者的代码是 Python2 版本的，下面替换成 Python3 版本的（仅仅针对不同 Python 版本语法进行了代码修改，原理没有任何修改）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142"""network.py~~~~~~~~~~A module to implement the stochastic gradient descent learningalgorithm for a feedforward neural network. Gradients are calculatedusing backpropagation. Note that I have focused on making the codesimple, easily readable, and easily modifiable. It is not optimized,and omits many desirable features."""#### Libraries# Standard libraryimport random# Third-party librariesimport numpy as npclass Network(object): def __init__(self, sizes): """The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won't set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.""" self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): """Return the output of the network if ``a`` is input.""" for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): """Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ``test_data`` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.""" test_data = list(test_data) training_data = list(training_data) if test_data: n_test = len(test_data) n = len(training_data) for j in range(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print("Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;".format( j, self.evaluate(test_data), n_test)) else: print("Epoch &#123;0&#125; complete".format(j)) def update_mini_batch(self, mini_batch, eta): """Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): """Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.""" nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforwar activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It's a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in range(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): """Return the number of test inputs for which the neural network outputs the correct result. Note that the neural network's output is assumed to be the index of whichever neuron in the final layer has the highest activation.""" test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): """Return the vector of partial derivatives \partial C_x / \partial a for the output activations.""" return (output_activations-y)#### Miscellaneous functionsdef sigmoid(z): """The sigmoid function.""" return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): """Derivative of the sigmoid function.""" return sigmoid(z)*(1-sigmoid(z)) 运行结果这个程序对识别手写数字效果如何？好吧，让我们先加载 MNIST 数据。我将用下面所描述的一小段辅助程序 mnist_loader.py 来完成。我们在一个 Python shell 中执行下面的命令， 123&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper() 在加载完 MNIST 数据之后，我们将设置一个有 30 个隐藏层神经元的 Network。我们在导入如上所列的名为 network 的 Python 程序后做， 12&gt;&gt;&gt; import network&gt;&gt;&gt; net = network.Network([784, 30, 10]) 最后，我们将使用随机梯度下降来从 MNIST training_data 学习超过 30 次 epoch，mini-batch 大小为 10，学习率 $\eta = 3.0$， 1&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data) 打印内容显示了在每轮训练期后神经网络能正确识别测试图像的数量。正如你所见到，在仅仅一次 epoch后，达到了 10,000 中选中的 9,129 个。而且数目还在持续增长， 1234567Epoch 0: 9129 / 10000Epoch 1: 9295 / 10000Epoch 2: 9348 / 10000...Epoch 27: 9528 / 10000Epoch 28: 9542 / 10000Epoch 29: 9534 / 10000 更确切地说，经过训练的网络给出的识别率约为 95% 在峰值时为 95.42%（“Epoch 28”）作为第一次尝试，这是非常令人鼓舞的。然而我应该提醒你，如果你运行代码然后得到的结果和我的不完全一样，那是因为我们使用了（不同的）随机权重和偏置来初始化我们的网络。 让我们重新运行上面的实验，将隐藏神经元数量改到 100。 12&gt;&gt;&gt; net = network.Network([784, 100, 10])&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data) 果然，它将结果提升至 96.59%。至少在这种情况下，使用更多的隐藏神经元帮助我们得到了更好的结果。（注意，有的反馈表明本实验在结果上有相当多的变化，而且一些训练运行给出的结果相当糟糕。使用第三章所介绍的技术将大大减少我们网络上这些不同训练运行性能的差别。） 当然，为了获得这些准确性，我不得不对训练的迭代期数量，mini-batch 大小和 学习率 $\eta$ 做特别的选择。正如我上面所提到的，这些在我们的神经网络中被称为超参数，以区别于通过我们的学习算法所学到的参数权重和偏置。如果我们选择了糟糕的\超参数，我们会得到较差的结果。假如我们定 学习率 为 $\eta = 0.001$, 结果则不太令人鼓舞了， 1234567Epoch 0: 1139 / 10000Epoch 1: 1136 / 10000Epoch 2: 1135 / 10000...Epoch 27: 2101 / 10000Epoch 28: 2123 / 10000Epoch 29: 2142 / 10000 然而，你可以看到网络的性能随着时间的推移慢慢地变好了。这表明应该增大 学习率，例如 $\eta = 0.01$。如果我们那样做了，我们会得到更好的结果，这表明我们应该再次增加\gls*{学习率}。（如果改变能够改善一些事情，试着做更多！）如果我们这样做几次，我们最终会得到一个像 $\eta = 1.0$ 的 学习率（或者调整到$3.0$），这跟我们之前的实验很接近。因此即使我们最初选择了糟糕的超参数，我们至少获得了足够的信息来帮助我们改善对于超参数的选择。通常，调试一个神经网络是具有挑战性的。 从这得到的教训是调试一个神经网络不是琐碎的，就像常规编程那样，它是一门艺术。你需要学习调试的艺术来获得神经网络更好的结果。更普通的是，我们需要启发式方法来选择好的超参数和好的结构。我们将在整本书中讨论这些，包括上面我是怎么样选择超参数的。 补充：加载训练数据的代码前文中，我跳过了如何加载 MNIST 数据的细节。这很简单。这里列出了完整的代码。用于存储 MNIST 数据的数据结构在文档注释中有详细描述，都是简单的类型，元组和 Numpy ndarry 对象的列表（如果你不熟悉 ndarray，那就把它们看成向量）： 原来的 mnist_loader 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677"""mnist_loader~~~~~~~~~~~~A library to load the MNIST image data. For details of the datastructures that are returned, see the doc strings for ``load_data``and ``load_data_wrapper``. In practice, ``load_data_wrapper`` is thefunction usually called by our neural network code."""#### Libraries# Standard libraryimport pickleimport gzip# Third-party librariesimport numpy as npdef load_data(): """Return the MNIST data as a tuple containing the training data, the validation data, and the test data. The ``training_data`` is returned as a tuple with two entries. The first entry contains the actual training images. This is a numpy ndarray with 50,000 entries. Each entry is, in turn, a numpy ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image. The second entry in the ``training_data`` tuple is a numpy ndarray containing 50,000 entries. Those entries are just the digit values (0...9) for the corresponding images contained in the first entry of the tuple. The ``validation_data`` and ``test_data`` are similar, except each contains only 10,000 images. This is a nice data format, but for use in neural networks it's helpful to modify the format of the ``training_data`` a little. That's done in the wrapper function ``load_data_wrapper()``, see below. """ f = gzip.open('../data/mnist.pkl.gz', 'rb') training_data, validation_data, test_data = pickle.load(f, encoding='bytes') f.close() return (training_data, validation_data, test_data)def load_data_wrapper(): """Return a tuple containing ``(training_data, validation_data, test_data)``. Based on ``load_data``, but the format is more convenient for use in our implementation of neural networks. In particular, ``training_data`` is a list containing 50,000 2-tuples ``(x, y)``. ``x`` is a 784-dimensional numpy.ndarray containing the input image. ``y`` is a 10-dimensional numpy.ndarray representing the unit vector corresponding to the correct digit for ``x``. ``validation_data`` and ``test_data`` are lists containing 10,000 2-tuples ``(x, y)``. In each case, ``x`` is a 784-dimensional numpy.ndarry containing the input image, and ``y`` is the corresponding classification, i.e., the digit values (integers) corresponding to ``x``. Obviously, this means we're using slightly different formats for the training data and the validation / test data. These formats turn out to be the most convenient for use in our neural network code.""" tr_d, va_d, te_d = load_data() training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] training_results = [vectorized_result(y) for y in tr_d[1]] training_data = list(zip(training_inputs, training_results)) validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] validation_data = list(zip(validation_inputs, va_d[1])) test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] test_data = list(zip(test_inputs, te_d[1])) return (training_data, validation_data, test_data)def vectorized_result(j): """Return a 10-dimensional unit vector with a 1.0 in the jth position and zeroes elsewhere. This is used to convert a digit (0...9) into a corresponding desired output from the neural network.""" e = np.zeros((10, 1)) e[j] = 1.0 return e 怎么做到识别手写数字的？ 迈向深度学习虽然我们的神经网络给出了令人印象深刻的表现，但这样的表现带有几分神秘。网络中的权重和偏置是被自动发现的。这意味着我们不能立即解释网络怎么做的、做了什么。我们能否找到一些方法来理解我们的网络通过什么原理分类手写数字？并且，在知道了这些原理后，我们能做得更好吗？ 为了让这些问题更具体，我们假设数十年后神经网络引发了人工智能（AI）。到那个时候，我们能明白这种智能网络的工作机制吗？或许，因为有着自动学习得到的权重和偏置，这些是我们无法理解的，这样的神经网络对我们来说是不透明的。在人工智能的早期研究阶段，人们希望在构建人工智能的努力过程中，也同时能够帮助我们理解智能背后的机制，以及人类大脑的运转方式。但结果可能是我们既不能够理解大脑的机制，也不能够理解人工智能的机制。 为解决这些问题，让我们重新思考一下我在本章开始时所给的人工神经元的解释，作为一种衡量证据的方法。假设我们要确定一幅图像是否显示有人脸： 我们可以用解决手写识别问题的相同方式来攻克这个问题。网络的输入是图像中的像素，网络的输出是一个单个的神经元用于表明“是的，这是一张脸”或“不，这不是一张脸”。 假设我们就采取了这个方法，但接下来我们先不去使用一个学习算法。而是去尝试亲手设计一个网络，并为它选择合适的权重和偏置。我们要怎样做呢？暂时先忘掉神经网络，我们受到启发的一个想法是将这个问题分解成子问题：图像的左上角有一个眼睛吗？右上角有一个眼睛吗？中间有一个鼻子吗？下面中央有一个嘴吗？上面有头发吗？诸如此类。 如果一些问题的回答是“是”，或者甚至仅仅是“可能是”，那么我们可以作出结论这个图像可能是一张脸。相反地，如果大多数这些问题的答案是“不是”，那么这张图像可能不是一张脸。 当然，这仅仅是一个粗略的想法，而且它存在许多缺陷。也许有个人是秃头，没有头发。也许我们仅仅能看到脸的部分，或者这张脸是有角度的，因此一些面部特征是模糊的。不过这个想法表明了如果我们能够使用神经网络来解决这些子问题，那么我们也许可以通过将这些解决子问题的网络结合起来，构成一个人脸检测的神经网络。下图是一个可能的结构，其中的方框表示子网络。注意，这不是一个人脸检测问题的现实的解决方法，而是为了帮助我们构建起网络如何运转的直观感受。下图是这个网络的结构： 子网络也可以被继续分解，这看上去很合理。假设我们考虑这个问题：“左上角有一个眼睛吗？”。 这个问题可以被分解成这些子问题：“有一个眉毛吗？”，“有睫毛吗？”，“有虹膜吗？”，等等。当然这些问题也应该包含关于位置的信息，诸如“在左上角有眉毛，上面有虹膜吗？”，但是让我们先保持简单。回答问题“左上角有一个眼睛吗？”的网络能够被分解成： 这些子问题也同样可以继续被分解，并通过多个网络层传递得越来越远。最终，我们的子网络可以回答那些只包含若干个像素点的简单问题。举例来说，这些简单的问题可能是询问图像中的几个像素是否构成非常简单的形状。这些问题就可以被那些与图像中原始像素点相连的单个神经元所回答。 最终的结果是，我们设计出了一个网络，它将一个非常复杂的问题，这张图像是否有一张人脸，分解成在单像素层面上就可回答的非常简单的问题。它通过一系列多层结构来完成，在前面的网络层，它回答关于输入图像非常简单明确的问题，在后面的网络层，它建立了一个更加复杂和抽象的层级结构。包含这种多层结构，两层或更多隐藏层的网络被称为深度神经网络。 当然，我没有提到如何去递归地分解成子网络。手工设计网络中的权重和偏置无疑是不切实际的。取而代之的是，我们希望使用学习算法来让网络能够自动从训练数据中学习权重和偏置。这样，形成一个概念的层次结构。80年代和 90年代的研究人员尝试了使用随机梯度下降和反向传播来训练深度网络。不幸的是，除了一些特殊的结构，他们并没有取得很好的效果。虽然网络能够学习，但是学习速度非常缓慢，不适合在实际中使用。 自 2006 年以来，人们已经开发了一系列技术使深度神经网络能够学习。这些深度学习技术基于随机梯度下降和反向传播，并引进了新的想法。这些技术已经使更深（更大）的网络能够被训练~——~现在训练一个有 5 到 10 层隐藏层的网络都是很常见的。而且事实证明，在许多问题上，它们比那些浅层神经网络，例如仅有一个隐藏层的网络，表现的更加出色。当然，原因是深度网络能够构建起一个复杂的概念的层次结构。这有点像传统编程语言使用模块化的设计和抽象的思想来创建复杂的计算机程序。将深度网络与浅层网络进行对比，有点像将一个能够进行函数调用的程序语言与一个不能进行函数调用的精简语言进行对比。抽象在神经网络中的形式和传统的编程方式相比不同，但它同样重要。 参考文献[1] Michael Nielsen. CHAPTER 1 Using neural nets to recognize handwritten digits[DB/OL]. http://neuralnetworksanddeeplearning.com/chap1.html, 2018-06-19. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap1.tex, 2018-06-19. [3] skylook. neural-networks-and-deep-learning, mnist_loader.py[DB/OL]. https://github.com/skylook/neural-networks-and-deep-learning/blob/master/src/mnist_loader.py, 2018-06-19. [4] skylook. neural-networks-and-deep-learning, network.py[DB/OL]. https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py, 2018-06-19.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 1 Using neural nets to recognize handwritten digits</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>手写数字识别</tag>
        <tag>代码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用神经网络识别手写数字——梯度下降算法]]></title>
    <url>%2F2018%2F06%2F18%2F%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 1 Using neural nets to recognize handwritten digits 神经网络的架构假设我们有这样的网络： 前面提过，这个网络中最左边的称为输入层，其中的神经元称为输入神经元。最右边的，即输出层包含有输出神经元，在本例中，输出层只有一个神经元。中间层，既然这层中的神经元既不是输入也不是输出，则被称为隐藏层。“隐藏”这一术语也许听上去有些神秘，我第一次听到这个词，以为它必然有一些深层的哲学或数学涵意，但它实际上仅仅意味着“既非输入也非输出”。上面的网络仅有一个隐藏层，但有些网络有多个隐藏层。例如，下面的四层网络有两个隐藏层： 有些令人困惑的是，由于历史的原因，尽管是由 S 型神经元而不是感知器构成，这种多层网络有时被称为多层感知器或者 MLP。 在这本书中我不会使用 MLP 这个术语，因为我认为这会引起混淆，但这里想提醒你它的存在。 目前为止，我们讨论的神经网络，都是以上一层的输出作为下一层的输入。这种网络被称为前馈神经网络。这意味着网络中是没有回路的，信息总是向前传播，从不反向回馈。如果确实有回路，我们最终会有这样的情况：$\sigma$ 函数的输入依赖于输出。这将难于理解，所以我们不允许这样的环路。 然而，也有一些人工神经网络的模型，其中反馈环路是可行的。它们原理上比前馈网络更接近我们大脑的实际工作。并且循环网络能解决一些重要的问题，这些问题如果仅仅用前馈网络来解决，则更加困难。然而为了篇幅，本书将专注于使用更广泛的前馈网络。 一个简单的分类手写数字的网络我们将使用一个三层神经网络来识别单个数字： 网络的输入层包含给输入像素的值进行编码的神经元。就像下一节会讨论的，我们给网络的训练数据会有很多扫描得到的 $28 \times 28$ 的手写数字的图像组成，所有输入层包含有 $784 = 28 \times 28$ 个神经元。为了简化，上图中我已经忽略了 $784$ 中大部分的输入神经元。输入像素是灰度级的，值为 $0.0$ 表示白色，值为 $1.0$ 表示黑色，中间数值表示逐渐暗淡的灰色。 网络的第二层是一个隐藏层。我们用 $n$ 来表示神经元的数量，我们将给 $n$ 实验不同的数值。示例中用一个小的隐藏层来说明，仅仅包含 $n=15$ 个神经元。 网络的输出层包含有 $10$ 个神经元。如果第一个神经元激活，即输出 $\approx 1$，那么表明网络认为数字是一个 $0$。如果第二个神经元激活，就表明网络认为数字是一个 $1$。依此类推。更确切地说，我们把输出神经元的输出赋予编号 $0$ 到 $9$，并计算出那个神经元有最高的激活值。比如，如果编号为 $6$ 的神经元激活，那么我们的网络会猜到输入的数字是 $6$。其它神经元相同。 为什么用 10 个输出神经元你可能会好奇为什么我们用 $10$ 个输出神经元。毕竟我们的任务是能让神经网络告诉我们哪个数字（ $0, 1, 2, \ldots, 9$ ）能和输入图片匹配。一个看起来更自然的方式就是使用 $4$ 个输出神经元，把每一个当做一个二进制值，结果取决于它的输出更靠近 $0$ 还是$1$。四个神经元足够编码这个问题了，因为 $2^4 = 16$ 大于 $10$ 种可能的输入。为什么我们反而要用 $10$ 个神经元呢？这样做难道效率不低吗？ 最终的判断是基于经验主义的：我们可以实验两种不同的网络设计，结果证明对于这个特定的问题而言，$10$ 个输出神经元的神经网络比4个的识别效果更好。但是令我们好奇的是为什么使用 $10$ 个输出神经元的神经网络更有效呢。 有没有什么启发性的方法能提前告诉我们用10个输出编码比使用 $4$个输出编码更有好呢？ 为了理解为什么我们这么做，我们需要从根本原理上理解神经网络究竟在做些什么。首先考虑有 $10$ 个神经元的情况。我们首先考虑第一个输出神经元，它告诉我们一个数字是不是0。它能那么做是因为可以权衡从隐藏层来的信息。隐藏层的神经元在做什么呢？假设隐藏层的第一个神经元只是用于检测如下的图像是否存在： 为了达到这个目的，它通过对此图像对应部分的像素赋予较大权重，对其它部分赋予较小的权重。同理，我们可以假设隐藏层的第二，第三，第四个神经元是为检测下列图片是否存在： 就像你能猜到的，这四幅图像组合在一起构成了前面显示的一行数字图像中的 $0$： 假设神经网络以上述方式运行，我们可以给出一个貌似合理的理由去解释为什么用 $10$ 个输出而不是 $4$ 个。如果我们有 $4$ 个输出，那么第一个输出神经元将会尽力去判断数字的最高有效位是什么。 把数字的最高有效位和数字的形状联系起来并不是一个简单的问题。很难想象出有什么恰当的历史原因，一个数字的形状要素会和一个数字的最高有效位有什么紧密联系。 上面我们说的只是一个启发性的方法。没有什么理由表明这个三层的神经网络必须按照我所描述的方式运行，即隐藏层是用来探测数字的组成形状。可能一个聪明的学习算法将会找到一些合适的权重能让我们仅仅用4个输出神经元就行。但是这个启发性的方法通常很有效，它会节省你大量时间去设计一个好的神经网络结构。 使用梯度下降算法进行学习现在我们有了神经网络的设计，它怎样可以学习识别数字呢？我们需要的第一样东西是一个用来学习的数据集称为训练数据集。我们将使用 MNIST 数据集。 我们将用符号 $x$ 来表示一个训练输入。为了方便，把每个训练输入 $x$ 看作一个 $28\times 28 = 784$ 维的向量。每个向量中的项目代表图像中单个像素的灰度值。我们用 $y= y(x)$ 表示对应的期望输出，这里 $y$ 是一个 $10$ 维的向量。例如，如果有一个特定的画成 $6$ 的训练图像，$x$ ，那么 $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$ 则是网络的期望输出。注意这里 $T$ 是转置操作，把一个行向量转换成一个列向量。 二次代价函数我们希望有一个算法，能让我们找到权重和偏置，以至于网络的输出$y(x)$ 能够拟合所有的训练输入$x$。为了量化我们如何实现这个目标，我们定义一个代价函数（有时被称为损失或目标函数。我们在这本书中可能会交换使用这几个术语）： C(w,b) \equiv \frac{1}{2n} \sum_x \| y(x) - a\|^2这里 $w$ 表示所有的网络中权重的集合，$b$ 是所有的偏置，$n$ 是训练输入数据的个数，$a$ 是表示当输入为 $x$ 时输出的向量，求和则是在总的训练输入$x$ 上进行的。当然，输出 $a$ 取决于 $x$, $w$ 和 $b$，但是为了保持符号的简洁性，我没有明确地指出这种依赖关系。符号 $|v|$ 是指向量 $v$ 的模。我们把 $C$ 称为二次代价函数；有时也被称为均方误差或者 MSE。观察二次代价函数的形式我们可以看到 $C(w,b)$ 是非负的，因为求和公式中的每一项都是非负的。此外，代价函数$C(w,b)$ 的值相当小，即 $C(w,b) \approx 0$，精确地说，是当对于所有的训练输入 $x$，$y(x)$ 接近于输出 $a$ 时。因此如果我们的学习算法能找到合适的权重和偏置，使得 $C(w,b) \approx 0$，它就能很好地工作。相反，当 $C(w,b)$ 很大时就不怎么好了，那意味着对于大量地输入， $y(x)$ 与输出 $a$ 相差很大。因此我们的训练算法的目的，是最小化权重和偏置的代价函数 $C(w,b)$。换句话说，我们想要找到一系列能让代价尽可能小的权重和偏置。我们将采用称为梯度下降的算法来达到这个目的。 为什么使用二次代价函数为什么要介绍二次代价呢？毕竟我们最初感兴趣的内容不是能正确分类的图像数量吗？为什么不试着直接最大化这个数量，而是去最小化一个类似二次代价的间接评量呢？这么做是因为在神经网络中，被正确分类的图像数量所关于权重和偏置的函数并不是一个平滑的函数。 大多数情况下，对权重和偏置做出的微小变动完全不会影响被正确分类的图像的数量。这会导致我们很难去解决如何改变权重和偏置来取得改进的性能。而用一个类似二次代价的平滑代价函数则能更好地去解决如何用权重和偏置中的微小的改变来取得更好的效果。 这就是为什么我们首先专注于最小化二次代价，只有这样，我们之后才能测试分类精度。 即使已经知道我们需要使用一个平滑的代价函数，你可能仍然想知道为什么我们选择二次函数。这是临时想出来的吗？是不是我们选择另一个不同的代价函数将会得到完全不同的最小化的权重和偏置呢？这种顾虑是合理的，我们后面会再次回到这个代价函数，并做一些修改。尽管如此，二次代价函数让我们更好地理解神经网络中学习算法的基础，所以目前我们会一直使用它。 重复一下，我们训练神经网络的目的是找到能最小化二次代价函数 $C(w,b)$ 的权重和偏置。这是一个适定问题，但是现在它有很多让我们分散精力的结构，对权重 $w$ 和偏置 $b$的解释，晦涩不清的 $\sigma$ 函数，神经网络结构的选择，MNIST 等等。事实证明我们可以忽略结构中大部分，把精力集中在最小化方面来理解它。现在我们打算忘掉所有关于代价函数的具体形式、神经网络的连接等等。现在让我们想象只要最小化一个给定的多元函数。我们打算使用一种被称为梯度下降的技术来解决这样的最小化问题。然后我们回到在神经网络中要最小化的特定函数上来。 梯度下降算法好了，假设我们要最小化某些函数，$C(v)$。它可以是任意的多元实值函数，$v = v_1,v_2, \ldots$。注意我们用 $v$ 代替了 $w$ 和 $b$ 以强调它可能是任意的函数，我们现在先不局限于神经网络的环境。为了最小化 $C(v)$，想象 $C$ 是一个只有两个变量$v1$ 和 $v2$ 的函数： 为什么要使用梯度下降算法一种解决这个问题的方式是用微积分来解析最小值。我们可以计算导数去寻找 $C$ 的极值点。运气好的话，$C$ 是一个只有一个或少数几个变量的函数。但是变量过多的话那就是噩梦。而且神经网络中我们经常需要大量的变量——最大的神经网络有依赖数亿权重和偏置的代价函数，极其复杂。用微积分来计算最小值已经不可行了。 好吧，微积分是不能用了。幸运的是，有一个漂亮的推导法暗示有一种算法能得到很好的效果。首先把我们的函数想象成一个山谷。只要瞄一眼上面的绘图就不难理解。我们想象有一个小球从山谷的斜坡滚落下来。我们的日常经验告诉我们这个球最终会滚到谷底。也许我们可以用这一想法来找到函数的最小值？我们会为一个（假想的）球体随机选择一个起始位置，然后模拟球体滚落到谷底的运动。我们可以通过计算 $C$ 的导数（或者二阶导数）来简单模拟——这些导数会告诉我们山谷中局部“形状”的一切，由此知道我们的球将怎样滚动。 注意小球不必遵常规的循物理学理论，我们在这里就是上帝，能够支配球体可以如何滚动，那么我们将会采取什么样的运动学定律来让球体能够总是滚落到谷底呢？我们让小球要往代价函数梯度的反方向滚动。 为什么小球要往梯度的反方向滚动为了更精确地描述这个问题，让我们思考一下，当我们在 $v1$ 和 $v2$ 方向分别将球体移动一个很小的量，即 $\Delta v1$ 和 $\Delta v2$ 时，球体将会发生什么情况。微积分告诉我们 $C$ 将会有如下变化： \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2我们要寻找一种选择 $\Delta v_1$ 和 $\Delta v_2$ 的方法使得 $\Delta C$ 为负；即，我们选择它们是为了让球体滚落。为了弄明白如何选择，需要定义 $\Delta v$ 为 $v$ 变化的向量，$\Delta v \equiv (\Delta v_1, \Delta v_2)^T$，$T$ 是转置符号。我们也定义 $C$ 的梯度为偏导数的向量，$\left(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2}\right)^T$。我们用 $\nabla C$ 来表示梯度向量，即： \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T有了这些定义，$\Delta C$ 的表达式可以被重写为： \Delta C \approx \nabla C \cdot \Delta v这个表达式解释了为什么 $\nabla C$ 被称为梯度向量：$\nabla C$ 把 $v$ 的变化关联为 $C$ 的变化，正如我们期望的用梯度来表示。但是这个方程真正让我们兴奋的是它让我们看到了如何选取 $\Delta v$ 才能让 $\Delta C$ 为负数。假设我们选取： \Delta v = -\eta \nabla C这里的 $\eta$ 是个很小的正数（称为学习率）。方程$\Delta v = -\eta \nabla C$告诉我们 $\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta |\nabla C|^2$。由于 $| \nabla C |^2 \geq 0$，这保证了$\Delta C \leq 0$，即，如果我们按照方程$\Delta v = -\eta \nabla C$的规则去改变 $v$，那么 $C$ 会一直减小，不会增加。（当然，要在方程$\Delta C \approx \nabla C \cdot \Delta v$的近似约束下）。这正是我们想要的特性！因此我们把方程$\Delta v = -\eta \nabla C$ 用于定义球体在梯度下降算法下的“运动定律”。也就是说，我们用方程$\Delta v = -\eta \nabla C$计算 $\Delta v$，来移动球体的位置 $v$： v \rightarrow v' = v -\eta \nabla C然后我们用它再次更新规则来计算下一次移动。如果我们反复持续这样做，我们将持续减小$C$ 直到,正如我们希望的,获得一个全局的最小值。 总结一下，梯度下降算法工作的方式就是重复计算梯度 $\nabla C$，然后沿着相反的方向移动，沿着山谷“滚落”。我们可以想象它像这样： 注意具有这一规则的梯度下降并不是模仿实际的物理运动。在现实中一个球体有动量，使得它岔开斜坡滚动，甚至（短暂地）往山上滚。只有在克服摩擦力的影响，球体才能保证滚到山谷。相比之下，我们选择 $\Delta v$ 规则只是说：“往下，现在”。这仍然是一个寻找最小值的非常好的规则！ 为了使梯度下降能够正确地运行，我们需要选择足够小的学习率 $\eta$ 使得方程$\Delta C \approx \nabla C \cdot \Delta v$ 能得到很好的近似。如果不这样，我们会以 $\Delta C &gt; 0$ 结束，这显然不好。同时，我们也不想 $\eta$ 太小，因为这会使 $\Delta v$ 的变化极小，梯度下降算法就会运行得非常缓慢。在真正的实现中，$\eta$ 通常是变化的，以至方程$\Delta C \approx \nabla C \cdot \Delta v$能保持很好的近似度，但算法又不会太慢。我们后面会看这是如何工作的。 梯度下降算法的形式化证明我已经解释了具有两个变量的函数 $C$ 的梯度下降。但事实上，即使 $C$ 是一个具有更多变量的函数也能很好地工作。我们假设 $C$ 是一个有 $m$ 个变量 $v_1,\ldots,v_m$ 的多元函数。那么对 $C$ 中自变量的变化 $\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$，$\Delta C$ 将会变为： \Delta C \approx \nabla C \cdot \Delta v这里的梯度 $\nabla C$ 是向量 \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots, \frac{\partial C}{\partial v_m}\right)^T正如两个变量的情况，我们可以选取 \Delta v = -\eta \nabla C而且 $\Delta C$ 的（近似）表达式 $\Delta C \approx \nabla C \cdot \Delta v$ 保证是负数。这给了我们一种方式从梯度中去取得最小值，即使 $C$ 是任意的多元函数，我们也能重复运用更新规则 v \rightarrow v' = v-\eta \nabla C你可以把这个更新规则看做定义梯度下降算法。这给我们提供了一种方式去通过重复改变 $v$ 来找到函数 $C$ 的最小值。这个规则并不总是有效的，有几件事能导致错误，让我们无法从梯度下降来求得函数 $C$ 的全局最小值，这个观点我们会在后面的章节中去探讨。但在实践中，梯度下降算法通常工作地非常好，在神经网络中这是一种非常有效的方式去求代价函数的最小值，进而促进网络自身的学习。 事实上，甚至有一种观点认为梯度下降法是求最小值的最优策略。假设我们正在努力去改变 $\Delta v$ 来让 $C$ 尽可能地减小。这相当于最小化 $\Delta C \approx \nabla C \cdot \Delta v$。我们首先限制步长为小的固定值，即 $| \Delta v | = \epsilon$，$ \epsilon &gt; 0$。当步长固定时，我们要找到使得 $C$ 减小最大的下降方向。可以证明，使得 $\nabla C \cdot \Delta v$ 取得最小值的 $\Delta v$ 为 $\Delta v = - \eta \nabla C$，这里 $\eta = \epsilon / |\nabla C|$ 是由步长限制 $|\Delta v| = \epsilon$ 所决定的。因此，梯度下降法可以被视为一种在 $C$ 下降最快的方向上做微小变化的方法。 人们已经研究出很多梯度下降的变化形式，包括一些更接近真实模拟球体物理运动的变化形式。这些模拟球体的变化形式有很多优点，但是也有一个主要的缺点：它最终必需去计算$C$ 的二阶偏导，这代价可是非常大的。 为了理解为什么这种做法代价高，假设我们想求所有的二阶偏导 $\partial^2 C/ \partial v_j \partial v_k$。如果我们有上百万的变量$v_j$，那我们必须要计算数万亿（即百万次的平方）级别的二阶偏导（实际上，更接近万亿次的一半，因为$\partial^2 C/ \partial v_j \partial v_k = \partial^2 C/ \partial v_k \partial v_j$。同样，你知道怎么做。！）这会造成很大的计算代价。不过也有一些避免这类问题的技巧，寻找梯度下降算法的替代品也是个很活跃的研究领域。但在这本书中我们将主要用梯度下降算法（包括变化形式）使神经网络学习。 随机梯度下降算法神经网络中如何使用梯度下降算法我们怎么在神经网络中用梯度下降算法去学习呢？其思想就是利用梯度下降算法去寻找能使得方程代价函数取得最小值的权重 $w_k$ 和偏置 $b_l$。为了清楚这是如何工作的，我们将用权重和偏置代替变量 $v_j$。也就是说，现在“位置”变量有两个分量组成：$w_k$ 和 $b_l$，而梯度向量 $\nabla C$ 则有相应的分量 $\partial C / \partial w_k$和$\partial C / \partial b_l$。用这些分量来写梯度下降的更新规则，我们得到： w_k \rightarrow w_k' = w_k-\eta \frac{\partial C}{\partial w_k}b_l \rightarrow b_l' = b_l-\eta \frac{\partial C}{\partial b_l}通过重复应用这一更新规则我们就能“让球体滚下山”，并且有望能找到代价函数的最小值。换句话说，这是一个能让神经网络学习的规则。 为什么使用随机梯度下降算法应用梯度下降规则有很多挑战。我们将在下一章深入讨论。但是现在只提及一个问题。为了理解问题是什么，我们先回顾 $C(w,b) \equiv \frac{1}{2n} \sum_x | y(x) - a|^2$ 中的二次代价函数。注意这个代价函数有着这样的形式 $C = \frac{1}{n} \sum_x C_x$，即，它是遍及每个训练样本代价 $C_x \equiv \frac{|y(x)-a|^2}{2}$ 的平均值。在实践中，为了计算梯度 $\nabla C$，我们需要为每个训练输入 $x$ 单独地计算梯度值 $\nabla C_x$，然后求平均值，$\nabla C = \frac{1}{n} \sum_x \nabla C_x$。不幸的是，当训练输入的数量过大时会花费很长时间，这样会使学习变得相当缓慢。 有种叫做随机梯度下降的算法能够加速学习。其思想就是通过随机选取小量训练输入样本来计算 $\nabla C_x$，进而估算梯度 $\nabla C$。通过计算少量样本的平均值我们可以快速得到一个对于实际梯度 $\nabla C$ 的很好的估算，这有助于加速梯度下降，进而加速学习过程。 怎么使用随机梯度下降算法准确地说，随机梯度下降通过随机选取小量的 $m$ 个训练输入来工作。我们将这些随机的训练输入标记为$X_1, X_2, \ldots, X_m$，并把它们称为一个小批次。假设样本数量 $m$ 足够大，我们期望 $\nabla C_{X_j}$ 的平均值大致相等于整个 $\nabla C_x$的平均值，即， \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C这里的第二个求和符号是在整个训练数据上进行的。交换两边我们得到 \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}}证实了我们可以通过仅仅计算随机选取的小批量数据的梯度来估算整体的梯度。 为了将其明确地和神经网络的学习联系起来，假设 $w_k$ 和 $b_l$ 表示我们神经网络中权重和偏置。随机梯度下降通过随机地选取并训练输入的小批量数据来工作， w_k \rightarrow w_k' = w_k-\frac{\eta}{m}\sum_j \frac{\partial C_{X_j}}{\partial w_k}b_l \rightarrow b_l' = b_l-\frac{\eta}{m}\sum_j \frac{\partial C_{X_j}}{\partial b_l}其中两个求和符号是在当前小批量数据中的所有训练样本 $X_j$ 上进行的。然后我们再挑选另一随机选定的小批量数据去训练。直到我们用完了所有的训练输入，这被称为完成了一个训练周期。然后我们就会开始一个新的训练周期。 另外值得提一下，对于改变代价函数大小的参数，和用于计算权重和偏置 的小批量数据的更新规则，会有不同的约定。在方程$C(w,b) \equiv \frac{1}{2n} \sum_x | y(x) - a|^2$ 中，我们通过因子 $\frac{1}{n}$ 来改变整个代价函数的大小。人们有时候忽略 $\frac{1}{n}$，直接取单个训练样本的代价总和，而不是取平均值。这对我们不能提前知道训练数据数量的情况下特别有效。例如，这可能发生在有更多的训练数据是实时产生的情况下。同样，小批量数据的更新规则有时也会舍弃前面的 $\frac{1}{m}$。从概念上这会有一点区别，因为它等价于改变了学习率 $\eta$ 的大小。但在对不同工作进行详细对比时，需要对它警惕。 随机梯度下降算法与梯度下降算法比较我们可以把随机梯度下降想象成一次民意调查：在一个小批量数据上采样比对一个完整数据集进行梯度下降分析要容易得多，正如进行一次民意调查比举行一次全民选举要更容易。例如，如果我们有一个规模为 $n = 60,000$ 的训练集，就像 MNIST，并选取 小批量数据大小为 $m = 10$，这意味着在估算梯度过程中加速了 $6,000$ 倍！当然，这个估算并不是完美的，存在统计波动，但是没必要完美：我们实际关心的是在某个方向上移动来减少 $C$，而这意味着我们不需要梯度的精确计算。在实践中，随机梯度下降是在神经网络的学习中被广泛使用、十分有效的技术，它也是本书中展开的大多数学习技术的基础。 梯度下降算法一个极端的版本是把小批量数据的大小设为 $1$。即，假设一个训练输入 $x$，我们按照规则 $w_k \rightarrow w_k’ = w_k - \eta \partial C_x / \partial w_k$ 和 $b_l \rightarrow b_l’ = b_l - \eta \partial C_x / \partialb_l$ 更新我们的权重和偏置。然后我们选取另一个训练输入，再一次更新权重和偏置。如此重复。这个过程被称为在线、online、on-line、或者递增学习。在 online 学习中，神经网络在一个时刻只学习一个训练输入（正如人类做的）。 理解梯度和导数CSDN ： [机器学习] ML重要概念：梯度（Gradient）与梯度下降法（Gradient Descent） 知乎 ： 如何直观形象的理解方向导数与梯度以及它们之间的关系？ 知乎 ： 什么是全导数？ 参考文献[1] Michael Nielsen. CHAPTER 1 Using neural nets to recognize handwritten digits[DB/OL]. http://neuralnetworksanddeeplearning.com/chap1.html, 2018-06-18. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap1.tex, 2018-06-18.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 1 Using neural nets to recognize handwritten digits</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络架构</tag>
        <tag>二次代价函数</tag>
        <tag>梯度下降算法</tag>
        <tag>随机梯度下降算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用神经网络识别手写数字——感知器]]></title>
    <url>%2F2018%2F06%2F17%2F%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E2%80%94%E2%80%94%E6%84%9F%E7%9F%A5%E5%99%A8%2F</url>
    <content type="text"><![CDATA[原文链接：CHAPTER 1 Using neural nets to recognize handwritten digits 本书序言在传统的编程方法中，我们告诉计算机做什么，把大问题分成许多小的、精确定义的任务，计算机可以很容易地执行。相比之下，在神经网络中，我们不告诉计算机如何解决我们的问题。相反，它从观测数据中学习，找出它自己的解决问题的方法。 一个以原理为导向的方法本书一个坚定的信念，是让读者更好地去深刻理解神经网络和深度学习，而不是像一张冗长的洗衣单一样模糊地列出一堆想法。如果你很好理解了核心理念，你就可以很快地理解其他新的推论。用编程语言对比，把这理解为掌握一种新语言的核心语法、库和数据结构。你可能仍然只是``知道’’整个编程语言的一小部分—-许多编程语言有巨大的标准库—-但新的库和数据结构可以很快且容易被理解。 这就意味着这本书的重点不是作为一个如何使用一些特定神经网络库的教程。如果你主要想围绕着某个程序库的方式去学习，那不要读这本书！找到你想学习的程序库，并通过教程和文档来完成。注意这点。虽然这也许能很快解决你的问题，但是，如果你想理解神经网络中究竟发生了什么，如果你想要了解今后几年都不会过时的原理，那么只是学习些热门的程序库是不够的。你需要领悟让神经网络工作的原理。技术来来去去，但原理是永恒的。 一个动手实践的方法我们将通过攻克一个具体的问题：教会计算机识别手写数字的问题，来学习神经网络和深度学习的核心理论。 这个问题用常规的方法来编程解决是非常困难的。然而，正如我们所看到的，它可以很好地利用一个简单的神经网络来解决，只需几十行代码，没有特别的库。更多的是，我们会通过多次迭代来改进程序，逐步融入神经网络和深度学习的核心思想。 难得有一本书能兼顾理论和动手实践。但是我相信，如果我们建立了神经网络的基本思路，你会学到最好的。我们将开发实际能用的代码，而不仅仅是抽象的理论，这些代码你可以探索和扩展。这样你就可以理解其基础，不论是理论还是实践，并且扩展和提高你的知识面。 使用神经网络识别手写数字——感知器本章我们将实现一个可以识别手写数字的神经网络。这个程序仅仅 74 行，不使用特别的神经网络库。然而，这个短小的网络不需要人类帮助便可以超过 96% 的准确率识别数字。而且，在后面的章节，我们会发展出将准确率提升到 99% 的技术。实际上，最优的商业神经网络已经足够好到被银行和邮局分别用在账单核查和识别地址上了。 手写识别常常被当成学习神经网络的原型问题，因此我们聚焦在这个问题上。作为一个原型，它具备一个关键点：挑战性，识别手写数字并不轻松，但也不会难到需要超级复杂的解决方法，或者超大规模的计算资源。另外，这其实也是一种发展出诸如深度学习更加高级的技术的方法。所以，整本书我们都会持续地讨论手写数字识别问题。本书后面部分，我们会讨论这些想法如何用在其他计算机视觉的问题或者语音、自然语言处理和其他一些领域中。 当然，如果仅仅为了编写一个计算机程序来识别手写数字，本章的内容可以简短很多！但前进的道路上，我们将扩展出很多关于神经网络的关键的思想，其中包括两个重要的人工神经元感知机和 S 型神经元），以及标准的神经网络学习算法，即随机梯度下降算法。自始至终，我专注于解释事情的原委，并构筑你对神经网络的直观感受。这需要一个漫长的讨论，而不是仅仅介绍些基本的技巧，但这对于更深入的理解是值得的。作为收益，在本章的最后，我们会准备好了解什么是深度学习，以及它为什么很重要。 感知器什么是神经网络？一开始，我将解释一种被称为“感知器”的人工神经元。今天，使用其它人工神经元模型更为普遍，在这本书中，以及更多现代的神经网络著作中，主要使用的是一种叫做 S 型的神经元模型。我们很快会讲到 S 型神经元。但是要理解为什么 S 型神经元被定义为那样的方式，值得花点时间先来理解下感知器。 感知器的定义感知器是如何工作的呢？一个感知器接受几个二进制输入，$x_1,x_2,\ldots$，并产生一个二进制输出： 示例中的感知器有三个输入，$x_1,x_2,x_3$。通常可以有更多或更少输入。Rosenblatt 提议一个简单的规则来计算输出。他引入权重，$w_1,w_2,\ldots$，表示相应输入对于输出重要性的实数。神经元的输出，$0$ 或者 $1$，则由分配权重后的总和 $\sum_j w_j x_j$ 小于或者大于一些阈值决定。和权重一样，阈值是一个实数，一个神经元的参数。用更精确的代数形式： \text{output} = \begin{cases} 0 & \quad \text{if } \sum_j w_j x_j \leq \text{ threshold} \\ 1 & \quad \text{if } \sum_j w_j x_j > \text{ threshold} \\ \end{cases}这就是一个感知器所要做的所有事情！ 目前为止我把像 $x_1$ 和 $x_2$ 这样的输入画成感知器网络左边浮动的变量。实际上，可以画一层额外的感知器输入层来方便对输入编码： 这种对有一个输出但没有输入的感知器的标记法， 是一种标准。它并不实际表示一个感知器没有输入。为了看清它，假设我们确实有一个没有输入的感知器。那么加权和 $\sum_j w_j x_j$ 会总是为零，并且感知器在 $b &gt; 0$ 时输出 $1$，当 $b \leq 0$时输出 $0$。那样，感知器会简单输出一个固定值，而不是期望值（上例中的 $x_1$）。倒不如完全不把输入感知器看作感知器，而是简单定义为输出期望值的特殊单元，$x_1, x_2,\ldots$。 感知器做决策的原理随着权重和阈值的变化，你可以得到不同的决策模型。我们来看一个由感知器构成的网络。 在这个网络中，第一列感知器我们称其为第一层感知器通过权衡输入依据做出三个非常简单的决定。那第二层的感知器呢？每一个都在权衡第一层的决策结果并做出决定。以这种方式，一个第二层中的感知器可以比第一层中的做出更复杂和抽象的决策。在第三层中的感知器甚至能进行更复杂的决策。以这种方式，一个多层的感知器网络可以从事复杂巧妙的决策。 顺便提一下，当我定义感知器时我说的是感知器只有一个输出。在上面的网络中感知器看上去像是有多个输出。实际上，他们仍然是单输出的。多个感知器输出箭头仅仅便于说明一个感知器的输出被用于其它感知器的输入。 它和把单个输出线条分叉相比，显得讨巧些。 让我们简化感知器的数学描述。条件 $\sum_j w_j x_j$ 看上去有些冗长，我们可以创建两个符号的变动来简化。第一个变动是把 $\sum_j w_j x_j$ 改写成点乘，$w\cdot x \equiv \sum_j w_j x_j$，这里 $w$ 和 $x$ 对应权重和输入的向量。第二个变动是把阈值移到不等式的另一边，并用感知器的偏置 $b \equiv -threshold$ 代替。用偏置而不是%阈值，那么感知器的规则可以重写为 \text{output} = \begin{cases} 0 & \quad \text{if } w\cdot x + b \leq 0 \\ 1 & \quad \text{if } w\cdot x + b > 0 \end{cases}我们可以把偏置看作一种表示让感知器输出 $1$（或者用生物学的术语，即激活感知器）有多容易的估算。对于具有一个非常大偏置的感知器来说，输出 $1$ 是很容易的。但是如果偏置是一个非常小的负数，输出$1$ 则很困难。很明显，引入偏置只是我们描述感知器的一个很小的变动，但是我们后面会看到它引导更进一步的符号简化。因此，在这本书的后续部分，我们不再用阈值，而总是使用偏置。 感知器与逻辑运算我已经描述过感知器是一种权衡依据来做出决策的方法。感知器被采用的另一种方式，是计算基本的逻辑功能，即我们通常认为的运算基础，例如“与”，“或”和“与非”。例如，假设我们有个两个输入的感知器，每个权重为 $-2$，整体的偏置为 $3$。这是我们的感知器， 这样我们得到：输入 $00$ 产生输出 $1$，即 (-2)0 + (-2)0 + 3 = 3 是正数。这里我用 $$ 符号来显式地表示乘法。但是输入 $11$ 产生输出 0，即 (-2)1 + (-2)*1 + 3 = -1 是负数。如此我们的感知器实现了一个与非门！ 与非门的例子显示了我们可以用感知器来计算简单的逻辑功能。实际上，我们完全能用感知器网络来计算任何逻辑功能。原因是与非门是通用运算，那样，我们能在多个与非门之上构建出任何运算。 感知器运算的通用性既是令人鼓舞的，又是令人失望的。令人鼓舞是因为它告诉我们感知器网络能和其它计算设备一样强大。但是它也令人失望，因为它看上去只不过是一种新的与非门。这简直不算个大新闻！ 然而，实际情况比这一观点认为的更好。其结果是我们可以设计学习算法，能够自动调整人工神经元的权重和偏置。这种调整可以自动响应外部的刺激，而不需要一个程序员的直接干预。这些学习算法是我们能够以一种根本区别于传统逻辑门的方式使用人工神经元。有别于显式地设计与非门或其它门，我们的神经网络能简单地学会解决问题，这些问题有时候直接用传统的电路设计是很难解决的。 S 型神经元学习算法听上去非常棒。但是我们怎样给一个神经网络设计这样的算法呢？假设我们有一个感知器网络，想要用它来解决一些问题。 引入 S 型神经元的原因例如，网络的输入可以是一幅手写数字的扫描图像。我们想要网络能学习权重和偏置，这样网络的输出能正确分类这些数字。为了看清学习是怎样工作的，假设我们把网络中的权重（或者偏置）做些微小的改动。就像我们马上会看到的，这一属性会让学习变得可能。这里简要示意我们想要的（很明显这个网络对于手写识别还是太简单了！）： 如果对权重（或者偏置）的微小的改动真的能够仅仅引起输出的微小变化，那我们可以利用这一事实来修改权重和偏置，让我们的网络能够表现得像我们想要的那样。例如，假设网络错误地把一个“9”的图像分类为“8”。我们能够计算出怎么对权重和偏置做些小的改动，这样网络能够接近于把图像分类为“9”。然后我们要重复这个工作，反复改动权重和偏置来产生更好的输出。这时网络就在学习。 问题是当我们给实际网络加上感知器时，结果并不像我们想象的那样。实际上，网络中单个感知器上一个权重或偏置的微小改动有时候会引起那个感知器的输出完全翻转，如 $0$ 变到 $1$。那样的翻转可能接下来引起其余网络的行为以极其复杂的方式完全改变。因此，虽然你的“9”可能被正确分类，网络在其它图像上的行为很可能以一些很难控制的方式被完全改变。这使得逐步修改%权重和偏置来让网络接近期望行为变得困难。也许有其它聪明的方式来解决这个问题。但是目前为止，我们还没发现有什么办法能让感知器网络进行学习。 我们可以引入一种称为 S 型神经元的新的人工神经元来克服这个问题。S 型神经元和感知器类似，但是经过修改后，权重和偏置的微小改动只引起输出的微小变化。这对于让神经元网络学习起来是很关键的。 好了, 让我来描述下 S 型神经元。我们用描绘感知器的相同方式来描绘 S 型神经元： 正如一个感知器， S 型神经元有多个输入，$x_1,x_2,\ldots$。但是这些输入可以取 $0$ 和$1$ 中的任意值，而不仅仅是 $0$ 或 $1$。例如，$0.638\ldots$ 是一个 S 型神经元的有效输入。同样， S 型神经元对每个输入有权重，$w_1,w_2,\ldots$，和一个总的偏置，$b$。但是输出不是 $0$ 或 $1$。相反，它现在是 $\sigma(w \cdot x+b)$，这里 $\sigma$ 被称为 S型函数（顺便提一下，$\sigma$ 有时被称为逻辑函数，而这种新的神经元类型被称为逻辑神经元。既然这些术语被很多从事于神经元网络的人使用，记住它是有用的。然而，我们将继续使用 S 型这个术语。）定义为： \sigma(z) \equiv \frac{1}{1+e^{-z}}把它们放在一起来更清楚地说明，一个具有输入 $x_1,x_2,\ldots$，权重$w_1,w_2,\ldots$，和偏置 $b$ 的 S 型神经元的输出是： \frac{1}{1+\exp(-\sum_j w_j x_j-b)}S 型神经元的性质初看上去， S 型神经元和感知器有很大的差别。如果你不熟悉 S型函数的代数形式，它看上去晦涩难懂又令人生畏。实际上，感知器和 S 型神经元之间有很多相似的地方，跨过理解上的障碍，S 型神经元的代数形式具有很多技术细节。 为了理解和感知器模型的相似性，假设 $z \equiv w \cdot x + b$ 是一个很大的正数。那么 $e^{-z} \approx 0$ 而 $\sigma(z) \approx 1$。即，当 $z = w \cdotx+b$ 很大并且为正， S 型神经元的输出近似为 $1$，正好和感知器一样。相反地，假设 $z = w \cdot x+b$ 是一个很大的负数。那么$e^{-z} \rightarrow \infty$，$\sigma(z) \approx 0$。所以当 $z = w \cdot x +b$ 是一个很大的负数， S 型神经元的行为也非常近似一个感知器。只有在 $w \cdot x+b$ 取中间值时，和感知器模型有比较大的偏离。 $\sigma$ 的代数形式又是什么？我们怎样去理解它呢？实际上，$\sigma$ 的精确形式不重要，重要的是这个函数绘制的形状。是这样： 这个形状是阶跃函数平滑后的版本： 如果 $\sigma$ 实际是个阶跃函数，既然输出会依赖于 $w\cdot x+b$ 是正数还是负数。（实际上，当 $w \cdot x +b = 0$ ，感知器输出 $0$，而同时阶跃函数输出$1$。所以严格地说，我们需要修改阶跃函数来符合这点。但是你知道怎么做。）那么S型神经元会成为一个感知器。利用实际的 $\sigma$ 函数，我们得到一个，就像上面说明的，平滑的感知器。确实，$\sigma$ 函数的平滑特性，正是关键因素，而不是其细部形式。$\sigma$ 的平滑意味着权重和%偏置的微小变化，即 $\Delta w_j$ 和 $\Delta b$，会从神经元产生一个微小的输出变化 $\Delta output$。实际上，微积分告诉我们 $\Delta output$ 可以很好地近似表示为： \Delta output \approx \sum_j \frac{\partial \, output}{\partial w_j} \Delta w_j + \frac{\partial \, output}{\partial b} \Delta b其中求和是在所有权重 $w_j$ 上进行的，而 $\partial \, output /\partial w_j$ 和$\partial \, output /\partial b$ 符号表示 $output$ 分别对于 $w_j$ 和 $b$ 的偏导数。如果偏导数这个概念让你不安，不必惊慌。上面全部用偏导数的表达式看上去很复杂，实际上它的意思非常简单（这可是个好消息）：$\Delta output$ 是一个反映权重和偏置变化，即 $\Delta w_j$ 和$\Delta b$，的线性函数。利用这个线性特性，我们比较容易细微地修改权重和偏置的值，从而获得我们需要的细微的输出变化。所以，因为 S 型神经元具有与感知器类似的本质行为，它们可以帮助我们了解权重和偏置的变化如何影响输出值。 如果对 $\sigma$ 来说重要的是形状而不是精确的形式，那为什么要在公式$\sigma(z) \equiv \frac{1}{1+e^{-z}}$中给 $\sigma$ 使用特定的形式呢？事实上，在下文我们还将不时地考虑一些神经元，它们给其它激活函数 $f(\cdot)$ 输出是 $f(w \cdot x + b)$。当我们使用一个不同的激活函数，最大的变化是公式$\Delta output \approx \sum_j \frac{\partial \, output}{\partial w_j} \Delta w_j + \frac{\partial \, output}{\partial b} \Delta b$ 中用于偏导数的特定值的改变。事实证明当我们后面计算这些偏导数，用 $\sigma$ 会简化数学计算，这是因为指数在求导时有些可爱的属性。无论如何，$\sigma$ 在神经网络的工作中被普遍使用，并且是这本书中我们最常使用的激活函数。 S 型神经元与感知器的关系一个感性的认识是：S 型神经元是阶跃函数平滑后的版本。 但可以在数学上证明如下两点： 假设我们把一个感知器网络中的所有权重和偏置乘以一个正的常数，$c&gt;0$。可以证明网络的行为并没有改变。 假设我们有上题中相同的设置，一个感知器网络。同样假设所有输入被选中。我们不需要实际的输入值，仅仅需要固定这些输入。假设对于网络中任何特定感知器的输入 $x$， 权重和偏置遵循 $w \cdot x + b\neq 0$。现在用 S 型神经元替换所有网络中的感知器，并且把权重和偏置乘以一个正的常量 $c&gt;0$。证明在 $c \rightarrow \infty$的极限情况下， S 型神经元网络的行为和感知器网络的完全一致。当一个感知器的 $w \cdot x + b = 0$ 时又为什么会不同？ 解释 S 型神经元的输出我们应该如何解释一个 S 型神经元的输出呢？很明显，感知器和 S 型神经元之间一个很大的不同是 S 型神经元不仅仅输出 $0$ 或 $1$。它可以输出 $0$ 到 $1$ 之间的任何实数，所以诸如 $0.173\ldots$ 和 $0.689\ldots$ 的值是合理的输出。这是非常有用的，例如，当我们想要输出来表示一个神经网络的图像像素输入的平均强度。但有时候这会是个麻烦。假设我们希望网络的输出表示“输入图像是一个9”或“输入图像不是一个9”。很明显，如果输出是 $0$ 或 $1$ 是最简单的，就像用感知器。但是在实践中，我们可以设定一个约定来解决这个问题，例如，约定任何至少为 $0.5$ 的输出为表示 “这是一个9”，而其它小于 $0.5$ 的输出为表示“不是一个9”。当我们正在使用这样的约定时，我总会清楚地提出来，这样就不会引起混淆。 人工神经元的其它模型到现在，我们使用的神经元都是 S 型神经元。理论上讲，从这样类型的神经元构建起来的神经网络可以计算任何函数。实践中，使用其他模型的神经元有时候会超过 S 型网络。取决于不同的应用，基于其他类型的神经元的网络可能会学习得更快，更好地泛化到测试集上，或者可能两者都有。让我们给出一些其他的模型选择，便于了解常用的模型上的变化。 可能最简单的变种就是 tanh 神经元，使用双曲正切函数替换了 S 型函数。输入为 $x$，权重向量为 $w$，偏置为 $b$ 的 tanh 神经元的输出是 \tanh(w \cdot x+b)这其实和 S 型神经元关系相当密切。回想一下 $\tanh$ 函数的定义： \tanh(z) \equiv \frac{e^z-e^{-z}}{e^z+e^{-z}}进行简单的代数运算，我们可以得到 \sigma(z) = \frac{1+\tanh(z/2)}{2}也就是说，$\tanh$ 仅仅是 S 型函数的按比例变化版本。我们同样也能用图像看看 $\tanh$ 的形状： 这两个函数之间的一个差异就是 $\tanh$ 神经元的输出的值域是 $(-1, 1)$ 而非 $(0,1)$。这意味着如果你构建基于 $\tanh$ 神经元，你可能需要正规化最终的输出（取决于应用的细节，还有你的输入），跟 sigmoid 网络略微不同。 类似于 S 型神经元，基于 S 型神经元的网络可以在理论上，计算任何将输入映射到$(-1, 1)$ 的函数（对于 tanh 和 S 型神经元，这个说法有一些技术上的预先声明。然而，非正式地，通常可以把神经网络看做可以以任意精度近似任何函数。）而且，诸如反向传播和随机梯度这样的想法也能够轻松地用在 tanh-neuron 神经元构成的网络上的。 参考文献[1] Michael Nielsen. CHAPTER 1 Using neural nets to recognize handwritten digits[DB/OL]. http://neuralnetworksanddeeplearning.com/chap1.html, 2018-06-17. [2] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap1.tex, 2018-06-17. [3] Michael Nielsen. CHAPTER 3 Improving the way neural networks learn[DB/OL]. http://neuralnetworksanddeeplearning.com/chap3.html, 2018-06-28. [4] Zhu Xiaohu. Zhang Freeman.Another Chinese Translation of Neural Networks and Deep Learning[DB/OL]. https://github.com/zhanggyb/nndl/blob/master/chap3.tex, 2018-06-28.]]></content>
      <categories>
        <category>深度学习</category>
        <category>Neural Networks and Deep Learning (Michael Nielsen)</category>
        <category>CHAPTER 1 Using neural nets to recognize handwritten digits</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>感知器</tag>
        <tag>Sigmoid函数</tag>
        <tag>S 型神经元</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[README]]></title>
    <url>%2F2018%2F05%2F20%2FREADME%2F</url>
    <content type="text"><![CDATA[yuanxiaosc.github.io个人博客；机器学习；深度学习；Python学习；]]></content>
  </entry>
  <entry>
    <title><![CDATA[学习资源归类]]></title>
    <url>%2F2017%2F07%2F28%2F%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E5%BD%92%E7%B1%BB%EF%BC%88%E7%BD%AE%E9%A1%B6%EF%BC%89%2F</url>
    <content type="text"><![CDATA[学习手册导航LATEX学习手册MarkDown学习手册NumPy学习手册1.0版 AIhttp://tools.google.com/seedbank/Collection of Interactive Machine Learning Examples JupyterGitHub | 教程 | data-science-ipython-notebooks 数据科学常用库的 jupyter notebook 教程 TensorFlowGoogle | 官网 | tensorflow 掘金翻译 | 翻译 | TensorFlow 官方文档中文版 香港科技大学计算机系教授 Sung KimGitHub | 课程代码 | DeepLearningZeroToAll 香港科技大学TensorFlow三天速成课件 PytorchFacebook | 官网 | PyTorch PyTorch 中文网 | 中文网 | PyTorch GitHub | 课程代码 | PyTorchZeroToAll PyTorchZeroToAll课件 Python知乎 | 代码教程 | 13 个 python3 才能用的特性 模型选择模型选择的一些基本思想和方法 PycharmWindows下的Pycharm远程连接虚拟机中Centos下的Python环境 面试算法工程师（机器学习） GitHub | 中山大学 郑永川 | 2018秋招笔记 书籍 Stanford | 书籍 | UFLDL Tutorial 本教程将教你无监督的特征学习和深度学习的主要思想。 O’Reilly book | 书籍 | Hands-on Machine Learning with Scikit-Learn and TensorFlow GitHub | 书籍代码 | Hands-on Machine Learning with Scikit-Learn and TensorFlowApachecn | 书籍翻译 | Sklearn 与 TensorFlow 机器学习实用指南 书籍下载区]]></content>
      <categories>
        <category>资源</category>
      </categories>
  </entry>
</search>
