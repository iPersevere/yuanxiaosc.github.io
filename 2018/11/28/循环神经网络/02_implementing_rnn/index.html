<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Implementing an RNN in TensorFlowThis script implements an RNN in TensorFlow to predict spam/ham from texts. We start by loading the necessary libraries and initializing a computation graph in TensorF">
<meta name="keywords" content="深度学习;机器学习;人工智能">
<meta property="og:type" content="article">
<meta property="og:title" content="望江人工智库">
<meta property="og:url" content="http://yoursite.com/2018/11/28/循环神经网络/02_implementing_rnn/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="Implementing an RNN in TensorFlowThis script implements an RNN in TensorFlow to predict spam/ham from texts. We start by loading the necessary libraries and initializing a computation graph in TensorF">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/11/28/循环神经网络/02_implementing_rnn/output_19_0.png">
<meta property="og:image" content="http://yoursite.com/2018/11/28/循环神经网络/02_implementing_rnn/output_19_1.png">
<meta property="og:updated_time" content="2018-11-28T13:31:18.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="望江人工智库">
<meta name="twitter:description" content="Implementing an RNN in TensorFlowThis script implements an RNN in TensorFlow to predict spam/ham from texts. We start by loading the necessary libraries and initializing a computation graph in TensorF">
<meta name="twitter:image" content="http://yoursite.com/2018/11/28/循环神经网络/02_implementing_rnn/output_19_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/11/28/循环神经网络/02_implementing_rnn/"/>





  <title> | 望江人工智库</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/28/循环神经网络/02_implementing_rnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="望江车神">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-28T21:32:30+08:00">
                2018-11-28
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Implementing-an-RNN-in-TensorFlow"><a href="#Implementing-an-RNN-in-TensorFlow" class="headerlink" title="Implementing an RNN in TensorFlow"></a>Implementing an RNN in TensorFlow</h1><p>This script implements an RNN in TensorFlow to predict spam/ham from texts.</p>
<p>We start by loading the necessary libraries and initializing a computation graph in TensorFlow.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> zipfile <span class="keyword">import</span> ZipFile</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line">ops.reset_default_graph()</span><br><span class="line"><span class="comment"># Start a graph</span></span><br><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure>
<p>Next we set the parameters for the RNN model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set RNN parameters</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line">batch_size = <span class="number">250</span></span><br><span class="line">max_sequence_length = <span class="number">25</span></span><br><span class="line">rnn_size = <span class="number">10</span></span><br><span class="line">embedding_size = <span class="number">50</span></span><br><span class="line">min_word_frequency = <span class="number">10</span></span><br><span class="line">learning_rate = <span class="number">0.0005</span></span><br><span class="line">dropout_keep_prob = tf.placeholder(tf.float32)</span><br></pre></td></tr></table></figure>
<p>We download and save the data next.  First we check if we have saved it before and load it locally, if not, we load it from the internet (UCI machine learning data repository).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Download or open data</span></span><br><span class="line">data_dir = <span class="string">'temp'</span></span><br><span class="line">data_file = <span class="string">'text_data.txt'</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(data_dir):</span><br><span class="line">    os.makedirs(data_dir)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.isfile(os.path.join(data_dir, data_file)):</span><br><span class="line">    zip_url = <span class="string">'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'</span></span><br><span class="line">    r = requests.get(zip_url)</span><br><span class="line">    z = ZipFile(io.BytesIO(r.content))</span><br><span class="line">    file = z.read(<span class="string">'SMSSpamCollection'</span>)</span><br><span class="line">    <span class="comment"># Format Data</span></span><br><span class="line">    text_data = file.decode()</span><br><span class="line">    text_data = text_data.encode(<span class="string">'ascii'</span>, errors=<span class="string">'ignore'</span>)</span><br><span class="line">    text_data = text_data.decode().split(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save data to text file</span></span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, data_file), <span class="string">'w'</span>) <span class="keyword">as</span> file_conn:</span><br><span class="line">        <span class="keyword">for</span> text <span class="keyword">in</span> text_data:</span><br><span class="line">            file_conn.write(<span class="string">"&#123;&#125;\n"</span>.format(text))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># Open data from text file</span></span><br><span class="line">    text_data = []</span><br><span class="line">    <span class="keyword">with</span> open(os.path.join(data_dir, data_file), <span class="string">'r'</span>) <span class="keyword">as</span> file_conn:</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> file_conn:</span><br><span class="line">            text_data.append(row)</span><br><span class="line">    text_data = text_data[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">text_data = [x.split(<span class="string">'\t'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> text_data <span class="keyword">if</span> len(x) &gt;= <span class="number">1</span>]</span><br><span class="line">[text_data_target, text_data_train] = [list(x) <span class="keyword">for</span> x <span class="keyword">in</span> zip(*text_data)]</span><br></pre></td></tr></table></figure>
<p>Next, we process the texts and turn them into numeric representations (words —&gt; indices).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a text cleaning function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_text</span><span class="params">(text_string)</span>:</span></span><br><span class="line">    text_string = re.sub(<span class="string">r'([^\s\w]|_|[0-9])+'</span>, <span class="string">''</span>, text_string)</span><br><span class="line">    text_string = <span class="string">" "</span>.join(text_string.split())</span><br><span class="line">    text_string = text_string.lower()</span><br><span class="line">    <span class="keyword">return</span> text_string</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Clean texts</span></span><br><span class="line">text_data_train = [clean_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> text_data_train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Change texts into numeric vectors</span></span><br><span class="line">vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,</span><br><span class="line">                                                                     min_frequency=min_word_frequency)</span><br><span class="line">text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From &lt;ipython-input-5-4e6c02d47d3d&gt;:14: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From /home/b418/anaconda3/envs/yuanxiao/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
WARNING:tensorflow:From /home/b418/anaconda3/envs/yuanxiao/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
</code></pre><blockquote>
<p>Note: there will be a WARNING:… use tensorflow/transform or tf.data.  Ignore this for now- there is an issue with getting tensorflow/transform to work. Hopefully this will be fixed soon and the code here will be updated.</p>
</blockquote>
<p>Now we shuffle and split the texts into train/tests (80% training, 20% testing).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shuffle and split data</span></span><br><span class="line">text_processed = np.array(text_processed)</span><br><span class="line">text_data_target = np.array([<span class="number">1</span> <span class="keyword">if</span> x == <span class="string">'ham'</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> text_data_target])</span><br><span class="line">shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))</span><br><span class="line">x_shuffled = text_processed[shuffled_ix]</span><br><span class="line">y_shuffled = text_data_target[shuffled_ix]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split train/test set</span></span><br><span class="line">ix_cutoff = int(len(y_shuffled)*<span class="number">0.80</span>)</span><br><span class="line">x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]</span><br><span class="line">y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]</span><br><span class="line">vocab_size = len(vocab_processor.vocabulary_)</span><br><span class="line">print(<span class="string">"Vocabulary Size: &#123;:d&#125;"</span>.format(vocab_size))</span><br><span class="line">print(<span class="string">"80-20 Train Test split: &#123;:d&#125; -- &#123;:d&#125;"</span>.format(len(y_train), len(y_test)))</span><br></pre></td></tr></table></figure>
<pre><code>Vocabulary Size: 933
80-20 Train Test split: 4459 -- 1115
</code></pre><p>Here we can define our RNN model.  We create the placeholders for the data, word embedding matrices (and embedding lookups), and define the rest of the model.</p>
<p>The rest of the RNN model will create a dynamic RNN cell (regular RNN type), which will vary the number of RNNs needed for variable input length (different amount of words for input texts), and then output into a fully connected logistic layer to predict spam or ham as output.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create placeholders</span></span><br><span class="line">x_data = tf.placeholder(tf.int32, [<span class="keyword">None</span>, max_sequence_length])</span><br><span class="line">y_output = tf.placeholder(tf.int32, [<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create embedding</span></span><br><span class="line">embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the RNN cell</span></span><br><span class="line"><span class="comment"># tensorflow change &gt;= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.</span></span><br><span class="line"><span class="keyword">if</span> tf.__version__[<span class="number">0</span>] &gt;= <span class="string">'1'</span>:</span><br><span class="line">    cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)</span><br><span class="line"></span><br><span class="line">output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)</span><br><span class="line">output = tf.nn.dropout(output, dropout_keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get output of RNN sequence</span></span><br><span class="line"><span class="comment">#output = tf.transpose(output, [1, 0, 2])</span></span><br><span class="line"><span class="comment">#last = tf.gather(output, int(output.get_shape()[0]) - 1)</span></span><br><span class="line"></span><br><span class="line">last = output[:,<span class="number">-1</span>,:]</span><br><span class="line"></span><br><span class="line">weight = tf.Variable(tf.truncated_normal([rnn_size, <span class="number">2</span>], stddev=<span class="number">0.1</span>))</span><br><span class="line">bias = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[<span class="number">2</span>]))</span><br><span class="line">logits_out = tf.matmul(last, weight) + bias</span><br></pre></td></tr></table></figure>
<p>Next we declare the loss function (softmax cross entropy), an accuracy function, and optimization function (RMSProp).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loss function</span></span><br><span class="line">losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)</span><br><span class="line">loss = tf.reduce_mean(losses)</span><br><span class="line"></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, <span class="number">1</span>), tf.cast(y_output, tf.int64)), tf.float32))</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.RMSPropOptimizer(learning_rate)</span><br><span class="line">train_step = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>You may ignore the warning, as the texts are small and our batch size is only 100.  If you increase the batch size and/or have longer sequences of texts, this model may consume too much memory.</p>
</blockquote>
<p>Next we initialize the variables in the computational graph.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">train_loss = []</span><br><span class="line">test_loss = []</span><br><span class="line">train_accuracy = []</span><br><span class="line">test_accuracy = []</span><br></pre></td></tr></table></figure>
<p>Now we can start our training!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Start training</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Shuffle training data</span></span><br><span class="line">    shuffled_ix = np.random.permutation(np.arange(len(x_train)))</span><br><span class="line">    x_train = x_train[shuffled_ix]</span><br><span class="line">    y_train = y_train[shuffled_ix]</span><br><span class="line">    num_batches = int(len(x_train)/batch_size) + <span class="number">1</span></span><br><span class="line">    <span class="comment"># TO DO CALCULATE GENERATIONS ExACTLY</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_batches):</span><br><span class="line">        <span class="comment"># Select train data</span></span><br><span class="line">        min_ix = i * batch_size</span><br><span class="line">        max_ix = np.min([len(x_train), ((i+<span class="number">1</span>) * batch_size)])</span><br><span class="line">        x_train_batch = x_train[min_ix:max_ix]</span><br><span class="line">        y_train_batch = y_train[min_ix:max_ix]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Run train step</span></span><br><span class="line">        train_dict = &#123;x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:<span class="number">0.5</span>&#125;</span><br><span class="line">        sess.run(train_step, feed_dict=train_dict)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Run loss and accuracy for training</span></span><br><span class="line">    temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)</span><br><span class="line">    train_loss.append(temp_train_loss)</span><br><span class="line">    train_accuracy.append(temp_train_acc)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run Eval Step</span></span><br><span class="line">    test_dict = &#123;x_data: x_test, y_output: y_test, dropout_keep_prob:<span class="number">1.0</span>&#125;</span><br><span class="line">    temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)</span><br><span class="line">    test_loss.append(temp_test_loss)</span><br><span class="line">    test_accuracy.append(temp_test_acc)</span><br><span class="line">    print(<span class="string">'Epoch: &#123;&#125;, Test Loss: &#123;:.2&#125;, Test Acc: &#123;:.2&#125;'</span>.format(epoch+<span class="number">1</span>, temp_test_loss, temp_test_acc))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch: 1, Test Loss: 0.71, Test Acc: 0.17
Epoch: 2, Test Loss: 0.68, Test Acc: 0.82
Epoch: 3, Test Loss: 0.63, Test Acc: 0.82
Epoch: 4, Test Loss: 0.57, Test Acc: 0.83
Epoch: 5, Test Loss: 0.51, Test Acc: 0.83
Epoch: 6, Test Loss: 0.46, Test Acc: 0.83
Epoch: 7, Test Loss: 0.43, Test Acc: 0.83
Epoch: 8, Test Loss: 0.42, Test Acc: 0.84
Epoch: 9, Test Loss: 0.41, Test Acc: 0.84
Epoch: 10, Test Loss: 0.4, Test Acc: 0.85
Epoch: 11, Test Loss: 0.4, Test Acc: 0.85
Epoch: 12, Test Loss: 0.4, Test Acc: 0.85
Epoch: 13, Test Loss: 0.4, Test Acc: 0.86
Epoch: 14, Test Loss: 0.39, Test Acc: 0.86
Epoch: 15, Test Loss: 0.39, Test Acc: 0.86
Epoch: 16, Test Loss: 0.39, Test Acc: 0.86
Epoch: 17, Test Loss: 0.38, Test Acc: 0.86
Epoch: 18, Test Loss: 0.38, Test Acc: 0.87
Epoch: 19, Test Loss: 0.37, Test Acc: 0.87
Epoch: 20, Test Loss: 0.37, Test Acc: 0.87
Epoch: 21, Test Loss: 0.35, Test Acc: 0.87
Epoch: 22, Test Loss: 0.33, Test Acc: 0.88
Epoch: 23, Test Loss: 0.3, Test Acc: 0.89
Epoch: 24, Test Loss: 0.28, Test Acc: 0.91
Epoch: 25, Test Loss: 0.25, Test Acc: 0.92
Epoch: 26, Test Loss: 0.24, Test Acc: 0.92
Epoch: 27, Test Loss: 0.22, Test Acc: 0.93
Epoch: 28, Test Loss: 0.21, Test Acc: 0.94
Epoch: 29, Test Loss: 0.2, Test Acc: 0.94
Epoch: 30, Test Loss: 0.21, Test Acc: 0.94
Epoch: 31, Test Loss: 0.19, Test Acc: 0.95
Epoch: 32, Test Loss: 0.19, Test Acc: 0.95
Epoch: 33, Test Loss: 0.19, Test Acc: 0.95
Epoch: 34, Test Loss: 0.16, Test Acc: 0.95
Epoch: 35, Test Loss: 0.16, Test Acc: 0.95
Epoch: 36, Test Loss: 0.15, Test Acc: 0.96
Epoch: 37, Test Loss: 0.15, Test Acc: 0.96
Epoch: 38, Test Loss: 0.15, Test Acc: 0.94
Epoch: 39, Test Loss: 0.15, Test Acc: 0.96
Epoch: 40, Test Loss: 0.15, Test Acc: 0.96
Epoch: 41, Test Loss: 0.14, Test Acc: 0.96
Epoch: 42, Test Loss: 0.15, Test Acc: 0.96
Epoch: 43, Test Loss: 0.15, Test Acc: 0.94
Epoch: 44, Test Loss: 0.14, Test Acc: 0.96
Epoch: 45, Test Loss: 0.14, Test Acc: 0.96
Epoch: 46, Test Loss: 0.13, Test Acc: 0.96
Epoch: 47, Test Loss: 0.14, Test Acc: 0.96
Epoch: 48, Test Loss: 0.13, Test Acc: 0.96
Epoch: 49, Test Loss: 0.13, Test Acc: 0.96
Epoch: 50, Test Loss: 0.12, Test Acc: 0.96
</code></pre><p>Here is matplotlib code to plot the loss and accuracy over the training generations for both the train and test sets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot loss over time</span></span><br><span class="line">epoch_seq = np.arange(<span class="number">1</span>, epochs+<span class="number">1</span>)</span><br><span class="line">plt.plot(epoch_seq, train_loss, <span class="string">'k--'</span>, label=<span class="string">'Train Set'</span>)</span><br><span class="line">plt.plot(epoch_seq, test_loss, <span class="string">'r-'</span>, label=<span class="string">'Test Set'</span>)</span><br><span class="line">plt.title(<span class="string">'Softmax Loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Softmax Loss'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot accuracy over time</span></span><br><span class="line">plt.plot(epoch_seq, train_accuracy, <span class="string">'k--'</span>, label=<span class="string">'Train Set'</span>)</span><br><span class="line">plt.plot(epoch_seq, test_accuracy, <span class="string">'r-'</span>, label=<span class="string">'Test Set'</span>)</span><br><span class="line">plt.title(<span class="string">'Test Accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/11/28/循环神经网络/02_implementing_rnn/output_19_0.png" alt="png"></p>
<p><img src="/2018/11/28/循环神经网络/02_implementing_rnn/output_19_1.png" alt="png"></p>
<h3 id="Evaluating-New-Texts"><a href="#Evaluating-New-Texts" class="headerlink" title="Evaluating New Texts"></a>Evaluating New Texts</h3><p>Here, we show how to use our trained model to evaluate new texts (which may or may not be spam/ham)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sample_texts = [<span class="string">'Hi, please respond 1111 asap to claim your change to win now!'</span>,</span><br><span class="line">                <span class="string">'Hey what are you doing for dinner tonight?'</span>,</span><br><span class="line">                <span class="string">'New offer, show this text for 50% off of our inagural sale!'</span>,</span><br><span class="line">                <span class="string">'Can you take the dog to the vet tomorrow?'</span>,</span><br><span class="line">                <span class="string">'Congratulations! You have been randomly selected to receive account credit!'</span>]</span><br></pre></td></tr></table></figure>
<p>Now we clean our sample texts.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clean_texts = [clean_text(text) <span class="keyword">for</span> text <span class="keyword">in</span> sample_texts]</span><br><span class="line">print(clean_texts)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;hi please respond asap to claim your change to win now&#39;, &#39;hey what are you doing for dinner tonight&#39;, &#39;new offer show this text for off of our inagural sale&#39;, &#39;can you take the dog to the vet tomorrow&#39;, &#39;congratulations you have been randomly selected to receive account credit&#39;]
</code></pre><p>Next, we transform each text as a sequence of words into a sequence of vocabulary indices.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">processed_texts = np.array(list(vocab_processor.transform(clean_texts)))</span><br><span class="line">print(processed_texts)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 93  99   0   0   1 114  13 524   1 178  21   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [121  52  20   3 151  12 332 208   0   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [ 92 376 483  39  69  12 203  15  86   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [ 28   3 104   5   0   1   5   0 143   0   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]
 [701   3  17  98   0 420   1 318 301 738   0   0   0   0   0   0   0   0
    0   0   0   0   0   0   0]]
</code></pre><p>Now we can run each of the texts through our model and get the output logits.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remember to wrap the resulting logits in a softmax to get probabilities</span></span><br><span class="line">eval_feed_dict = &#123;x_data: processed_texts, dropout_keep_prob: <span class="number">1.0</span>&#125;</span><br><span class="line">model_results = sess.run(tf.nn.softmax(logits_out), feed_dict=eval_feed_dict)</span><br><span class="line"></span><br><span class="line">print(model_results)</span><br></pre></td></tr></table></figure>
<pre><code>[[0.86792374 0.13207628]
 [0.00838861 0.9916114 ]
 [0.00871871 0.99128133]
 [0.00838833 0.99161166]
 [0.6345383  0.36546162]]
</code></pre><p>Now print results</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">categories = [<span class="string">'spam'</span>, <span class="string">'ham'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ix, result <span class="keyword">in</span> enumerate(model_results):</span><br><span class="line">    prediction = categories[np.argmax(result)]</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">'Text: &#123;&#125;, \nPrediction: &#123;&#125;\n'</span>.format(sample_texts[ix], prediction))</span><br></pre></td></tr></table></figure>
<pre><code>Text: Hi, please respond 1111 asap to claim your change to win now!, 
Prediction: spam

Text: Hey what are you doing for dinner tonight?, 
Prediction: ham

Text: New offer, show this text for 50% off of our inagural sale!, 
Prediction: ham

Text: Can you take the dog to the vet tomorrow?, 
Prediction: ham

Text: Congratulations! You have been randomly selected to receive account credit!, 
Prediction: spam
</code></pre>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢金主！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="望江车神 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="望江车神 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/28/动手学深度学习Gluon/" rel="next" title="动手学深度学习Gluon">
                <i class="fa fa-chevron-left"></i> 动手学深度学习Gluon
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/29/循环神经网络/" rel="prev" title="循环神经网络 Recurrent Neural Networks">
                循环神经网络 Recurrent Neural Networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4"
                alt="望江车神" />
            
              <p class="site-author-name" itemprop="name">望江车神</p>
              <p class="site-description motion-element" itemprop="description">深度学习你~~~</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">77</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">99</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yuanxiaoSC" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Implementing-an-RNN-in-TensorFlow"><span class="nav-number">1.</span> <span class="nav-text">Implementing an RNN in TensorFlow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluating-New-Texts"><span class="nav-number">1.0.1.</span> <span class="nav-text">Evaluating New Texts</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">望江车神</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  


  

  

</body>
</html>
