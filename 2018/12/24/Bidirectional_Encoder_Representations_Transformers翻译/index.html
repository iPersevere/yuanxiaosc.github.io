<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="BERT," />










<meta name="description" content="BERT 文中简写 原始标论文标题 其它     Peters et al., 2018 Deep contextualized word representations ELMo   Radford et al., 2018 Improving Language Understanding with Unsupervised Learning OpenAI GPT   Dai and L">
<meta name="keywords" content="BERT">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT Pre-training of Deep Bidirectional Transformers for Language Understanding 翻译">
<meta property="og:url" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="BERT 文中简写 原始标论文标题 其它     Peters et al., 2018 Deep contextualized word representations ELMo   Radford et al., 2018 Improving Language Understanding with Unsupervised Learning OpenAI GPT   Dai and L">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/figure_3.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_2.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/a9.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/4_3_1.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_3.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/4_4_1.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_4.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_5.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_6.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/figure_4.png">
<meta property="og:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_7.png">
<meta property="og:updated_time" content="2018-12-24T14:00:20.824Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BERT Pre-training of Deep Bidirectional Transformers for Language Understanding 翻译">
<meta name="twitter:description" content="BERT 文中简写 原始标论文标题 其它     Peters et al., 2018 Deep contextualized word representations ELMo   Radford et al., 2018 Improving Language Understanding with Unsupervised Learning OpenAI GPT   Dai and L">
<meta name="twitter:image" content="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/figure_3.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/"/>





  <title>BERT Pre-training of Deep Bidirectional Transformers for Language Understanding 翻译 | 望江人工智库</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="望江车神">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">BERT Pre-training of Deep Bidirectional Transformers for Language Understanding 翻译</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-24T09:30:00+08:00">
                2018-12-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文/" itemprop="url" rel="index">
                    <span itemprop="name">论文</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文/论文写作/" itemprop="url" rel="index">
                    <span itemprop="name">论文写作</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <div class="table-container">
<table>
<thead>
<tr>
<th>BERT 文中简写</th>
<th>原始标论文标题</th>
<th>其它</th>
</tr>
</thead>
<tbody>
<tr>
<td>Peters et al., 2018</td>
<td><a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Deep contextualized word representations</a></td>
<td>ELMo</td>
</tr>
<tr>
<td>Radford et al., 2018</td>
<td><a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Improving Language Understanding with Unsupervised Learning</a></td>
<td>OpenAI GPT</td>
</tr>
<tr>
<td>Dai and Le, 2015</td>
<td><a href="http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning" target="_blank" rel="noopener">Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087</a></td>
<td>AndrewMDai and Quoc V Le. 2015</td>
</tr>
<tr>
<td>Howard and Ruder, 2018</td>
<td><a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></td>
<td><strong>ULMFiT</strong>；Jeremy Howard and Sebastian Ruder.</td>
</tr>
<tr>
<td>Bow-man et al., 2015</td>
<td><a href="https://arxiv.org/abs/1508.05326v1" target="_blank" rel="noopener">A large annotated corpus for learning natural language inference</a></td>
<td>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning.</td>
</tr>
<tr>
<td>Williams et al., 2018</td>
<td><a href="https://arxiv.org/abs/1704.05426v4" target="_blank" rel="noopener">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</a></td>
<td>Adina Williams, Nikita Nangia, and Samuel R Bowman.</td>
</tr>
<tr>
<td>Dolan and Brockett, 2005</td>
<td><a href="https://www.researchgate.net/publication/228613673_Automatically_constructing_a_corpus_of_sentential_paraphrases" target="_blank" rel="noopener">Automatically constructing a corpus of sentential paraphrases</a></td>
<td>William B Dolan and Chris Brockett. 2005.</td>
</tr>
<tr>
<td>Tjong Kim Sang and De Meulder, 2003</td>
<td><a href="http://www.oalib.com/paper/4018980" target="_blank" rel="noopener">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</a></td>
<td>Erik F Tjong Kim Sang and Fien De Meulder. 2003.</td>
</tr>
<tr>
<td>Rajpurkar et al., 2016</td>
<td><a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></td>
<td>SQuAD</td>
</tr>
<tr>
<td>Tay-lor, 1953</td>
<td><a href="https://www.researchgate.net/publication/232539913_Cloze_Procedure_A_New_Tool_For_Measuring_Readability" target="_blank" rel="noopener">“Cloze Procedure”: A New Tool For Measuring Readability</a></td>
<td>Wilson L Taylor. 1953.</td>
</tr>
</tbody>
</table>
</div>
<h1 id="BERT：预训练的深度双向-Transformer-语言模型"><a href="#BERT：预训练的深度双向-Transformer-语言模型" class="headerlink" title="BERT：预训练的深度双向 Transformer 语言模型"></a>BERT：预训练的深度双向 Transformer 语言模型</h1><p>Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova<br>Google AI Language<br>{jacobdevlin,mingweichang,kentonl,kristout}@google.com</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种新的称为 BERT 的语言表示模型，BERT 代表来自 Transformer 的双向编码器表示（<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers）。不同于最近的语言表示模型（<a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Peters et al., 2018</a>，<a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Radford et al., 2018</a>）， BERT 通过在所有层的左右上下文中联合调节来预先训练深层双向表示。因此，只需要一个额外的输出层，就可以对预先训练的 BERT 表示进行微调，从而为广泛的任务（比如回答问题和语言推断任务）创建最先进的模型，而无需对特定于任务的体系结构进行大量修改。</p>
<p>BERT 的概念很简单，但实验效果很强大。它刷新了 11 个 NLP 任务的当前最优结果，包括将 GLUE 基准提升至 80.4%（7.6% 的绝对改进）、将 MultiNLI 的准确率提高到 86.7%（5.6% 的绝对改进），以及将 SQuAD v1.1 的问答测试 F1 得分提高至 93.2 分（提高 1.5 分）——比人类表现还高出 2 分。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>语言模型预训练对提高许多自然语言处理任务的效果是显著的（<a href="http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning" target="_blank" rel="noopener">Dai and Le, 2015</a>；<a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Peters et al., 2018</a>；<a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Radford et al., 2018</a>；<a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Howard and Ruder, 2018</a>）。这些任务包括句子级任务，如自然语言推理（<a href="https://arxiv.org/abs/1508.05326v1" target="_blank" rel="noopener">Bow-man et al., 2015</a>；<a href="https://arxiv.org/abs/1704.05426v4" target="_blank" rel="noopener">Williams et al., 2018</a>）和释义（<a href="https://www.researchgate.net/publication/228613673_Automatically_constructing_a_corpus_of_sentential_paraphrases" target="_blank" rel="noopener">Dolan and Brockett, 2005</a>），目的是通过整体分析来预测句子之间的关系，以及标记级任务，如命名实体识别（<a href="http://www.oalib.com/paper/4018980" target="_blank" rel="noopener">Tjong Kim Sang and De Meulder, 2003</a>）和 SQuAD 问答（<a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">Rajpurkar et al., 2016</a>），模型需要在标记级生成细粒度的输出。</p>
<p>现有的两种方法可以将预先训练好的语言表示应用到下游任务中：基于特征的和微调。基于特征的方法，如 ELMo （<a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Peters et al., 2018</a>)，使用特定于任务的模型结构，其中包含预先训练的表示作为附加特特征。微调方法，如生成预训练 Transformer  (OpenAI GPT) （<a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Radford et al., 2018</a>），引入最小的特定于任务的参数，并通过简单地微调预先训练的参数对下游任务进行训练。在之前的工作中，两种方法在预训练任务中都具有相同的目标函数，即使用单向的语言模型来学习通用的语言表示。</p>
<p>我们认为，当前的技术严重地重新严格了预训练的表示的效果，特别是对于微调方法。主要的局限性是标准语言模型是单向的，这就限制了可以在预训练期间可以使用的模型结构的选择。例如，在 OpenAI GPT中，作者使用了从左到右的模型结构，其中每个标记只能关注 Transformer 的自注意层中该标记前面的标记（<a href="https://arxiv.org/abs/1704.05426v4" target="_blank" rel="noopener">Williams et al., 2018</a>）。这样的限制对于句子级别的任务来说不是最理想的，并且当应用基于微调的方法来处理标记级别的任务(如 SQuAD 问答)时可能会造成不良的影响（<a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">Rajpurkar et al., 2016</a>），因为在标记级别的任务下，从两个方向分析上下文是至关重要的。</p>
<p>在本文中，我们通过提出 BERT 改进了基于微调的方法：来自 Transformer 的双向编码器表示。受完形填空任务的启发，BERT 通过提出一个新的预训练目标来解决前面提到的单向约束：“遮蔽语言模型”（MLM masked language model）（<a href="https://www.researchgate.net/publication/232539913_Cloze_Procedure_A_New_Tool_For_Measuring_Readability" target="_blank" rel="noopener">Tay-lor, 1953</a>）。遮蔽语言模型从输入中随机遮蔽一些标记，目的是仅根据被遮蔽标记的上下文来预测它对应的原始词汇的 id。与从左到右的语言模型预训练不同，MLM 目标允许表示融合左右上下文，这允许我们预训练一个深层双向 Transformer。除了遮蔽语言模型之外，我们还提出了一个联合预训练文本对表示来进行“下一个句子预测”的任务。</p>
<p>本文的贡献如下：</p>
<ul>
<li>我们论证了双向预训练对语言表征的重要性。与 <a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Radford et al., 2018</a> 使用单向语言模型进行预训练不同，BERT 使用遮蔽语言模型来实现预训练深层双向表示。这也与 <a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Peters et al., 2018</a> 的研究形成了对比，他们使用了一个由左到右和由右到左的独立训练语言模型的浅层连接。</li>
<li>我们表明，预先训练的表示消除了许多特定于任务的高度工程化的的模型结构的需求。BERT 是第一个基于微调的表示模型，它在大量的句子级和标记级任务上实现了最先进的性能，优于许多特定于任务的结构的模型。</li>
<li>BERT 为 11 个 NLP 任务提供了最先进的技术。我们还进行大量的消融研究，证明了我们模型的双向本质是最重要的新贡献。代码和预训练模型将可在 <a href="https://github.com/google-research/bert" target="_blank" rel="noopener">goo.gl/language/bert</a> 获取。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>BERT 文中简写</th>
<th>原始标论文标题</th>
<th>其它</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brown et al., 1992</td>
<td><a href="https://dl.acm.org/citation.cfm?id=176316" target="_blank" rel="noopener">Class-based n-gram models of natural language</a></td>
<td>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992.</td>
</tr>
<tr>
<td>Ando and Zhang, 2005</td>
<td><a href="http://academictorrents.com/details/f4470eb8bc3a6f697df61bde319fd56e3a9d6733" target="_blank" rel="noopener">A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data</a></td>
<td>Rie Kubota Ando and Tong Zhang. 2005.</td>
</tr>
<tr>
<td>Blitzer et al., 2006</td>
<td><a href="https://dl.acm.org/citation.cfm?id=1610094" target="_blank" rel="noopener">Domain adaptation with structural correspondence learning</a></td>
<td>John Blitzer, Ryan McDonald, and Fernando Pereira.</td>
</tr>
</tbody>
</table>
</div>
<p>2006.|<br>|Collobert and Weston, 2008|<a href="https://www.researchgate.net/publication/200044432_A_Unified_Architecture_for_Natural_Language_Processing" target="_blank" rel="noopener">A Unified Architecture for Natural Language Processing</a>|Ronan Collobert and Jason Weston. 2008.|<br>|Mikolov et al., 2013|<a href="https://arxiv.org/abs/1310.4546v1" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a>|CBOW Model；Skip-gram Model|<br>|Pennington et al., 2014|<a href="http://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a>|GloVe|<br>|Turian et al., 2010|<a href="https://www.researchgate.net/publication/220873681_Word_Representations_A_Simple_and_General_Method_for_Semi-Supervised_Learning" target="_blank" rel="noopener">Word Representations: A Simple and General Method for Semi-Supervised Learning</a>|Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.|<br>|Kiros et al., 2015|<a href="https://arxiv.org/abs/1506.06726v1" target="_blank" rel="noopener">Skip-Thought Vectors</a>|Skip-Thought Vectors|<br>|Logeswaran and Lee, 2018|<a href="https://arxiv.org/abs/1803.02893v1" target="_blank" rel="noopener">An efficient framework for learning sentence representations</a>|Lajanugen Logeswaran and Honglak Lee. 2018.|<br>|Le and Mikolov, 2014|<a href="https://arxiv.org/abs/1405.4053v2" target="_blank" rel="noopener">Distributed Representations of Sentences and Documents</a>|Quoc Le and Tomas Mikolov. 2014.|<br>|Peters et al., 2017|<a href="https://arxiv.org/abs/1705.00108v1" target="_blank" rel="noopener">Semi-supervised sequence tagging with bidirectional language models</a>|Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017.|<br>|Peters et al., 2018|<a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Deep contextualized word representations</a>|ELMo|<br>|Rajpurkar et al., 2016|<a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a>|SQuAD|<br>|Socher et al., 2013|<a href="https://nlp.stanford.edu/sentiment/" target="_blank" rel="noopener">Deeply Moving: Deep Learning for Sentiment Analysis</a>|SST-2|<br>|Tjong Kim Sang and De Meulder, 2003|<a href="http://www.oalib.com/paper/4018980" target="_blank" rel="noopener">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</a>|Erik F Tjong Kim Sang and Fien De Meulder. 2003.|<br>|Dai and Le, 2015|<a href="http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning" target="_blank" rel="noopener">Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087</a>|AndrewMDai and Quoc V Le. 2015|<br>|Howard and Ruder, 2018|<a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a>|<strong>ULMFiT</strong>；Jeremy Howard and Sebastian Ruder.|<br>|Radford et al., 2018|<a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Improving Language Understanding with Unsupervised Learning</a>|OpenAI GPT|<br>|Wang et al.(2018)|<a href="https://arxiv.org/abs/1804.07461v2" target="_blank" rel="noopener">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a>|GLUE|<br>|Con-neau et al., 2017|<a href="https://www.aclweb.org/anthology/D17-1070" target="_blank" rel="noopener">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</a>|Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017.|<br>|McCann et al., 2017|<a href="https://einstein.ai/static/images/pages/research/cove/McCann2017LearnedIT.pdf" target="_blank" rel="noopener">Learned in Translation: Contextualized Word Vectors</a>|Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.|<br>|Deng et al.|<a href="https://ieeexplore.ieee.org/document/5206848" target="_blank" rel="noopener">ImageNet: A large-scale hierarchical image database</a>|J. Deng,W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. 2009.|<br>|Yosinski et al., 2014|<a href="https://arxiv.org/abs/1411.1792v1" target="_blank" rel="noopener">How transferable are features in deep neural networks?</a>|Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014.|</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>预训练通用语言模型表示有很长的历史，我们将在本节简要回顾最流行的方法。</p>
<h3 id="2-1-基于特征的方法"><a href="#2-1-基于特征的方法" class="headerlink" title="2.1 基于特征的方法"></a>2.1 基于特征的方法</h3><p>几十年来，学习广泛适用的词汇表示一直是一个活跃的研究领域，包括非神经网络学领域（<a href="https://dl.acm.org/citation.cfm?id=176316" target="_blank" rel="noopener">Brown et al., 1992</a>;<a href="http://academictorrents.com/details/f4470eb8bc3a6f697df61bde319fd56e3a9d6733" target="_blank" rel="noopener"></a>;<a href="https://dl.acm.org/citation.cfm?id=1610094" target="_blank" rel="noopener">Blitzer et al., 2006</a>）和神经网络领域（<a href="https://www.researchgate.net/publication/200044432_A_Unified_Architecture_for_Natural_Language_Processing" target="_blank" rel="noopener">Collobert and Weston, 2008</a>；<a href="https://arxiv.org/abs/1310.4546v1" target="_blank" rel="noopener">Mikolov et al., 2013</a>；<a href="http://www.aclweb.org/anthology/D14-1162" target="_blank" rel="noopener">Pennington et al., 2014</a>）方法。经过预先训练的词嵌入被认为是现代 NLP 系统的一个不可分割的部分，词嵌入提供了比从头开始学习的显著改进（<a href="https://www.researchgate.net/publication/220873681_Word_Representations_A_Simple_and_General_Method_for_Semi-Supervised_Learning" target="_blank" rel="noopener">Turian et al., 2010</a>）。</p>
<p>这些方法已被推广到更粗的粒度，如句子嵌入（<a href="https://arxiv.org/abs/1506.06726v1" target="_blank" rel="noopener">Kiros et al., 2015</a>；<a href="https://arxiv.org/abs/1803.02893v1" target="_blank" rel="noopener">Logeswaran and Lee, 2018</a>）或段落嵌入（<a href="https://arxiv.org/abs/1405.4053v2" target="_blank" rel="noopener">Le and Mikolov, 2014</a>）。与传统的单词嵌入一样，这些学习到的表示通常也用作下游模型的特征。</p>
<p>ELMo（<a href="https://arxiv.org/abs/1705.00108v1" target="_blank" rel="noopener">Peters et al., 2017</a>）从不同的维度对传统的词嵌入研究进行了概括。他们建议从语言模型中提取上下文敏感的特征。在将上下文嵌入与特定于任务的架构集成时，ELMo 为几个主要的 NLP 标准提供了最先进的技术（<a href="https://arxiv.org/abs/1802.05365v2" target="_blank" rel="noopener">Peters et al., 2018</a>)，包括在 SQuAD 上的问答（<a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">Rajpurkar et al., 2016</a>），情感分析（<a href="https://nlp.stanford.edu/sentiment/" target="_blank" rel="noopener">Socher et al., 2013</a>），和命名实体识别（<a href="http://www.oalib.com/paper/4018980" target="_blank" rel="noopener">jong Kim Sang and De Meul-der, 2003</a>）。</p>
<h3 id="2-2-基于微调的方法"><a href="#2-2-基于微调的方法" class="headerlink" title="2.2 基于微调的方法"></a>2.2 基于微调的方法</h3><p>语言模型迁移学习（LMs）的一个最新趋势是，在对受监督的下游任务的模型进行微调之前，先对 LM 目标上的一些模型构造进行预训练（<a href="http://papers.nips.cc/paper/5949-semi-supervised-sequence-learning" target="_blank" rel="noopener">Dai and Le, 2015</a>；<a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Howard and Ruder, 2018</a>；<a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Radford et al., 2018</a>）。这些方法的优点是只有很少的参数需要从头开始学习。至少部分得益于这一优势，OpenAI GPT （<a href="https://blog.openai.com/language-unsupervised/" target="_blank" rel="noopener">Radford et al., 2018</a>）在 GLUE 基准测试的许多句子级任务上取得了此前最先进的结果（<a href="https://arxiv.org/abs/1804.07461v2" target="_blank" rel="noopener">Wang et al.(2018)</a>)。</p>
<h3 id="2-3-从有监督的数据中迁移学习"><a href="#2-3-从有监督的数据中迁移学习" class="headerlink" title="2.3 从有监督的数据中迁移学习"></a>2.3 从有监督的数据中迁移学习</h3><p>虽然无监督预训练的优点是可用的数据量几乎是无限的，但也有研究表明，从具有大数据集的监督任务中进行有效的迁移，如自然语言推理（<a href="https://www.aclweb.org/anthology/D17-1070" target="_blank" rel="noopener">Con-neau et al., 2017</a>）和机器翻译（<a href="https://einstein.ai/static/images/pages/research/cove/McCann2017LearnedIT.pdf" target="_blank" rel="noopener">McCann et al., 2017</a>）。在NLP之外，计算机视觉研究也证明了从大型预训练模型中进行迁移学习的重要性，有一个有效的方法可以微调在 ImageNet 上预先训练的模型（<a href="https://ieeexplore.ieee.org/document/5206848" target="_blank" rel="noopener">Deng et al., 2009</a>；<a href="https://arxiv.org/abs/1411.1792v1" target="_blank" rel="noopener">Yosinski et al., 2014</a>）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>BERT 文中简写</th>
<th>原始标论文标题</th>
<th>其它</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vaswani et al. (2017)</td>
<td><a href="https://arxiv.org/abs/1706.03762v5" target="_blank" rel="noopener">Attention Is All You Need</a></td>
<td>Transformer</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-BERT"><a href="#3-BERT" class="headerlink" title="3 BERT"></a>3 BERT</h2><p>本节将介绍 BERT 及其具体实现。首先介绍了 BERT 模型构造和输入表示。然后我们在 3.3 节介绍本文的核心创新预训练任务。在 3.4 和 3.5 节中分别详细介绍了预训练和微调模型过程。最后，在 3.6 节中讨论了 BERT 和 OpenAI GPT 之间的区别。</p>
<h2 id="3-1-模型结构"><a href="#3-1-模型结构" class="headerlink" title="3.1 模型结构"></a>3.1 模型结构</h2><p>BERT 的模型架构是一个基于 <a href="https://arxiv.org/abs/1706.03762v5" target="_blank" rel="noopener">Vaswani et al.(2017)</a>  描述的原始实现的多层双向 Transformer 编码器，并且 Transformer 编码器发布在 <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noopener">tensor2tensor</a> 代码库中。由于最近 Transformer 的使用已经非常普遍，而且我们的实现与最初的实现实际上是相同的，所以我们将省略对模型架构的详尽的背景描述，并向读者推荐 <a href="https://arxiv.org/abs/1706.03762v5" target="_blank" rel="noopener">Vaswani et al.(2017)</a> 以及优秀的指南，如“<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">带注释的 Transformer</a>”。</p>
<p>在这项工作中，我们表示层的数量(即，Transformer 块)为 $L$，隐藏尺寸为 $H$，自注意头个数为 $A$。在所有例子中，我们将前馈/过滤器的大小设置为 $4H$，即当 $H = 768$ 时是 $3072$；当 $H = 1024$ 是 $4096$。我们主要分析两个模型大小的结果:</p>
<ul>
<li>$BERT_{BASE}: L=12, H=768, A=12, Total Parameters=110M$</li>
<li>$BERT_{LARGE}: L=24, H=1024, A=16, Total Parameters=340M$</li>
</ul>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/figure_3.png" alt=""></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>BERT 文中简写</th>
<th>原始标论文标题</th>
<th>其它</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wang et al.(2018)</td>
<td><a href="https://arxiv.org/abs/1804.07461v2" target="_blank" rel="noopener">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></td>
<td>GLUE</td>
</tr>
<tr>
<td>Williams et al., 2018</td>
<td><a href="https://arxiv.org/abs/1704.05426v4" target="_blank" rel="noopener">A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference</a></td>
<td>MNLI</td>
</tr>
<tr>
<td>Chen et al., 2018</td>
<td><a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" target="_blank" rel="noopener">First Quora Dataset Release: Question Pairs</a></td>
<td>QQP</td>
</tr>
<tr>
<td>Rajpurkar et al., 2016</td>
<td><a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></td>
<td>QNLI</td>
</tr>
<tr>
<td>Socher et al., 2013</td>
<td><a href="https://nlp.stanford.edu/sentiment/" target="_blank" rel="noopener">Deeply Moving: Deep Learning for Sentiment Analysis</a></td>
<td>SST-2</td>
</tr>
<tr>
<td>Warstadt et al., 2018</td>
<td><a href="https://nyu-mll.github.io/CoLA/" target="_blank" rel="noopener">The Corpus of Linguistic Acceptability</a></td>
<td>CoLA</td>
</tr>
<tr>
<td>Cer et al., 2017</td>
<td><a href="https://arxiv.org/abs/1708.00055v1" target="_blank" rel="noopener">SemEval-2017 Task 1: Semantic Textual Similarity - Multilingual and Cross-lingual Focused Evaluation</a></td>
<td>STS-B</td>
</tr>
<tr>
<td>Dolan and Brockett, 2005</td>
<td><a href="https://www.researchgate.net/publication/228613673_Automatically_constructing_a_corpus_of_sentential_paraphrases" target="_blank" rel="noopener">Automatically constructing a corpus of sentential paraphrases</a></td>
<td>MRPC</td>
</tr>
<tr>
<td>Bentivogli et al., 2009</td>
<td><a href="https://www.mendeley.com/catalogue/fifth-pascal-recognizing-textual-entailment-challenge/" target="_blank" rel="noopener">The fifth pascal recognizing textual entailment challenge</a></td>
<td>RTE</td>
</tr>
<tr>
<td>Levesque et al., 2011</td>
<td>The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.</td>
<td>WNLI</td>
</tr>
<tr>
<td>Rajpurkar et al., 2016</td>
<td><a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a></td>
<td>SQuAD</td>
</tr>
<tr>
<td>Joshi et al., 2017</td>
<td><a href="https://arxiv.org/abs/1705.03551v2" target="_blank" rel="noopener">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</a></td>
<td>TriviaQA</td>
</tr>
<tr>
<td>Clark et al., 2018</td>
<td><a href="https://arxiv.org/abs/1809.08370v1" target="_blank" rel="noopener">Semi-Supervised Sequence Modeling with Cross-View Training</a></td>
<td></td>
</tr>
<tr>
<td>Zellers et al., 2018</td>
<td><a href="https://arxiv.org/abs/1808.05326v1" target="_blank" rel="noopener">SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference</a></td>
<td>SWAG</td>
</tr>
<tr>
<td>Vaswani et al. (2017)</td>
<td><a href="https://arxiv.org/abs/1706.03762v5" target="_blank" rel="noopener">Attention Is All You Need</a></td>
<td>Transformer</td>
</tr>
<tr>
<td>Al-Rfou et al., 2018</td>
<td><a href="https://arxiv.org/abs/1808.04444v1" target="_blank" rel="noopener">Character-Level Language Modeling with Deeper Self-Attention</a></td>
</tr>
</tbody>
</table>
</div>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><p>在这一节，我们将展示 BERT 在 11 项自然语言处理任务中的微调结果。</p>
<h3 id="4-1-GLUE-数据集"><a href="#4-1-GLUE-数据集" class="headerlink" title="4.1 GLUE 数据集"></a>4.1 GLUE 数据集</h3><p>通用语言理解评价 (GLUE General Language Understanding Evaluation) 基准（<a href="https://arxiv.org/abs/1804.07461v2" target="_blank" rel="noopener">Wang et al.(2018)</a>）是对多种自然语言理解任务的集合。大多数 GLUE 数据集已经存在多年，但 GLUE 的用途是（1）以标准分离的训练集、验证集和测试集的形式发布这些数据集；并且（2）建立一个评估服务器来缓解评估不一致和过度拟合测试集的问题。GLUE不分发测试集的标签，用户必须将他们的预测上传到GLUE服务器进行评估，并对提交的数量进行限制。<br>  GLUE 基准包括以下数据集，其描述最初在 <a href="https://arxiv.org/abs/1804.07461v2" target="_blank" rel="noopener">Wang et al.(2018)</a>中总结:</p>
<p><strong>MNLI</strong>  多类型的自然语言推理（Multi-Genre Natural Language Inference）是一项大规模的、众包的隐含分类任务 <a href="https://arxiv.org/abs/1704.05426v4" target="_blank" rel="noopener">(Williams et al.， 2018)</a>。给定一对句子，目的是预测第二个句子相对于第一个句子是暗含的、矛盾的还是中立的关系。</p>
<p><strong>QQP</strong>  Quora问题对（Quora Question Pairs）是一个二元分类任务，目的是确定两个问题在Quora上问的语义是否相等 <a href="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" target="_blank" rel="noopener">(Chen et al., 2018)</a>。</p>
<p><strong>QNLI</strong>  问题自然语言推理（Question Natural Language Inference）是斯坦福问题回答数据集 <a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">(Rajpurkar et al., 2016)</a> 已经转换为二进制分类任务的一个版本 <a href="https://arxiv.org/abs/1804.07461v2" target="_blank" rel="noopener">Wang et al.(2018)</a>。正类的例子是(问题，句子)对，句子中包含正确的答案，和负类的例子是来自同一段的(问题，句子)对，句子中不包含正确的答案。</p>
<p><strong>SST-2</strong> 斯坦福情感语义树（Stanford Sentiment Treebank）数据集是一个二元单句分类任务，数据由电影评论中提取的句子组成，并对由人工对这些句子进行标注 <a href="https://nlp.stanford.edu/sentiment/" target="_blank" rel="noopener">(Socher et al., 2013)</a>。</p>
<p><strong>CoLA</strong>  语言可接受性单句二元分类任务语料库（Corpus of Linguistic Acceptability），它的目的是预测一个英语句子在语言学上是否 “可接受” <a href="https://nyu-mll.github.io/CoLA/" target="_blank" rel="noopener">(Warstadt et al., 2018)</a>。</p>
<p><strong>STS-B</strong>  文本语义相似度基准（Semantic Textual Similarity Bench-mark ）是从新闻标题中和其它来源里提取的句子对的集合 <a href="https://arxiv.org/abs/1708.00055v1" target="_blank" rel="noopener">(Cer et al., 2017)</a>。他们用从1到5的分数标注，表示这两个句子在语义上是多么相似。</p>
<p><strong>MRPC</strong>  微软研究释义语料库（Microsoft Research Paraphrase Corpus）从在线新闻中自动提取的句子对组成，并用人工注解来说明这两个句子在语义上是否相等 <a href="https://www.researchgate.net/publication/228613673_Automatically_constructing_a_corpus_of_sentential_paraphrases" target="_blank" rel="noopener">(Dolan and Brockett, 2005).</a>。</p>
<p><strong>RTE</strong>  识别文本蕴含（Recognizing Textual Entailment）是一个与 MNLI 相似的二元蕴含任务，只是 RTE 的训练数据更少 <a href="https://www.mendeley.com/catalogue/fifth-pascal-recognizing-textual-entailment-challenge/" target="_blank" rel="noopener">Bentivogli et al., 2009</a>。</p>
<p><strong>WNLI</strong>  威诺格拉德自然语言推理（Winograd NLI）是一个来自 <a href="">(Levesque et al., 2011)</a> 的小型自然语言推理数据集。GLUE网页提示到这个数据集的构造存在问题，每一个被提交给 GLUE 的经过训练的系统在预测多数类时都低于 65.1 这个基线准确度。因此，出于对 OpenAI GPT 的公平考虑，我们排除了这一数据集。对于我们的 GLUE 提交，我们总是预测多数类。</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_2.png" alt=""></p>
<h3 id="4-1-1-GLUE-结果"><a href="#4-1-1-GLUE-结果" class="headerlink" title="4.1.1 GLUE 结果"></a>4.1.1 GLUE 结果</h3><p>为了在 GLUE 上微调模型，我们按照本文第三节中描述的那样表示输入的句子或者句子对，并且使用最后一层的隐藏向量 $C \in \mathbb{R}^H$ 中的第一个输入标记（[CLS]）作为句子总的表示。如图3 (a)和(b)所示。在微调期间唯一引入的新的参数是一个分类层 $W \in \mathbb{R}^{K \times H}$，其中 $K$ 是标签的数量。我们用 $C$ 和 $W$ 计算一个标准的分类损失，换句话说是 $log(softmax(CW^T))$。<br>我们在 GLUE 所有的任务中使用 32 的批次大小和 3 个周期。对于每个任务我们使用 $5e-5, 4e-5, 3e-5, 2e-5$ 的学习率来微调，然后在验证集中选择表现最好的学习率。此外，对于 $BERT_{LARGE}$ 我们发现它有时在小数据集上微调时不稳定（换句话说是，有时运行时会使结果更差），因此，我们进行了几次随机重启，并选择了在开发集上表现最好的模型。对于随机重启，我们使用相同的预训练检查点，但执行不同的数据打乱和分类器层初始化来微调模型。我们注意到，GLUE 发布的数据集不包括测试的标签，所以我们为每个 $BERT_{BASE}$ 和 $BERT_{LARGE}$ 分别向 GLUE 评估服务器提交结果。<br>结果如表 1 所示。在所有的任务上，$BERT_{BASE}$ 和 $BERT_{LARGE}$ 都比现有的系统更加出色 ，与先进水平相比，分别取得 4.4% 及 6.7% 的平均改善。请注意，除了 $BERT_{BASE}$ 含有注意力屏蔽（attention masking），$BERT_{BASE}$ 和 OpenAI GPT 的模型架构方面几乎是相同的。对于最大和最广泛使用的 GLUE 任务 MNLI，BERT 比当前最优模型获得了 4.7% 的绝对提升。在 GLUE 官方的排行榜上， $BERT_{LARGE}$ 获得了 80.4 的分数，与原榜首的 OpenAI GPT 相比截止本文写作时只获得了 72.8 分。<br>有趣的是， $BERT_{LARGE}$ 在所有任务中都显著优于 $BERT_{BASE}$，即使是在那些只有很少训练数据的任务上。BERT 模型大小的影响在本文 5.2 节有更深入的探讨。</p>
<h3 id="4-2-SQuAD-v1-1"><a href="#4-2-SQuAD-v1-1" class="headerlink" title="4.2 SQuAD v1.1"></a>4.2 SQuAD v1.1</h3><p>斯坦福问答数据集(SQuAD Standford Question Answering Dataset)是一个由 100k 个众包的问题/答案对组成的集合 <a href="https://arxiv.org/abs/1606.05250v3" target="_blank" rel="noopener">(Rajpurkar et al., 2016)</a>。给出一个问题和一段来自维基百科包含这个问题答案的段落，我们的任务是预测这段答案文字的区间。例如:</p>
<ul>
<li>输入问题：<br>Where do water droplets collide with ice crystals to form precipitation?</li>
<li>输入段落<br>… Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. …</li>
<li>输出答案<br>within a cloud</li>
</ul>
<p>这种区间预测任务与 GLUE 的序列分类任务有很大的区别，但是我们能够让 BERT 以一种直接的方式在 SQuAD 上运行。就像在 GLUE 中，我们将输入问题和段落表示为一个单一打包序列（packed sequence），其中问题使用 A 嵌入（A embedding），段落使用 B 嵌入（B embedding）。在微调模型期间唯一需要学习的新参数是区间开始向量 $S \in \mathbb{R}^H$ 和区间结束向量 $E \in \mathbb{R}^H$。让 BERT 模型最后一层的隐藏向量的第 $i^{th}$ 输入标记被表示为 $T_i \in \mathbb{R}^H$。如图 3 (c) 可视化的表示。然后，计算单词 $i$ 作为答案区间开始的概率，它是 $T_i$ 和 $S$ 之间的点积并除以该段落所有单词的 softmax:</p>
<script type="math/tex; mode=display">P_i=\dfrac{e^{S \cdot T_i}}{\sum_j e^{S \cdot T_j}}</script><p>同样的式子用来计算单词作为答案区间的结束的概率，并采用得分最高的区间作为预测结果。训练目标是正确的开始和结束位置的对数可能性。<br>我们使用 $5e-5$ 的学习率，32 的批次大小训练模型 3 个周期。在模型推断期间，因为结束位置与开始位置没有条件关系，我们增加了结束位置必须在开始位置之后的条件，但没有使用其他启发式。为了方便评估，把序列化后的标记区间对齐回原始未序列化的输入。<br>结果如表 2 中称述那样。SQuAD 使用一个高度严格的测试过程，其中提交者必须手动联系小组组织人员，以在一个隐藏的测试集上运行他们的系统，所以我们只提交了最好的模型来测试。表中显示的结果是我们提交给小组的第一个也是唯一一个测试。我们注意到上面的结果在小组排行榜上没有最新的公共模型描述，并被允许在训练他们的模型时使用任何的公共数据。因此，我们在提交的模型中使用非常有限的数据增强，通过在 SQuAD 和 TriviaQA<a href="https://arxiv.org/abs/1705.03551v2" target="_blank" rel="noopener">(Joshi et al., 2017)</a> 联合训练。<br>我们表现最好的模型在集成模型排名中上比排名第一模型高出 1.5 个 F1 值，在一个单模型排行榜中比排名第一的模型高出 1.7（译者注：原文是 1.3） 个 F1 值。实际上，我们的单模型 BERT 就比最优的集成模型表现更优。即使只在 SQuAD 数据集上（不用 TriviaQA 数据集）我们只损失 0.1-0.4 个 F1 值，而且我们的模型输出仍然比现有模型的表现好很多。</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/a9.png" alt=""></p>
<h3 id="4-3-命名实体识别"><a href="#4-3-命名实体识别" class="headerlink" title="4.3 命名实体识别"></a>4.3 命名实体识别</h3><p>为了评估标记任务的性能，我们在 CoNLL 2003 命名实体识别数据集（NER Named Entity Recognition）上微调 BERT 模型。该数据集由 200k 个训练词组成，这些训练词被标注为人员、组织、地点、杂项或其他（无命名实体）。<br>为了微调，我们将最后一层每个单词的隐藏表示 $T_i \in \mathbb{R}^H$ 送入一个在 NER 标签集合的分类层。每个单词的分类不以周围预测为条件（换句话说，没有自回归和没有 CRF）。为了与词块（WordPiece）序列化相适应，我们把 CoNLI-序列化的（CoNLL-tokenized）的输入词输入我们的词块序列化器，然后使用这些隐藏状态相对应的第一个块而不用预测的块标记为 X。例如：</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/4_3_1.png" alt=""></p>
<p>由于单词块序列化边界是输入中已知的一部分，因此对训练和测试都要这样做。<br>结果如表 3 所示。$BERT_{LARGE}$ 优于现存的最优模型，使用多任务学习的交叉视野训练 <a href="https://arxiv.org/abs/1809.08370v1" target="_blank" rel="noopener">(Clark et al., 2018)</a>，CoNLL-2003 命名实体识别测试集上高 0.2 F1 值。</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_3.png" alt=""></p>
<h3 id="4-4-SWAG"><a href="#4-4-SWAG" class="headerlink" title="4.4 SWAG"></a>4.4 SWAG</h3><p>Adversarial Generations（SWAG）数据集由 113k 个句子对组合而成，用于评估基于常识的推理 <a href="https://arxiv.org/abs/1808.05326v1" target="_blank" rel="noopener">(Zellers et al., 2018)</a>。<br>给出一个来自视频字幕数据集的句子，任务是在四个选项中选择最合理的延续。例如:</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/4_4_1.png" alt=""></p>
<p>为 SWAG 数据集调整 BERT 模型的方式与为 GLUE 数据集调整的方式相似。对于每个例子，我们构造四个输入序列，每一个都连接给定的句子(句子A)和一个可能的延续(句子B)。唯一的特定于任务的参数是我们引入向量 $V \in \mathbb{R}^{H}$，然后它点乘最后层的句子总表示 $C_i \in \mathbb{R}^H$ 为每一个选择 $i$ 产生一个分数。概率分布为 softmax 这四个选择:</p>
<script type="math/tex; mode=display">P_i=\dfrac{e^{V \cdot C_i}}{\sum_j^4 e^{S \cdot C_j}}</script><p>我们使用 $2e-5$ 的学习率，16 的批次大小训练模型 3 个周期。结果如表 4 所示。$BERT_{LARGE}$ 优于作者的 ESIM+ELMo 的基线标准模型的 27.1% 。</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_4.png" alt=""></p>
<h2 id="5-消融研究（Ablation-Studies）"><a href="#5-消融研究（Ablation-Studies）" class="headerlink" title="5. 消融研究（Ablation Studies）"></a>5. 消融研究（Ablation Studies）</h2><p>虽然我们已经证明了非常强有力的实证结果，但到目前为止提出的结果并没有提现出 BERT 框架的每个部分具体的贡献。在本节中，我们对 BERT 的许多方面进行了消融实验，以便更好地理解每个部分的相对重要性。</p>
<h3 id="5-1-预训练任务的影响"><a href="#5-1-预训练任务的影响" class="headerlink" title="5.1 预训练任务的影响"></a>5.1 预训练任务的影响</h3><p>我们的核心观点之一是，与之前的工作相比，BERT 的深层双向性（通过遮蔽语言模型预训练）是最重要的改进。为了证明这一观点，我们评估了两个新模型，它们使用与 $BERT_{BASE}$ 完全相同的预训练数据、微调方案和 Transformer 超参数：</p>
<ol>
<li>No NSP：模型使用“遮蔽语言模型”（MLM）但是没有“预测下一句任务”（NSP）。</li>
<li>LTR &amp; No NSP：模型使用一个从左到右（LTR）的语言模型，而不是遮蔽语言模型。在这种情况下，我们预测每个输入词，不应用任何遮蔽。在微调中也应用了仅限左的约束，因为我们发现使用仅限左的上下文进行预训练和使用双向上下文进行微调总是比较糟糕。此外，该模型未经预测下一句任务的预训练。这与OpenAI GPT有直接的可比性，但是使用更大的训练数据集、输入表示和微调方案。<br>结果如表 5 所示。我们首先分析了 NSP 任务所带来的影响。我们可以看到去除 NSP 对 QNLI、MNLI 和 SQuAD 的表现造成了显著的伤害。这些结果表明，我们的预训练方法对于获得先前提出的强有力的实证结果是至关重要的。<br>接着我们通过对比 “No NSP” 与 “LTR &amp; No NSP” 来评估训练双向表示的影响。LTR 模型在所有任务上的表现都比 MLM 模型差，在 MRPC 和 SQuAD 上的下降特别大。对于SQuAD来说，很明显 LTR 模型在区间和标记预测方面表现很差，因为标记级别的隐藏状态没有右侧上下文。因为 MRPC 不清楚性能差是由于小的数据大小还是任务的性质，但是我们发现这种性能差是在一个完全超参数扫描和许多随机重启之间保持一致的。<br>为了增强LTR系统，我们尝试在其上添加一个随机初始化的 BiLSTM 来进行微调。这确实大大提高了 SQuAD 的成绩，但是结果仍然比预训练的双向模型表现差得多。它还会损害所有四个 GLUE 任务的性能。<br>我们注意到，也可以培训单独的 LTR 和 RTL 模型，并将每个标记表示为两个模型表示的连接，就像 ELMo 所做的那样。但是：(a)这是单一双向模型参数的两倍大小；(b)对于像 QA 这样的任务来说，这是不直观的，因为 RTL 模型不能确定问题的答案；(c)由于深度双向模型可以选择使用左语境或右语境，因此严格地说，它不如深度双向模型强大。</li>
</ol>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_5.png" alt=""></p>
<h3 id="5-2-模型大小的影响"><a href="#5-2-模型大小的影响" class="headerlink" title="5.2 模型大小的影响"></a>5.2 模型大小的影响</h3><p>在本节中，我们将探讨模型大小对微调任务准确度的影响。我们用不同的层数、隐藏单位和注意力头训练了许多 BERT 模型，同时使用了与前面描述的相同的超参数和训练过程。<br>选定 GLUE 任务的结果如表 6 所示。在这个表中，我们报告了 5 次微调的随机重启的平均验证集上的模型准确度。我们可以看到，更大的模型在所选 4 个数据集上都带来了明显的准确率上升，甚至对于只有3600个标记为训练示例 MRPC 来说也是如此，并且与预训练任务有很大的不同。也许令人惊讶的是，相对于现有文献，我们能够在现成的模型基础上实现如此显著的改进。例如，<a href="https://arxiv.org/abs/1706.03762v5" target="_blank" rel="noopener">Vaswani et al.(2017)</a> 研究的最大 Transformer 为(L=6, H=1024, A=16)，编码器参数为 100M，我们所知的文献中的最大 Transformer 为(L=64, H=512, A=2)，参数为235M <a href="https://arxiv.org/abs/1808.04444v1" target="_blank" rel="noopener">(Al-Rfou et al., 2018)</a>。相比之下，$BERT_{BASE}$ 含有 110M 参数而 $BERT_{LARGE}$ 含有 340M 参数。<br>多年来人们都知道，增加模型的大小将导致在大型任务(如机器转换和语言建模)上的持续改进，表 6 所示的由留存训练数据（held-out traing data）计算的语言模型困惑度（perplexity）。然而，我们相信，这是第一次证明，如果模型得到了足够的预训练，那么将模型扩展到极端的规模也可以在非常小的任务中带来巨大的改进。</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_6.png" alt=""></p>
<h3 id="5-3-训练步数的影响"><a href="#5-3-训练步数的影响" class="headerlink" title="5.3 训练步数的影响"></a>5.3 训练步数的影响</h3><p>图 4 显示了经过 K 步预训练模型的检查点再模型微调之后在 MNLI 验证集上的准确率。这让我们能够回答下列问题:</p>
<ol>
<li>问:BERT真的需要这么多的预训练 (128,000 words/batch * 1,000,000 steps) 来实现高的微调精度吗?<br>答：是的，$BERT_{BASE}$ 在 MNLI 上进行 1M 步预训练时的准确率比 500k 步提高了近 1.0%。</li>
<li>问:遮蔽语言模型的预训练是否比 LTR 模型预训练训收敛得慢，因为每批只预测 15% 的单词，而不是每个单词?<br>答：遮蔽语言模型的收敛速度确实比 LTR 模型稍慢。然而，在绝对准确性方面，遮蔽语言模型几乎在训练一开始就超越 LTR 模型。</li>
</ol>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/figure_4.png" alt=""></p>
<h3 id="5-4-使用-BERT-基于特征的方法"><a href="#5-4-使用-BERT-基于特征的方法" class="headerlink" title="5.4 使用 BERT 基于特征的方法"></a>5.4 使用 BERT 基于特征的方法</h3><p>到目前为止，所有的 BERT 结果都使用了微调方法，将一个简单的分类层添加到预先训练的模型中，并在一个下行任务中对所有参数进行联合微调。然而，基于特征的方法，即从预训练模型中提取固定的特征，具有一定的优势。首先，并不是所有 NLP 任务都可以通过 Transformer 编码器体系结构轻松地表示，因此需要添加特定于任务的模型体系结构。其次，能够一次预先计算训练数据的昂贵表示，然后在这种表示的基础上使用更节省计算的模型进行许多实验，这有很大的计算优势。<br>在本节中，我们通过在 CoNLL-2003 命名实体识别任务上生成类似于 elmo 的预先训练的上下文表示来评估基于特征的方法中的 BERT 表现有多好。为此，我们使用与第 4.3 节相同的输入表示，但是使用来自一个或多个层的激活，而不需要对BERT的任何参数进行微调。在分类层之前，这些上下文嵌入被用作对一个初始化的两层 768 维 Bi-LSTM 的输入。<br>结果如表 7 所示。最佳的执行方法是从预训练的转换器的前 4 个隐藏层串联符号表示，这只比整个模型的微调落后 0.3 F1 值。这说明 BERT 对于微调和基于特征的方法都是有效的。</p>
<p><img src="/2018/12/24/Bidirectional_Encoder_Representations_Transformers翻译/table_7.png" alt=""></p>
<h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h2><p>最近，由于使用语言模型进行迁移学习而取得的经验改进表明，丰富的、无监督的预习是许多语言理解系统不可或缺的组成部分。特别是，这些结果使得即使是低资源（少量标签的数据集）的任务也能从非常深的单向结构模型中受益。我们的主要贡献是将这些发现进一步推广到深层的双向结构，使同样的预训练模型能够成功地广泛地处理 NLP 任务。<br>虽然这些实证结果很有说服力，在某些情况下甚至超过了人类的表现，但未来重要的工作是研究 BERT 可能捕捉到的或不捕捉到的语言现象。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>
      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢金主！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="望江车神 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="望江车神 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/BERT/" rel="tag"># BERT</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/23/EM最大期望算法/" rel="next" title="EM最大期望算法">
                <i class="fa fa-chevron-left"></i> EM最大期望算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4"
                alt="望江车神" />
            
              <p class="site-author-name" itemprop="name">望江车神</p>
              <p class="site-description motion-element" itemprop="description">深度学习你~~~</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">89</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">36</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">105</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yuanxiaoSC" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#BERT：预训练的深度双向-Transformer-语言模型"><span class="nav-number">1.</span> <span class="nav-text">BERT：预训练的深度双向 Transformer 语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#摘要"><span class="nav-number">1.1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍"><span class="nav-number">1.2.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-相关工作"><span class="nav-number">1.3.</span> <span class="nav-text">2 相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-基于特征的方法"><span class="nav-number">1.3.1.</span> <span class="nav-text">2.1 基于特征的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-基于微调的方法"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.2 基于微调的方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-从有监督的数据中迁移学习"><span class="nav-number">1.3.3.</span> <span class="nav-text">2.3 从有监督的数据中迁移学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-BERT"><span class="nav-number">1.4.</span> <span class="nav-text">3 BERT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-模型结构"><span class="nav-number">1.5.</span> <span class="nav-text">3.1 模型结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-实验"><span class="nav-number">1.6.</span> <span class="nav-text">4. 实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-GLUE-数据集"><span class="nav-number">1.6.1.</span> <span class="nav-text">4.1 GLUE 数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-GLUE-结果"><span class="nav-number">1.6.2.</span> <span class="nav-text">4.1.1 GLUE 结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-SQuAD-v1-1"><span class="nav-number">1.6.3.</span> <span class="nav-text">4.2 SQuAD v1.1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-命名实体识别"><span class="nav-number">1.6.4.</span> <span class="nav-text">4.3 命名实体识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-SWAG"><span class="nav-number">1.6.5.</span> <span class="nav-text">4.4 SWAG</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-消融研究（Ablation-Studies）"><span class="nav-number">1.7.</span> <span class="nav-text">5. 消融研究（Ablation Studies）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-预训练任务的影响"><span class="nav-number">1.7.1.</span> <span class="nav-text">5.1 预训练任务的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-模型大小的影响"><span class="nav-number">1.7.2.</span> <span class="nav-text">5.2 模型大小的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-训练步数的影响"><span class="nav-number">1.7.3.</span> <span class="nav-text">5.3 训练步数的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-使用-BERT-基于特征的方法"><span class="nav-number">1.7.4.</span> <span class="nav-text">5.4 使用 BERT 基于特征的方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-结论"><span class="nav-number">1.8.</span> <span class="nav-text">6. 结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">1.9.</span> <span class="nav-text">参考文献</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">望江车神</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
