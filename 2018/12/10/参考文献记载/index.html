<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="原文 中文or用法 查看到的论文 原始论文     Nowadays, a large amount of conversational exchanges is available in social media websites such as Twitter and Reddit, which raise the prospect of building data-driven mo">
<meta name="keywords" content="深度学习;机器学习;人工智能">
<meta property="og:type" content="article">
<meta property="og:title" content="参考文献记载">
<meta property="og:url" content="http://yoursite.com/2018/12/10/参考文献记载/index.html">
<meta property="og:site_name" content="望江人工智库">
<meta property="og:description" content="原文 中文or用法 查看到的论文 原始论文     Nowadays, a large amount of conversational exchanges is available in social media websites such as Twitter and Reddit, which raise the prospect of building data-driven mo">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-12-10T07:02:39.593Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="参考文献记载">
<meta name="twitter:description" content="原文 中文or用法 查看到的论文 原始论文     Nowadays, a large amount of conversational exchanges is available in social media websites such as Twitter and Reddit, which raise the prospect of building data-driven mo">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/12/10/参考文献记载/"/>





  <title>参考文献记载 | 望江人工智库</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">望江人工智库</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/10/参考文献记载/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="望江车神">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="望江人工智库">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">参考文献记载</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-10T11:30:15+08:00">
                2018-12-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文/" itemprop="url" rel="index">
                    <span itemprop="name">论文</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/论文/论文写作/" itemprop="url" rel="index">
                    <span itemprop="name">论文写作</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <div class="table-container">
<table>
<thead>
<tr>
<th>原文</th>
<th>中文or用法</th>
<th>查看到的论文</th>
<th>原始论文</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nowadays, a large amount of conversational exchanges is available in social media websites such as Twitter and Reddit, which raise the prospect of building data-driven models[64].</td>
<td>如今，在Twitter和Reddit等社交媒体网站上可以进行大量的对话交流，这提高了构建数据驱动模型的前景。</td>
<td><a href="https://arxiv.org/abs/1711.01731v3" target="_blank" rel="noopener">A Survey on Dialogue Systems: Recent Advances and New Frontiers</a></td>
<td>64<a href="https://www.researchgate.net/publication/221013184_Data-Driven_Response_Generation_in_Social_Media" target="_blank" rel="noopener">Data-Driven Response Generation in Social Media</a></td>
</tr>
<tr>
<td>The <strong>encoder</strong> RNN calculates the context vector c by $h_t=f(x_t,h_{t-1})), where ht is the hidden state at time step t, f is a non-linear function such as long-short term memory unit <strong>(LSTM)</strong> [27] and gated recurrent unit <strong>(GRU)</strong> [12]</td>
<td>涉及LSTM可以用[27];涉及encoder RNN 中的GRU可以用[12]</td>
<td><a href="https://arxiv.org/abs/1711.01731v3" target="_blank" rel="noopener">A Survey on Dialogue Systems: Recent Advances and New Frontiers</a></td>
<td>27<a href="https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735" target="_blank" rel="noopener">Long Short-Term Memory</a>12<a href="https://www.semanticscholar.org/paper/Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer/0b544dfe355a5070b60986319a3f51fb45d1348e" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></td>
</tr>
<tr>
<td>[5] then improved the performance by the <strong>attention mechanism</strong>, where each word in Y is conditioned on different context vector c, with the observation that each word in Y may relate to different parts in x. In particular, yi corresponds to a context vector ci, and ci is a weighted average of the encoder hidden states h1, …, hT :$c_i=\sum^T_{j=1}\alpha_{ij}h_j$</td>
<td>涉及Seq2Seq中的 attention mechanism 可以用[5]</td>
<td><a href="https://arxiv.org/abs/1711.01731v3" target="_blank" rel="noopener">A Survey on Dialogue Systems: Recent Advances and New Frontiers</a></td>
<td>5<a href="https://arxiv.org/abs/1409.0473v2" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></td>
</tr>
<tr>
<td></td>
<td>涉及RNN 和 language model</td>
<td></td>
<td><a href="https://www.researchgate.net/publication/221489926_Recurrent_neural_network_based_language_model" target="_blank" rel="noopener">Recurrent neural network based language model</a></td>
</tr>
<tr>
<td>With progress of machine learning techniques in recent years, it has become possible to train more complex models on much larger data set, and they typically outperform the simple models. Probably the most successful concept is to use distributed representations of words [10]. For example, neural network based language models significantly outperform N-gram models [1, 27, 17].</td>
<td>随着近年来机器学习技术的进步，在更大的数据集上训练更复杂的模型已经成为可能，而且它们通常优于简单的模型。也许最成功的概念是使用单词[10]的分布式表示。例如，基于神经网络的语言模型显著优于N-gram模型[1,27,17]。</td>
<td><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></td>
<td>10<a href="https://www.researchgate.net/publication/237044580_Parallel_Distributed_Processing_Explorations_in_the_Microstructure_of_Cognition" target="_blank" rel="noopener">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</a>;1<a href="https://www.researchgate.net/publication/221618573_A_Neural_Probabilistic_Language_Model" target="_blank" rel="noopener">A Neural Probabilistic Language Model</a>;17<a href="http://xueshu.baidu.com/usercenter/paper/show?paperid=ba1a76bf87138d6cd581b1caed077094&amp;site=xueshu_se" target="_blank" rel="noopener">Empirical Evaluation and Combination of Advanced Language Modeling Techniques</a></td>
</tr>
<tr>
<td>Representation of words as continuous vectors has a long history [10, 26, 8]</td>
<td>单词作为连续向量的表示有着悠久的历史[Word2Vec 中10,26,8]</td>
<td><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></td>
<td>10<a href="https://www.researchgate.net/publication/237044580_Parallel_Distributed_Processing_Explorations_in_the_Microstructure_of_Cognition" target="_blank" rel="noopener">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</a>;26<a href="https://www.nature.com/articles/323533a0?foxtrotcallback=true" target="_blank" rel="noopener">Learning representations by back-propagating errors</a>;8<a href="http://www.oalib.com/references/19951553" target="_blank" rel="noopener">Finding Structure in Time. Cognitive Science</a></td>
</tr>
<tr>
<td>A very popular model architecture for estimating neural network language model (NNLM) was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model. This work has been followed by many others.</td>
<td>在[1]中，提出了一种非常流行的神经网络语言模型(NNLM)的估计模型体系结构，利用线性投影层和非线性隐层的前馈神经网络联合学习词向量表示和统计语言模型。这项工作已被许多人仿效。</td>
<td><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></td>
<td>1<a href="https://www.researchgate.net/publication/221618573_A_Neural_Probabilistic_Language_Model" target="_blank" rel="noopener">A Neural Probabilistic Language Model</a></td>
</tr>
<tr>
<td>It was later shown that the word vectors can be used to significantly improve and simplify many NLP applications [4, 5, 29].</td>
<td>后来的研究表明，word vector可以显著改进和简化许多NLP应用程序[4,5,29]。</td>
<td><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></td>
<td>4<a href="http://xueshu.baidu.com/usercenter/paper/show?paperid=f117d2cdd52eb707dcd8b0f82cd3d84e&amp;site=xueshu_se" target="_blank" rel="noopener">A unified architecture for natural language processing</a>;5<a href="https://www.bibsonomy.org/bibtex/2ca77f3716b06805465d830bb3c01989d/alexgrimm94" target="_blank" rel="noopener">Natural language processing (almost) from scratch</a>;29<a href="https://www.researchgate.net/publication/220873681_Word_Representations_A_Simple_and_General_Method_for_Semi-Supervised_Learning" target="_blank" rel="noopener">Word Representations: A Simple and General Method for Semi-Supervised Learning.</a></td>
</tr>
<tr>
<td></td>
<td>「对表征（representation）空间的依赖贯穿计算机科学乃至日常生活的始终。在计算机科学中，如果数据有精当的结构，辅以智能化的索引，那么搜索任务的速度可以指数级加快；对于人来说，计算『 210 除以 6 等于几？』是容易的，计算『 CCX 除以 VI 等于几？』则需要更多时间。表征空间的选择对机器学习算法的性能影响，由此可见一斑。」《深度学习》[1] 一书如是评价表征的重要性。</td>
<td></td>
<td>1<a href="http://www.oalib.com/references/20191330" target="_blank" rel="noopener">Goodfellow, I., Bengio, Y., Courville, A., &amp; Bengio, Y. (2016). Deep learning (Vol. 1). Cambridge: MIT press.</a></td>
</tr>
<tr>
<td></td>
<td>2007 年，Bengio 与 Yann LeCun 合著的论文 [2] 着重强调表征必须是多层的、逐渐抽象的。13 年，Bengio 在综述论文中 [3]，增加了对解纠缠（Disentangling）的强调。</td>
<td></td>
<td>2<a href="http://xueshu.baidu.com/usercenter/paper/show?paperid=f721b2a34bc8841701d6cc8b8417bd8b&amp;site=xueshu_se" target="_blank" rel="noopener"> Bengio, Y., &amp; LeCun, Y. (2007). Scaling learning algorithms towards AI. Large-scale kernel machines, 34(5), 1-41.</a>;3<a href="https://link.springer.com/chapter/10.1007%2F978-3-642-39593-2_1" target="_blank" rel="noopener">Bengio, Y. (2013, July). Deep learning of representations: Looking forward. In International Conference on Statistical Language and Speech Processing (pp. 1-37). Springer, Berlin, Heidelberg.</a></td>
</tr>
<tr>
<td>While Deep Learning models have achieved state-of-the-art on many NLP tasks, these models are trained from scratch, requiring large datasets, and days to converge. Adoption of transfer learning for NLP has lagged behind computer vision (CV).</td>
<td>虽然深度学习模型在许多NLP任务上已经达到了最先进的水平，但是这些模型是从头开始训练的，需要大量的数据集，并且需要几天的时间来收敛。NLP中迁移学习的采用滞后于计算机视觉。</td>
<td><a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></td>
<td></td>
</tr>
<tr>
<td>Fine-tuning pretrained word embeddings Mikolov et al. (2013); Pennington et al. (2014), a simple transfer learning technique that only targets a model’s first layer, has had an outsized impact in practice and is used in most state-of-the-art models.</td>
<td>微调预训练词嵌入Mikolov等。 （2013）; Pennington等。 （2014），一种仅针对模型第一层的简单转移学习技术，在实践中产生了巨大的影响，并在大多数最先进的模型中使用。</td>
<td><a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></td>
<td><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></td>
</tr>
<tr>
<td>Recent approaches concatenate embeddings derived from other tasks such as language modeling or machine translation with the input at different layers Peters et al. (2017); McCann et al. (2017); Peters et al. (2018).  These approaches, however, still train the main task model from scratch and treat the pretrained embeddings as fixed parameters, limiting their usefulness.</td>
<td>2018最近的方法将从其他任务(如语言建模或机器翻译)中获得的嵌入与彼得斯等人在不同层上的输入连接起来。(2017);麦肯et al。(2017);匿名(2018)。然而，这些方法仍然从头开始训练主要的任务模型，并将预先训练的嵌入作为固定的参数，从而限制了它们的有用性。</td>
<td><a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></td>
<td>Peters et al. (2017)<a href="">Semi-supervised sequence tagging with bidirectional language models</a>;McCann et al. (2017)<a href="">Learned in Translation: Contextualized Word Vectors</a>;Peters et al. (2018)<a href="">Deep contextualized word representations</a></td>
</tr>
<tr>
<td>In NLP, only recently have methods been proposed that go beyond transferring word embeddings.The prevailing approach is to pretrain embeddings that capture additional context via other tasks.The embeddings are then concatenated either with the word embeddings or with the inputs at intermediate layers.</td>
<td>在NLP中，直到最近2017才有人提出了超越嵌入式传输的方法。流行的方法是预先嵌入通过其他任务捕获额外上下文的嵌入。然后将嵌入与单词嵌入或中间层的输入连接起来。</td>
<td><a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></td>
<td></td>
</tr>
<tr>
<td>Multi-task learning A related direction is multi-task learning (MTL) (Caruana, 1993). This is the approach taken by Rei (2017) and Liu et al.(2018) who add a language modeling objective to the model that is trained jointly with the main task model. MTL requires the tasks to be trained from scratch every time, which makes it inefficient and often requires careful weighting of the task-specific objective functions (Chen et al., 2017).</td>
<td><strong>多任务学习的缺点</strong>一个相关的方向是多任务学习（MTL）（CARUANA，1993）。这是Rei（2017）和Liu等人（2018）采取的方法，他们向与主要任务模型联合训练的模型添加语言建模目标。MTL每次都要求从头开始对任务进行训练，这使得其效率低下，并且常常需要对任务特定的目标函数进行仔细加权（Chen等人，2017）。</td>
<td><a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></td>
<td>Caruana, 1993<a href="">(Multitask Learning: A Knowledge-Based Source of Inductive Bias</a>;Rei (2017)<a href="">Semi-supervised multitask learning for sequence labeling</a>; Liu et al.(2018)<a href="">Empower sequence labeling with task-aware neural language</a>;chen 2017<a href="">Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks pages 1–10.</a></td>
</tr>
<tr>
<td>As different layers capture different types of information Yosinski et al. (2014), they should be fine-tuned to different extents.</td>
<td>由于不同的层捕获不同类型的信息，Yosinski等。(2014)，需要对其进行不同程度的微调。</td>
<td><a href="https://arxiv.org/abs/1801.06146v5" target="_blank" rel="noopener">Universal Language Model Fine-tuning for Text Classification</a></td>
<td><a href="">How transferable are features in deep neural networks?In Advances in neural information processing systems. pages 3320–3328.</a></td>
</tr>
</tbody>
</table>
</div>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>感谢金主！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="望江车神 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="望江车神 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/10/Bag_of_Tricks_for_Image_Classification_with_Convolutional_Neural_Networks/" rel="next" title="Bag of Tricks for Image Classification with Convolutional Neural Networks">
                <i class="fa fa-chevron-left"></i> Bag of Tricks for Image Classification with Convolutional Neural Networks
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/10/TensorFlow_seq2seq_attention_wrapper/" rel="prev" title="TensorFlow_seq2seq_attention_wrapper源码阅读">
                TensorFlow_seq2seq_attention_wrapper源码阅读 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="https://avatars0.githubusercontent.com/u/16183570?s=400&u=5e09ebb784cfd47de99d249f2be2413adcf4e672&v=4"
                alt="望江车神" />
            
              <p class="site-author-name" itemprop="name">望江车神</p>
              <p class="site-description motion-element" itemprop="description">深度学习你~~~</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">111</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">41</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">118</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yuanxiaoSC" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">望江车神</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
